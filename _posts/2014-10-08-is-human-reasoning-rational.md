---
title: 'Is human reasoning rational?'
tags: [Brain & Cognition, Readings]
status: publish
type: post
published: true
categories: []
#header:
#  overlay_image: /assets/images/blog/default_header.jpg
#  overlay_filter: rgba(0,0,0,0.2)
  #teaser: /assets/images/blog/default_header.jpg

#excerpt: ""
---
Shamelessly plugging course work into the blog. Why not, right? Since I have
to write once a week, might as well get my blog back up to healthy numbers.

* * *

To tackle the problem of rationality, we must first define explicitly what
being rational means. It seems that, through the readings, rationality is
taken to be equivalent as deductive logic, which is then studied in laboratory
settings and results in surprising performance incongruences. I will address
the irrationality of that in the latter half of this essay, but first, I want
to share some anecdotal experiences of my encounter with rationality.

I went to college for engineering, and lived with 3 of my best friends who
were all mathematically-inclined and extremely logical people- in other words,
the most rational people I knew. We’d have debates on a regular basis, during
which we try to subdue each other’s argument by finding logical
inconsistencies. But these arguments were not entirely scholarly in nature, as
they are often a true reflection of our opinions, which meant that the defeat
of one’s argument was equivalent to denouncing one’s rationality, and that was
a cardinal sin in the undergraduate engineering population. To give an
example, one argument went like this: since we’re all studying to become
engineers, and that we want to contribute to society, then we should do
everything we can to tilt the balance towards positive progress. If this is
the case, then it is only rational that when the time comes to have children,
we should instead adopt children from less fortunate circumstances and raise
them as our own, which will simultaneously improve the quality of a person’s
life by removing them from all kinds of risk factors associated with
orphanages, as well as remove a burden from the social welfare system. After
flailing about in trying to find a rational counter to this argument backed by
any sort of empirical data- stretching from evolution reasons of passing down
one’s genes, to the chemical pheromones of one’s own offspring facilitating
bonding- in the end, I gave up my own rationality, because I just want my own
kid and I don’t quite know why. This was not the only argument to end this
way, as it wasn’t a question of IF we’re proven to be irrational, but a
question of when and after how many “why”s.

From these exchanges, I concluded that human behaviour is actually very
rational, or at least we have the capacity to act rationally. The problem was
that at the very bottom level, there are always axioms that cannot be further
produced through logical deductions. In fact, this is true of any formal
system: a logical deduction must have a point of origin, which serves to be
the axioms of the system and are taken unquestioned. And as Kurt Godel has
pointed out, no meaningful formal system can be complete and consistent, which
boils down to that some things cannot be proven and will have to be taken for
granted. Of course, some axioms are useful, and ideally they’re consistent,
but they don’t have to be, especially in humans. So really, being rational
just means that we are able to go from axiom to theorem, and one theorem to
the next, while consistently following a set of syntactical rules. I don’t
claim to understand these human axioms or their origins, nor am I a proponent
of Freudian theories that some things are just the way they are, like wishing
to sleep with one’s mother. But I do believe that there is a clear parallel
between the human mind and formal systems. A big caveat, though, is that
formal systems exclude the possibility of learning anything new, and if humans
strictly operated using deductive (or rational) reasoning, then we will not be
able to survive, even as individuals. It could be that humans are rational
within our scope of knowledge, but outside of it, we need to be able to act
irrationally – inconsistently – in order to learn new behavioural rules to
survive.

Having set up my position, I want to address some of the points raised in the
readings. On the surface, it feels like the 3 pieces seem to progressively
increase in their reasonableness, as a function of time. Newell and Simon’s
general problem solver is an archaic and ridiculous model of human cognition,
as it “replicated” a very narrow set of behaviors and called it mission
accomplished, not to mention its assumption of logical operations in the human
mind to start with. Oaksford and Chater, on the other hand, summarized the
more agreeable, Baysian models of thought and decision making. But if we take
a step back, it should be apparent at the time of each of the three articles,
the proposed model for human thought was nowhere near to be the complete
blueprint for cognition, but it was simply claimed to be a worthwhile model
because that was the state of the art in our advancement of inference
algorithms. In other words, these models weren’t cognition-inspired, but we
decided to fit our conception of the mind to these models because that’s the
best we could do at the time - in the 60s, it was deductive logic, and in the
90s, it was Bayesian inference. Knowing that, perhaps we should be reminded of
our humility the next time we claim to have a model of the human mind.

My biggest issues with modeling human rationality, though, are in the
following two problems. The first being the scientific methods employed in
testing rationality so far, or rather, what we deem to be rational in a
laboratory setting. Given that this is a scientific enterprise, I understand
that we must tightly control parameters in order to extract meaningful
interpretations from experiments. Although, having indeed controlled for the
parameters, we must then present conclusions with qualifiers for the
experimental settings, and not paint with broad strokes on human rationality.
This was touched on in Oaksford & Chater, though I’d like to draw emphasis to
it. Plopping a subject down in a room and presenting a deduction type puzzle
certainly provides insight on one’s ability to operate within a well-defined
system. However, it should not come as a surprise to both the logic and the
models people that the subject comes from an outside system, having practiced
rationality using a separate set of rules for some 20 years (for your average
undergraduate participant anyway). It may be true that the performance in the
Wason task significantly improves when a real life scenario is used instead of
its original wording, but that should be expected as well, because if you
present a calculus problem, people do much better when they can utilize their
intuition and experience on velocity, rather than the “derivative of
distance”. So in the end, are people’s failures on the Wason task an
indication of irrationality, or an inability to translate between parallel
situations?

My second problem comes from the disregard for the level of abstraction from
the mind to the model, and thus what meaningful conclusions we can and cannot
draw from it. In this particular aspect, I agree with John Searle, which is
that a model of something is not the thing itself. The deductive GPS mimics
the behavioral process a human uses when solving a logic puzzle, then humans
must be deductive! Or even now, because Bayesian models can predict human
behavior, we must (implicitly) think in Bayesian terms! Then, if we build a
robot that can intake food and lie down for 7 hours at night, would we call it
human as well? I am aware that no one claimed that the GPS IS a human, but I
do believe that we should make these kinds of general comments with extreme
caution. This, then, begs the question of can we ever create something that
thinks, as no matter how close the abstraction is to the real deal, there is
always a gap, even if the model consists of physiologically accurate neuronal
processes. To that, I offer the point that no two human minds are even alike,
so if the closest thing to a human mind isn’t even really like that particular
human mind, how close do we hope to approximate using a machine? But,
depending on how much detail we want to smear away, we CAN make (often useful)
abstractions and lump certain human faculty together, such as deductive
reasoning and probabilistic inference, we just have to be conscious of that
fact.

In closing, I found it amusing that while Newel/Simon and Laird attempt to
argue that the mind is a symbol manipulating machine, Searle argues that the
machine cannot be the mind – and I disagreed with both accounts. I think this
really boils down to personal philosophy, and mine is that no matter how close
a machine gets, it can never BE human. But it can approximate human thoughts
very accurately, depending on the level of abstraction, though it will have to
be years removed from the ones in the readings to be considered even close to
human. It is, in a way, asymptotic, but since each individual human mind can
only be asymptotically similar to another, once a machine gets within range,
we may not be able to tell the difference.

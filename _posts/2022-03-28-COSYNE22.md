---
title: 'COSYNE 2022'
tags: [General Science, Reflections, Science Communication]
status: publish
type: post
published: False
header:
  overlay_image: /assets/images/blog/2022-03-28-cosyne22.jpg
  overlay_filter: rgba(0,0,0,0.8)
  teaser: /assets/images/blog/2022-03-28-cosyne22.jpg
classes:
  - wide

toc: true
toc_label: "outline"
toc_icon: "space-shuttle"

permalink: /blog/:year/:month/:day/

excerpt: ""
---

At the risk of trying to replicate my own success (but also to revive this blog), I will attempt to summarize some of my own takeaways after attending Cosyne 2022. Off the bat, I can tell you that it won't be as comprehensive as the 2019 post. One simple reason for that is just that I didn't go to as many of the talks (lol), so the coverage will not be nearly as comprehensive. In fact, you should by no means read this as an "objective" survey of the topics and trends at the conference, but rather views through a very filtered lens. This is not _just_ a product of my laziness and skipping talks, though. I'll try to convince you later that this was (at least in part) by design. But also, in the 3 years since Cosyne 2019, a lot of things have changed. While the scientific community is still struggling to claw back towards some kind of in-person normalcy, the upside is that most of the conference material exists online in one form or another, even the poster. More importantly, they exist in a systematically-curated and accessible form, in no small part thanks to efforts from the [organizers][yt_cosyne], but also to community-driven initiatives (like [World Wide Neuron][wwn_virtual]) and crazy [individual efforts][tw_talukder], for making such resources available.

The biggest difference between 2019 and 2022, though, is that I'm 3 years older, having existed as a fly on the Cosyne auditorium wall for 3 years longer, and somehow finding myself to be partially integrated into this community. Actually, in true, unbashful, senior(er) academic fashion, I'm going to tell you that the most interesting thing I took away this year was in fact this particular piece of "meta-perspective", and it manifests in several concrete ways:

First, my view of the science that was presented at the conference is now less of a point observation, but one of a noisy trajectory or derivative. Of course, this is something the organizers needed to be aware of in constructing the conference program, and it shows up implicitly in the reviewer biases in which abstracts were accepted (in terms of what's "in" and "out"). I'm sure they talked about what their intentions were at some point, but I somehow managed to miss the opening talk. In any case, I will raise the disclaimer again that this in no way represents a "true" reflection of the conference themes, but my filtered version, so I will only highlight and comment on a few of my personal favorites in Section I.

Second, beyond passively participating in the conference program, I had the good fortune to actively shape a small part of it by co-organizing a workshop with [Roxana Zeraati][tw_zeraati] on neuronal timescales, along with a line-up of fantastic speakers (and cool people). This was obviously a stressful and intense experience, especially on the day of, but it was also exhilarting and insanely productive scientifically. Not only that, I felt like I was able to connect with the people there—_as human beings_—in a much deeper way than I was able to while just chatting at a poster, or casually meeting up as a part of socializing at the conference, especially being a relatively introverted person. In Section II, I will give a brief report of the discussions we had on __"Mechanisms, functions, and methods for diversity of neuronal and network timescales"__, as well as some reflections about my experience as a workshop co-organizer and participant.

Lastly, after 7 years of going to conferences, it finally occured to me in Lisbon that I don't (explicitly) know _how to conference_. There are a lot of conventional wisdoms floating around about how to do conferences, and sometimes specific instructions for specific conferences, most of which about prioritizing watching talks vs. going to posters vs. socializing. A lot of it is helpful, but I think it dawned on me that nobody really ever gave me _systematic_ instructions on **how** to conference, and that probably most people didn't get this as a part of how-to-science manual either. What made me realize this in the first place: that walking around, how my environment interacted with me as a 30-year old postdoc was a lot different than a 27-year old grad student, and so maybe I should approach my environment differently as well? In the last section, I will continue to not give instructions, because there are no one-size-fit-all advice: it's different for different people, at different conferences, and most importantly, at different stages of career. At the same time, lots of people share the feeling that conferences are overwhelming, exhausting, and guilt-inducing. Instead, I will write a bit about this realization, and make some _meta-suggestions_ for how to conference—__in particular, setting goals that are appropriate for your interests, career stage, and personality.__

---
# Section I. Science Highlights
I took notes for some talks and posters. They're not very good notes. To be honest I'm not even 100% confident I left with the correct takeaways, so please feel free to correct me if I misrepresented something. This is partially due to my lack of attention span, but also speaks to the incredible volume of cool works at Cosyne. Actually, a conference like this is really difficult because everything looks interesting and at least tangentially relevant to the broader theme of neural dynamics and computations (at Cosyne? wow dude who would've thought). In the end, instead of _a priori_ choosing what's relevant for me personally, I've basically given up, and instead rely on _learning_ about what interests me the most after the fact, based on which of the talks I happened to be at that inspired the most thoughts. They mostly fell into the following categories (though I had to do some shoe-horning for some):

### I love "weird" stuff
Hands down, what I enjoyed the most are talks that are quite different from the "classical" Cosyne stuff, and I'm very grateful that the organizers decided to include a broader set of topics for the talks. Nothing against PCAs & ANNs, it's just that the bandwidth (i.e., new information per minute) is much higher for talks on weird stuff I've never thought about before. I guess I enjoy the feeling of hearing about ideas that could fundamentally change how I think about something, or just ideas that are so completely unfamiliar to me, that it triggers a novelty reward. Of these, I want to highlight 2 talks and a poster:

[Asya Rolls][link_rolls] talked about the similarities and differences between the nervous system and the immune system, how they both make "memories", and how memories in these two systems interact. She showed results from some super interesting experiments, and her talk is [online][yt_rolls], I highly recommend checking it out because it's quite accessible as an outsider to immunology. In a nutshell, these two systems face similar environmental demands, in that they have to adapt to novel situations, especially situations where remembering how to optimally act might save your life the next time around (think a tiger vs. a novel pathogen). Like a lot of people, I got my Immunology degree from Twitter after COVID vaccines dropped, but never did I think about how the brain might be involved in the immune response (the one thing I study that is suppose to "remember" stuff). Among their crazy results: chemogenetic activation (via DREADD) of dopaminergic neurons in the VTA leads to a more potent lymphocyte response, which (I think shown in a separate experiment) can trigger a pathway through the bone marrow (?!?), resulting in proliferation of cells that can kill tumor cells more effectively after VTA activation. In plain English: when your reward system is activated in coincidence with an immune response to a foreign pathogen, the immune response is stronger next time. On the flip side, you could have an allergic reaction (which is a "rogue" immune response) upon holding fake flowers if you have an allergy to real flowers, just because your brain recognizes it. A quite relevant example is "phantom COVID symptoms", where knowing somebody who you were in contact with that tests positive will immediately make your throat tingle (though no causal claims are made with respect to this)—this literally happens to me every other week. Lastly, they show that this neuro-immune link is quite specific, where ensembles of neurons in the insular cortex (or, insula) that were active during initial infection can trigger a similar immune response when artificially activated, but not via non-specific activation of neurons across the insula. This is literally the insular analog of memory engrams in the hippocampus and amygdala (see [Josselyn 2020][josselyn_2020], shoutout to the 6ix). This really brought home the message for me that we should rethink what "placebo" means, because anything that the brain "sees"—or "thinks it sees"—could cause a very real bodily response, and her talk showed the extent to which this is true.




[link_rolls]: https://rolls.net.technion.ac.il/
[yt_rolls]:https://youtu.be/-7gNchGxI9s?t=614
[josselyn_2020]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7577560/

### Neural Manifolds Plus (TM)

### Unstructured behavior

### Moar humans

### Mechanism AND Computations

### Timescales, oscillations & mechanistic modeling

### DEI discussions

### Snazzy presentations & broader representations


---

## Workshop Report and Reflection: Mechanisms, functions, and methods for diversity of neuronal and network timescales

This whole workshop-planning thing started quite spontaneously: I saw the call on Twitter maybe a week before the deadline, retweeted into the void and tagged [Roxana][tw_zeraati] to see if she would be willing to do this crazy thing together, and 4 months later, we’re in Cascais, sitting at the front of the meeting room as co-organizers in front of some dope speakers.

![fig_speakers][fig_speakers]

I never imagined myself organizing a workshop, much less at Cosyne, it just feels so...grown up? Luckily, the workshop organization wasn’t too much work after we had written up the proposal, especially with several of the speakers volunteering themselves after seeing the tweet. I don’t know if this makes the selection process more equitable, but it certainly made our lives easier. Nevertheless, we filled the rest of the roster while trying to optimize for diversity in a few different aspects, including gender and career stage, as well as to cover the huge breadth of topics that now make up the “field” of neural timescales.

And holy shit it's a **big wide field**.

To provide a bit of context, here was our thought process while deciding on the topics, which became the workshop title:

![fig_wstopics][fig_wstopics]

- **diversity**: we are still amassing data from different brain regions in different model organisms, under different task constraints and from different recording modalities, to see just how timescales vary across space and time. Much of this data attempts to connect with cortical hierarchies (or gradients), as per [Murray et al. 2014][murray_2014] (shoutout to the OG), but also to characterize heterogeneity more generally. Then, we chose 3 broad themes: **function, mechanism, and methods**.
- First, **function** refers to behavioral or “computational” relevance, i.e., why is a diversity or gradient of timescales useful for the organism? The cookie-cutter response that I personally always give is: well, the environment is dynamic and has many temporal hierarchies, so the brain should have the same in order to keep track of them. But this is quite vague, since it doesn’t specify where, how, and what the precise relationship between the two is, e.g., we can remember stuff from many years ago, does that mean there are neuronal dynamics whose timescale is on the order of years, or is there some kind of conversion factor?
- Second, what are the biological mechanisms that gives rise to the heterogeneity of timescales? This can include synaptic, cellular, and network factors that all mix together, which can produce unexpected behaviors. Of course, one has to be careful in linking specific mechanisms to specific observations at different spatial resolutions, which is to say, single neuron spike train timescales probably arise from a different mechanism than population-level timescales, which also differ from membrane timescales, all of which are important and potentially (casually) influence one another.
- Lastly, and implicit in all the above, are **methods** that drive these investigations, and that include both analysis and modeling methods. Fundamentally, are we measuring the same (biological) quantity if the algorithms we use to arrive at those final numbers drastically differ? And this is before even considering modality differences (e.g., continuous time series vs. point processes). This is something that’s good to catalogue, even if we cannot hope to be prescriptive in standardizing them. On the other hand, computational models are always a great model organism for investigating mechanisms that are infeasible to dissect in biological organism, so how do we leverage artificial networks—be it spiking, rate, or deep RNNs—to study these same questions above?


[fig_speakers]: /assets/images/blog/2022-03-28-speakers.png#center
[fig_wstopics]: /assets/images/blog/2022-03-28-wstopics.png#center

You can probably write a whole book, or at least a 10-page review paper, to cover all the works that touch on the above in the last 10 years alone (actually, check [here][soltani_2021] and [here][cavanagh_2020] for exactly such a thing). In the workshop, we got a chance to see some of the newest works in the last couple of years. I’m not gonna be modest about it: it was the best workshop I went to that day (but also probably in general). Not only did our speakers cover the incredibly broad spectrum of topics relating to timescales, there were such rich and unexpected intersections between works that it really felt to me like a coherent and self-organizing emergent entity. It’ll be hard to not completely butcher their findings (so feel free to correct), but in the interest of space, I will just briefly summarize the key points from each talk below, as well as some of the discussions and takeaways points we had. Note that I use third person throughout to avoid switching back and forth between the speaker and their team, just for convenience, but in all cases it was acknowledged just how much of a team effort it really was, from lab tech, research assistants, all the way up to the supervisor.

- [Lucas Pinto][tw_pinto] started the day by blowing my mind with some experimental data that continues to remind me to put some respect on the Experimentalist’s name. In their virtual reality setup, mice have to make a left-or-right decision based on always-present or transient visual information as they run down a virtual track, depending on the specific task. During this, he has the ability to do focal optogenetic inactivation, **but cortex-wide**. This means that during any part of the mouse’s run, he could turn off any part of the brain in a systematic way to see how different regions contribute to, and what their timescales of involvement are in, e.g., visual perception (seeing the pillars), evidence accumulation (”counting” and remembering the quantity), or decision-making (recalling and acting based on the information). More concretely, the question is: if you shut down a part of the brain momentarily, how much does that screw up the mouse’s performance on the task, and for how long into the future is this deficit present for? Somewhat surprisingly, he showed that shutting off almost any part of the dorsal cortex will induce a performance deficit, suggesting that these “cognitive processes” involve multiple brain regions. However, how much and for how long the deficit lasts for depends on the inactivated brain area. Something about this systematic casual manipulation of the brain to interrogate cognitive faculties really blew me away, because you can start to disentangle, among other things, “when” vs. “for how long”, and this is without recording from a single neuron...but, of course, he also showed wide-field calcium imaging data from many cortical areas and recover a hierarchy of activity timescales. There is so much more, so you can check out the tasks [here][pinto_2019], and the main findings [here][pinto_2021], and we even got to see some super cool preliminary data on timescales **across cortical layers**.

- Going from mouse to monkeys, [Ana Manea][tw_manea] showed us some ultrahigh field fMRI data from the monkey brain and related them to the classical single-unit timescales, as well as to functional connectivity gradients. I’m a huge fan of this type of work that bridges modalities, and her data showed that the spiking timescale hierarchy is preserved in fMRI, though I’m very curious what the explicit scaling factor is and whether that’s consistent between macro regions. The power of fMRI, of course, is that you can now look across the whole brain, and they find a smooth gradient (e.g., across the dorsal visual pathway), and not only across the cortex, but in the striatum as well. This is really nice to see, but if you think about it, it doesn’t have to be this way at all, because the temporal dynamics of single-unit or population spiking could be totally different from that of hemodynamics recorded via fMRI, so I really wonder what drives this consistency. On top of that, she showed that functional connectivity gradients (a hot topic in human resting-state fMRI) are correlated with the timescale gradient, wrapping up a nice story connecting spikes, BOLD, and (functional) anatomy. [Her paper][manea_2022] with all the details is hot off the press, so check it out (it’s also super interesting to read the open reviews). One thought I had while listening to her talk the following: was how the  can account for functional connectivity gradients. In particular, two autocorrelated signals tend to have a stronger pairwise correlation just by chance, and these functional connectivity gradients are typically taken as the singular vectors of the resting state BOLD covariance matrix. So how much of the functional connectivity can be expected by the signal statistics of the univariate BOLD autocorrelation alone? Beyond fMRI, she also had some spiking data from the “top” of the hierarchy, including frontal and cingulate regions, but I won’t cover that here other than to say I’m very excited to see more spike-LFP timescale comparisons.

- From there, [Lucas Rudelt][gs_rudelt], who was thankfully able to tag in for his advisor, Viola Priesemann, continued with the theme of crossing scales. He also threw the first punch, so to say, by introducing the concepts of criticality and scale-freeness. Their works start from a complementary and first-principles approach by positing a general process of activity propagation (i.e., branching process), which models how much influence a given “node” in a network (e.g., a neuron) has on its downstream nodes. This influence can be parameterized by a number called the branching ratio, or in their specific case of a neural network, neural efficacy (memory is a bit fuzzy here but I think it’s the same conceptually). Intuitively, a network with low efficacy does not propagate activity very far, or for very long, resulting in shorter timescales. You might expect the converse to also be true, that networks with high efficacy would have long activity timescales. However, among many of the results Lucas talked about, one surprising observation is that it’s a lot more complicated: when networks are set to an efficacy of near 1 (otherwise known as the critical regime), timescales *can* very quickly become longer, but are also more *variable*. In fact, it’s not great if the branching ratio is **at 1**, because then the propagation would explode, so it’s more reasonable to expect that neuronal networks operate slightly below criticality in order to balance information propagation and stability, and this slightly sub-unity region allows the balance to shift dynamically and flexibly. Furthermore, he makes the distinction of intrinsic timescale with information predictability, and find that as timescale increases along the visual cortical hierarchy (in agreement with the original findings [in mouse Neuropixel recordings][siegle_2021]), but information predictability decreases. This is quite puzzling as it contradicts the (more intuitive) notion that longer timescales would translate to higher predictability into the future, since things don’t change as quickly. You can find some examples of their work on this topic [here][wilting_2019], [here][rudelt_2021], and at Cosyne 2022 poster 3-036.

- [Brandon Munn][tw_munn] further represented the scalefree perspective with a tour-de-force overview of his PhD and postdoc works in Sydney. Well, it was both cross-scale and scalefree, in every sense of those words, as he reviewed some earlier works that span from local spiking analysis ([looking at 1/f PSDs][munn_2020]!) to macroscale modeling of the neuromodulatory and thalamocortical systems. Of the latter work done [jointly with Eli Müller][muller_2020], he presented some really interesting results that linked cortical timescales (of fMRI signals) with matrix vs. core populations in the thalamus. **Very** broadly speaking, the thalamus has two types of projections to the cortex: “core” populations have precise targets in the cortex, while “matrix” populations have diffuse cortical projections. Among the many thalamus-cortex associations they find, the most topical was that the cortical gradient of timescale correlated with the level of core vs. matrix projections, i.e., regions with more matrix projections have longer timescales. This brings yet another complicating factor to the mechanisms discussion: in addition to single-cell properties and feedforward- vs. recurrent-dominated local connectivity patterns, the thalamic input may also play a direct role in shaping the “intrinsic” timescale—you might ask yourself at this point, what’s intrinsic at all about intrinsice timescales? Just because monkeys and humans were not enough, Brandon also talked about some exciting new analyses he did with whole brain calcium recordings from zebrafish larvae and Neuropixels recordings from mice. Without delving into the details, he used a procedure called coarse-graining to lump more and more neurons together to see if population timescale lengthens with the number of neurons you pool together. Indeed, it does, and at the risk of putting words into his mouth, I think this raises the possibility that long timescales are maintained not by a specific neuronal population (say, from the association areas), but simply by **more** neurons. More broadly, this loops back the idea of scalefreeness, where spatial correlations scale with temporal correlations, i.e., smaller populations, be it neurons or sand grains, sustain events of shorter durations, and vice versa.

- To close up the morning session, Roxana Zeraati, my co-host, presented her recent works on a new method for robust estimation of timescales, as well as its application to characterize timescale changes from the monkey brain under attention manipulations. Fun story, I was asked to review her method paper more than a year ago, and that’s how I got to know her and her work in the first place, which led to the idea of co-organizing this workshop. The paper is [now published][zeraati_2022] (along with a nice python package, abcTau) so you can just go check it out, but briefly, she tackles the problem of biased estimation of the decay time constant when fitting exponentials to the autocorrelation function, due to various factors such as low spike count, short trial duration, etc. Ideally, we would want both a less biased estimate, as well as a quantification of the uncertainty, especially when the underlying process has multiple timescales. Her approach applies the framework of approximate Bayesian computation (ABC), which has a more modern synonym under [simulation-based inference (SBI)][cranmer_2020] (which, funny enough, is what I work on now with Jakob): ABC (or SBI) takes a generative model, runs many simulations with different parameter configurations, and accepts the parameters that successfully generate simulated data that matches the observed data as the “true” generative parameters. Actually, many methods fit this description, including naive brute-force search, and ABC methods essentially cast the problem into a Bayesian setting (i.e., posterior estimation) and do it in a more efficient way. I won’t bore you with the details, and she actually used most of her talk to showcase the application, I just think it’s a nice method and obviously is very related to the stuff I do now. But the [empirical findings][zeraati_2021] are just as nice: using abcTau, she was able to parse two timescales from single-unit recordings from monkeys doing a selective attention task. She finds that the fast timescale on the order of 5ms (membrane? synaptic?) does not change with attention demands, but the slower one on the order of 100ms does, and builds a computational model to suggest that between-column interactions across the visual cortex can explain the slow timescale change.

By this point, I was pretty thankful that it’s lunch time. If you’re counting, in these first 5 talks, we’ve had 5 different methods for computing neural timescales and even more model organisms. We also saw that timescales not only vary across the cortical hierarchy in a “static” way, but change across layers and over time (whose rate of change can also change), are related to different potential mechanisms (from connectivity to variation in thalamic projection) and cognitive processes (from decision-making to attention), and, just for fun, could potentially scale in a scalefree manner in the goldilock zone of quasi-critical dynamics—and this is just a tiny summary of the data we’ve seen so far. Good thing we got a nice lunch break together and a quick stroll on the beach, which might have been my favorite part of the workshop (more on this later). But back to the science, and in the afternoon session, we were treated with 4 talks with 4 entirely different kinds of computational models, each of which were used to study similar questions of heterogeneity, function, and mechanisms.

- So far, we've talked about neural timescales as useful for implicitly tracking timescales in the environment. [Manuel Beiran][tw_beiran] started the afternoon by making this proposed function very explicit, investigating the conditions and mechanisms that allow rate RNNs to not only track examples of durations, but to generalize to unseen intervals. The task is straightfoward: a network receives an input that encodes the intended interval (either via amplitude or delay), and after a Go-signal, is asked to produce a ramping output for just as long. Unsurprisingly, RNNs (with full-rank connectivity) can learn the examples fine, and could even learn to interpolate across unseen durations within the bounds of the training examples. However, these networks have a hard time extrapolating, whereas networks whose recurrent weight matrix is low-rank could extrapolate. Looking at the dimensionality of the network dynamics, it appears that the low-rank networks essentially keep to a low-dimensional manifold whose geometry is fixed, but the speed at which the dynamics unfold along the manifold changes for the different durations. Relating back to the overall theme, Manuel's talk suggests that connectivity constraints could be a useful thing for networks to be flexible and generalizable in tracking time(scales), instead of falling into tailored solution for individual timescales. Note that the full-rank networks probably **can** find the same solutions, since they are a superset of the low-rank networks, but the latter have an easier time reaching these solutions. [The preprint][beiran_2021] has all the information plus more. I also have to mention Manuel's [older paper][beiran_2019] on disentangling the contribution of adaptation vs. synaptic filtering to network timescales, which I only learned about recently, but has some quite nice (and surprising) results.

- Going from rate to spiking networks, [Alex van Meegen][tw_avm] delved more into the mechanisms of network timescales and leans into statistical physics and mean-field theory to



- Alex van Meegen
- Nicholas Perez Nieves (for Dan Goodman)
- Vy Vo and Shailee Jain

[tw_pinto]: https://mobile.twitter.com/lucasmpinto
[tw_manea]: https://mobile.twitter.com/anamanea10
[gs_rudelt]: https://scholar.google.com/citations?user=wPpGhg4AAAAJ&hl=de
[tw_munn]: https://twitter.com/DrBMunn
[tw_zeraati]: https://twitter.com/roxana_zeraati
[tw_beiran]: https://twitter.com/mBeiran
[tw_avm]: https://twitter.com/AlexVanMeegen
[gs_npn]: https://scholar.google.com/citations?user=OqOeYNoAAAAJ&hl=en
[tw_vo]: https://twitter.com/vvobot
[tw_jain]: https://twitter.com/shaileeejain

[murray_2014]: https://www.nature.com/articles/nn.3862
[soltani_2021]: https://pubmed.ncbi.nlm.nih.gov/34026949/
[cavanagh_2020]: https://www.frontiersin.org/articles/10.3389/fncir.2020.615626/full
[pinto_2019]: https://www.sciencedirect.com/science/article/pii/S0896627319307317
[pinto_2021]: https://www.biorxiv.org/content/10.1101/2020.12.28.424600v2.abstract
[manea_2022]: https://elifesciences.org/articles/75540#content
[wilting_2019]: https://academic.oup.com/cercor/article/29/6/2759/5476016
[rudelt_2021]: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008927
[siegle_2021]: https://www.nature.com/articles/s41586-020-03171-x
[munn_2020]: https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/JP278935
[muller_2020]: https://www.sciencedirect.com/science/article/pii/S1053811920307102
[zeraati_2022]: https://www.nature.com/articles/s43588-022-00214-3
[zeraati_2021]: https://www.biorxiv.org/content/10.1101/2021.05.17.444537v1
[cranmer_2020]: https://www.pnas.org/doi/10.1073/pnas.1912789117
[beiran_2021]: https://www.biorxiv.org/content/10.1101/2021.11.08.467806v1
[beiran_2019]: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006893



[yt_cosyne]: https://www.youtube.com/channel/UCzOTbZTHTubFNjANAR33AAg
[wwn_virtual]: https://www.world-wide.org/cosyne-22/
[tw_talukder]: https://twitter.com/SaberaTalukder/status/1507183050177884162?s=20&t=K17HvJ49jTwOujZuVSzuqA
[tw_zeraati]: https://twitter.com/roxana_zeraati

---
<iframe width="560" height="315" src="https://www.youtube.com/embed/eSRNzNF9rgM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
---
title: 'COSYNE 2022'
tags: [General Science, Reflections, Science Communication]
status: publish
type: post
published: False
header:
  overlay_image: /assets/images/blog/2022-03-28-cosyne22.jpg
  overlay_filter: rgba(0,0,0,0.8)
  teaser: /assets/images/blog/2022-03-28-cosyne22.jpg
classes:
  - wide

toc: true
toc_label: "outline"
toc_icon: "space-shuttle"

permalink: /blog/:year/:month/:day/

excerpt: ""
---

<!-- At the risk of trying to replicate my own success (but also to revive this blog), I will attempt to summarize some of my own takeaways after attending Cosyne 2022. Off the bat, I can tell you that it won't be as comprehensive as the 2019 post. One simple reason for that is just that I didn't go to as many of the talks (lol), so the coverage will not be nearly as comprehensive. In fact, you should by no means read this as an "objective" survey of the topics and trends at the conference, but rather views through a very filtered lens. This is not _just_ a product of my laziness and skipping talks, though. I'll try to convince you later that this was (at least in part) by design. But also, in the 3 years since Cosyne 2019, a lot of things have changed. While the scientific community is still struggling to claw back towards some kind of in-person normalcy, the upside is that most of the conference material exists online in one form or another, even the poster. More importantly, they exist in a systematically-curated and accessible form, in no small part thanks to efforts from the [organizers][yt_cosyne], but also to community-driven initiatives (like [World Wide Neuron][wwn_virtual]) and crazy [individual efforts][tw_talukder], for making such resources available.

The biggest difference between 2019 and 2022, though, is that I'm 3 years older, having existed as a fly on the Cosyne auditorium wall for 3 years longer, and somehow finding myself to be partially integrated into this community. Actually, in true, unbashful, senior(er) academic fashion, I'm going to tell you that the most interesting thing I took away this year was in fact this particular piece of "meta-perspective", and it manifests in several concrete ways:

First, my view of the science that was presented at the conference is now less of a point observation, but one of a noisy trajectory or derivative. Of course, this is something the organizers needed to be aware of in constructing the conference program, and it shows up implicitly in the reviewer biases in which abstracts were accepted (in terms of what's "in" and "out"). I'm sure they talked about what their intentions were at some point, but I somehow managed to miss the opening talk. In any case, I will raise the disclaimer again that this in no way represents a "true" reflection of the conference themes, but my filtered version, so I will only highlight and comment on a few of my personal favorites in Section I.

Second, beyond passively participating in the conference program, I had the good fortune to actively shape a small part of it by co-organizing a workshop with [Roxana Zeraati][tw_zeraati] on neuronal timescales, along with a line-up of fantastic speakers (and cool people). This was obviously a stressful and intense experience, especially on the day of, but it was also exhilarting and insanely productive scientifically. Not only that, I felt like I was able to connect with the people there—_as human beings_—in a much deeper way than I was able to while just chatting at a poster, or casually meeting up as a part of socializing at the conference, especially being a relatively introverted person. In Section II, I will give a brief report of the discussions we had on __"Mechanisms, functions, and methods for diversity of neuronal and network timescales"__, as well as some reflections about my experience as a workshop co-organizer and participant.

Lastly, after 7 years of going to conferences, it finally occured to me in Lisbon that I don't (explicitly) know _how to conference_. There are a lot of conventional wisdoms floating around about how to do conferences, and sometimes specific instructions for specific conferences, most of which about prioritizing watching talks vs. going to posters vs. socializing. A lot of it is helpful, but I think it dawned on me that nobody really ever gave me _systematic_ instructions on **how** to conference, and that probably most people didn't get this as a part of how-to-science manual either. What made me realize this in the first place: that walking around, how my environment interacted with me as a 30-year old postdoc was a lot different than a 27-year old grad student, and so maybe I should approach my environment differently as well? In the last section, I will continue to not give instructions, because there are no one-size-fit-all advice: it's different for different people, at different conferences, and most importantly, at different stages of career. At the same time, lots of people share the feeling that conferences are overwhelming, exhausting, and guilt-inducing. Instead, I will write a bit about this realization, and make some _meta-suggestions_ for how to conference—__in particular, setting goals that are appropriate for your interests, career stage, and personality.__

---
# Section I. Science Highlights
I took notes for some talks and posters. They're not very good notes. To be honest I'm not even 100% confident I left with the correct takeaways, so please feel free to correct me if I misrepresented something. This is partially due to my lack of attention span, but also speaks to the incredible volume of cool works at Cosyne. Actually, a conference like this is really difficult because everything looks interesting and at least tangentially relevant to the broader theme of neural dynamics and computations (at Cosyne? wow dude who would've thought). In the end, instead of _a priori_ choosing what's relevant for me personally, I've basically given up, and instead rely on _learning_ about what interests me the most after the fact, based on which of the talks I happened to be at that inspired the most thoughts. They mostly fell into the following categories (though I had to do some shoe-horning for some):

### I love "weird" stuff
Hands down, what I enjoyed the most are talks that are quite different from the "classical" Cosyne stuff, and I'm very grateful that the organizers decided to include a broader set of topics for the talks. Nothing against PCAs & ANNs, it's just that the bandwidth (i.e., new information per minute) is much higher for talks on weird stuff I've never thought about before. I guess I enjoy the feeling of hearing about ideas that could fundamentally change how I think about something, or just ideas that are so completely unfamiliar to me, that it triggers a novelty reward. Of these, I want to highlight 2 talks and a poster:

[Asya Rolls][link_rolls] talked about the similarities and differences between the nervous system and the immune system, how they both make "memories", and how memories in these two systems interact. She showed results from some super interesting experiments, and her talk is [online][yt_rolls], I highly recommend checking it out because it's quite accessible as an outsider to immunology. In a nutshell, these two systems face similar environmental demands, in that they have to adapt to novel situations, especially situations where remembering how to optimally act might save your life the next time around (think a tiger vs. a novel pathogen). Like a lot of people, I got my Immunology degree from Twitter after COVID vaccines dropped, but never did I think about how the brain might be involved in the immune response (the one thing I study that is suppose to "remember" stuff). Among their crazy results: chemogenetic activation (via DREADD) of dopaminergic neurons in the VTA leads to a more potent lymphocyte response, which (I think shown in a separate experiment) can trigger a pathway through the bone marrow (?!?), resulting in proliferation of cells that can kill tumor cells more effectively after VTA activation. In plain English: when your reward system is activated in coincidence with an immune response to a foreign pathogen, the immune response is stronger next time. On the flip side, you could have an allergic reaction (which is a "rogue" immune response) upon holding fake flowers if you have an allergy to real flowers, just because your brain recognizes it. A quite relevant example is "phantom COVID symptoms", where knowing somebody who you were in contact with that tests positive will immediately make your throat tingle (though no causal claims are made with respect to this)—this literally happens to me every other week. Lastly, they show that this neuro-immune link is quite specific, where ensembles of neurons in the insular cortex (or, insula) that were active during initial infection can trigger a similar immune response when artificially activated, but not via non-specific activation of neurons across the insula. This is literally the insular analog of memory engrams in the hippocampus and amygdala (see [Josselyn 2020][josselyn_2020], shoutout to the 6ix). This really brought home the message for me that we should rethink what "placebo" means, because anything that the brain "sees"—or "thinks it sees"—could cause a very real bodily response, and her talk showed the extent to which this is true.




[link_rolls]: https://rolls.net.technion.ac.il/
[yt_rolls]:https://youtu.be/-7gNchGxI9s?t=614
[josselyn_2020]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7577560/

### Neural Manifolds Plus (TM)

### Unstructured behavior

### Moar humans

### Mechanism AND Computations

### Timescales, oscillations & mechanistic modeling

### DEI discussions

### Snazzy presentations & broader representations
 -->

---

## Workshop Report and Reflection: Mechanisms, functions, and methods for diversity of neuronal and network timescales

This whole workshop-planning thing started quite spontaneously: I saw the call on Twitter maybe a week before the deadline, retweeted into the void and tagged [Roxana][tw_zeraati] to see if she would be willing to do this crazy thing together, and 4 months later, we’re in Cascais, sitting at the front of the meeting room as co-organizers in front of some dope speakers.

![fig_speakers][fig_speakers]
*Note: this slide was from the introduction I gave, updated with our two heroic speakers that subbed in on the day of, circled in green.*

I never imagined myself organizing a workshop, much less at Cosyne. Nevermind the imposter syndrome, this just feels so...grown up? Luckily, the workshop organization wasn’t too much work after we had written up the proposal, especially with several of the speakers volunteering themselves after seeing the tweet. I don’t know if this makes the selection process more equitable, but it certainly made our lives easier. Nevertheless, we filled the rest of the roster while trying to optimize for diversity in a few different aspects, including gender and career stage, as well as to cover the huge breadth of topics that now make up the “field” of neural timescales.

And holy shit it's a **big wide field**.

To provide a bit of context, here was our thought process while deciding on the topics, which became the workshop title:

![fig_wstopics][fig_wstopics]

- **diversity**: we are still amassing data from different brain regions in different model organisms, under different task constraints and from different recording modalities, to see just how timescales vary across space and time. Much of this data attempts to connect with cortical hierarchies (or gradients), as per [Murray et al. 2014][murray_2014] (shoutout to the OG), but also to characterize heterogeneity more generally. Then, we chose 3 broad themes: **function, mechanism, and methods**.
- First, **function** refers to behavioral or “computational” relevance, i.e., why is a diversity or gradient of timescales useful for the organism? The cookie-cutter response that I personally always give is: well, the environment is dynamic and has many temporal hierarchies, so the brain should have the same in order to keep track of them. But this is quite vague, since it doesn’t specify where, how, and what the precise relationship between the two is, e.g., we can remember stuff from many years ago, does that mean there are neuronal dynamics whose timescale is on the order of years, or is there some kind of conversion factor?
- Second, what are the biological mechanisms that gives rise to the heterogeneity of timescales? This can include synaptic, cellular, and network factors that all mix together, which can produce unexpected behaviors. Of course, one has to be careful in linking specific mechanisms to specific observations at different spatial resolutions, which is to say, single neuron spike train timescales probably arise from a different mechanism than population-level timescales, which also differ from membrane timescales, all of which are important and potentially (casually) influence one another.
- Lastly, and implicit in all the above, are **methods** that drive these investigations, and that include both analysis and modeling methods. Fundamentally, are we measuring the same (biological) quantity if the algorithms we use to arrive at those final numbers drastically differ? And this is before even considering modality differences (e.g., continuous time series vs. point processes). This is something that’s good to catalogue, even if we cannot hope to be prescriptive in standardizing them. On the other hand, computational models are always a great model organism for investigating mechanisms that are infeasible to dissect in biological organism, so how do we leverage artificial networks—be it spiking, rate, or deep RNNs—to study these same questions above?

You can probably write a whole book, or at least a 10-page review paper, to cover all the works that touch on the above in the last 10 years alone (actually, check [here][soltani_2021] and [here][cavanagh_2020] for exactly such a thing). In the workshop, we got a chance to see some of the newest works in the last couple of years. I’m not gonna be modest about it: it was the best workshop I went to that day (but also probably in general). Not only did our speakers cover the incredibly broad spectrum of topics relating to timescales, there were such rich and unexpected intersections between works that it really felt to me like a coherent and self-organizing emergent entity. It’ll be hard to not completely butcher their findings (so feel free to correct), but in the interest of space, I will just briefly summarize the key points from each talk below, as well as some of the discussions and takeaways points we had. Note that I use third person throughout to avoid switching back and forth between the speaker and their team, just for convenience, but in all cases it was acknowledged just how much of a team effort it really was, from lab tech, research assistants, all the way up to the supervisor.

---
### Summary of talks
- [Lucas Pinto][tw_pinto] started the day by blowing my mind with some experimental data that continues to remind me to put some respect on the Experimentalist’s name. In their virtual reality setup, mice have to make a left-or-right decision based on always-present or transient visual information as they run down a virtual track, depending on the specific task. During this, he has the ability to do focal optogenetic inactivation, **but cortex-wide**. This means that during any part of the mouse’s run, he could turn off any part of the brain in a systematic way to see how different regions contribute to, and what their timescales of involvement are in, e.g., visual perception (seeing the pillars), evidence accumulation (”counting” and remembering the quantity), or decision-making (recalling and acting based on the information). More concretely, the question is: if you shut down a part of the brain momentarily, how much does that screw up the mouse’s performance on the task, and for how long into the future is this deficit present for? Somewhat surprisingly, he showed that shutting off almost any part of the dorsal cortex will induce a performance deficit, suggesting that these “cognitive processes” involve multiple brain regions. However, how much and for how long the deficit lasts for depends on the inactivated brain area. Something about this systematic casual manipulation of the brain to interrogate cognitive faculties really blew me away, because you can start to disentangle, among other things, “when” vs. “for how long”, and this is without recording from a single neuron...but, of course, he also showed wide-field calcium imaging data from many cortical areas and recover a hierarchy of activity timescales. There is so much more, so you can check out the tasks [here][pinto_2019], and the main findings [here][pinto_2021], and we even got to see some super cool preliminary data on timescales **across cortical layers**.

- Going from mouse to monkeys, [Ana Manea][tw_manea] showed us some ultrahigh field fMRI data from the monkey brain and related them to the classical single-unit timescales, as well as to functional connectivity gradients. I’m a huge fan of this type of work that bridges modalities, and her data showed that the spiking timescale hierarchy is preserved in fMRI, though I’m very curious what the explicit scaling factor is and whether that’s consistent between macro regions. The power of fMRI, of course, is that you can now look across the whole brain, and they find a smooth gradient (e.g., across the dorsal visual pathway), and not only across the cortex, but in the striatum as well. This is really nice to see, but if you think about it, it doesn’t have to be this way at all, because the temporal dynamics of single-unit or population spiking could be totally different from that of hemodynamics recorded via fMRI, so I really wonder what drives this consistency. On top of that, she showed that functional connectivity gradients (a hot topic in human resting-state fMRI) are correlated with the timescale gradient, wrapping up a nice story connecting spikes, BOLD, and (functional) anatomy. [Her paper][manea_2022] with all the details is hot off the press, so check it out (it’s also super interesting to read the open reviews). One thought I had while listening to her talk the following: was how the  can account for functional connectivity gradients. In particular, two autocorrelated signals tend to have a stronger pairwise correlation just by chance, and these functional connectivity gradients are typically taken as the singular vectors of the resting state BOLD covariance matrix. So how much of the functional connectivity can be expected by the signal statistics of the univariate BOLD autocorrelation alone? Beyond fMRI, she also had some spiking data from the “top” of the hierarchy, including frontal and cingulate regions, but I won’t cover that here other than to say I’m very excited to see more spike-LFP timescale comparisons.

- From there, [Lucas Rudelt][gs_rudelt], who was thankfully able to tag in for his advisor, Viola Priesemann, continued with the theme of crossing scales. He also threw the first punch, so to say, by introducing the concepts of criticality and scale-freeness. Their works start from a complementary and first-principles approach by positing a general process of activity propagation (i.e., branching process), which models how much influence a given “node” in a network (e.g., a neuron) has on its downstream nodes. This influence can be parameterized by a number called the branching ratio, or in their specific case of a neural network, neural efficacy (memory is a bit fuzzy here but I think it’s the same conceptually). Intuitively, a network with low efficacy does not propagate activity very far, or for very long, resulting in shorter timescales. You might expect the converse to also be true, that networks with high efficacy would have long activity timescales. However, among many of the results Lucas talked about, one surprising observation is that it’s a lot more complicated: when networks are set to an efficacy of near 1 (otherwise known as the critical regime), timescales *can* very quickly become longer, but are also more *variable*. In fact, it’s not great if the branching ratio is **at 1**, because then the propagation would explode, so it’s more reasonable to expect that neuronal networks operate slightly below criticality in order to balance information propagation and stability, and this slightly sub-unity region allows the balance to shift dynamically and flexibly. Furthermore, he makes the distinction of intrinsic timescale with information predictability, and find that as timescale increases along the visual cortical hierarchy (in agreement with the original findings [in mouse Neuropixel recordings][siegle_2021]), but information predictability decreases. This is quite puzzling as it contradicts the (more intuitive) notion that longer timescales would translate to higher predictability into the future, since things don’t change as quickly. You can find some examples of their work on this topic [here][wilting_2019], [here][rudelt_2021], and at Cosyne 2022 poster 3-036.

- [Brandon Munn][tw_munn] further represented the scalefree perspective with a tour-de-force overview of his PhD and postdoc works in Sydney. Well, it was both cross-scale and scalefree, in every sense of those words, as he reviewed some earlier works that span from local spiking analysis ([looking at 1/f PSDs][munn_2020]!) to macroscale modeling of the neuromodulatory and thalamocortical systems. Of the latter work done [jointly with Eli Müller][muller_2020], he presented some really interesting results that linked cortical timescales (of fMRI signals) with matrix vs. core populations in the thalamus. **Very** broadly speaking, the thalamus has two types of projections to the cortex: “core” populations have precise targets in the cortex, while “matrix” populations have diffuse cortical projections. Among the many thalamus-cortex associations they find, the most topical was that the cortical gradient of timescale correlated with the level of core vs. matrix projections, i.e., regions with more matrix projections have longer timescales. This brings yet another complicating factor to the mechanisms discussion: in addition to single-cell properties and feedforward- vs. recurrent-dominated local connectivity patterns, the thalamic input may also play a direct role in shaping the “intrinsic” timescale—you might ask yourself at this point, what’s intrinsic at all about intrinsice timescales? Just because monkeys and humans were not enough, Brandon also talked about some exciting new analyses he did with whole brain calcium recordings from zebrafish larvae and Neuropixels recordings from mice. Without delving into the details, he used a procedure called coarse-graining to lump more and more neurons together to see if population timescale lengthens with the number of neurons you pool together. Indeed, it does, and at the risk of putting words into his mouth, I think this raises the possibility that long timescales are maintained not by a specific neuronal population (say, from the association areas), but simply by **more** neurons. More broadly, this loops back the idea of scalefreeness, where spatial correlations scale with temporal correlations, i.e., smaller populations, be it neurons or sand grains, sustain events of shorter durations, and vice versa.

- To close up the morning session, Roxana Zeraati, my co-host, presented her recent works on a new method for robust estimation of timescales, as well as its application to characterize timescale changes from the monkey brain under attention manipulations. Fun story, I was asked to review her method paper more than a year ago, and that’s how I got to know her and her work in the first place, which led to the idea of co-organizing this workshop. The paper is [now published][zeraati_2022] (along with a nice python package, abcTau) so you can just go check it out, but briefly, she tackles the problem of biased estimation of the decay time constant when fitting exponentials to the autocorrelation function, due to various factors such as low spike count, short trial duration, etc. Ideally, we would want both a less biased estimate, as well as a quantification of the uncertainty, especially when the underlying process has multiple timescales. Her approach applies the framework of approximate Bayesian computation (ABC), which has a more modern synonym under [simulation-based inference (SBI)][cranmer_2020] (which, funny enough, is what I work on now with Jakob): ABC (or SBI) takes a generative model, runs many simulations with different parameter configurations, and accepts the parameters that successfully generate simulated data that matches the observed data as the “true” generative parameters. Actually, many methods fit this description, including naive brute-force search, and ABC methods essentially cast the problem into a Bayesian setting (i.e., posterior estimation) and do it in a more efficient way. I won’t bore you with the details, and she actually used most of her talk to showcase the application, I just think it’s a nice method and obviously is very related to the stuff I do now. But the [empirical findings][zeraati_2021] are just as nice: using abcTau, she was able to parse two timescales from single-unit recordings from monkeys doing a selective attention task. She finds that the fast timescale on the order of 5ms (membrane? synaptic?) does not change with attention demands, but the slower one on the order of 100ms does, and builds a computational model to suggest that between-column interactions across the visual cortex can explain the slow timescale change.

By this point, I was pretty thankful that it’s lunch time. If you’re counting, in these first 5 talks, we’ve had 5 different methods for computing neural timescales and even more model organisms. We also saw that timescales not only vary across the cortical hierarchy in a “static” way, but change across layers and over time (whose rate of change can also change), are related to different potential mechanisms (from connectivity to variation in thalamic projection) and cognitive processes (from decision-making to attention), and, just for fun, could potentially scale in a scalefree manner in the goldilock zone of quasi-critical dynamics—and this is just a tiny summary of the data we’ve seen so far. Good thing we got a nice lunch break together and a quick stroll on the beach, which might have been my favorite part of the workshop (more on this later). But back to the science, and in the afternoon session, we were treated with 4 talks with 4 entirely different kinds of computational models, each of which were used to study similar questions of heterogeneity, function, and mechanisms.

- So far, we've talked about neural timescales as useful for implicitly tracking timescales in the environment. [Manuel Beiran][tw_beiran] started the afternoon by making this proposed function very explicit, investigating the conditions and mechanisms that allow rate RNNs to not only learn examples of durations, but to generalize to unseen ones. The task is straightfoward: a network receives an input that encodes the intended interval (either via an amplitude, or a delay between two pulses), and after a Go-signal, is asked to produce a ramping output for just as long. Unsurprisingly, RNNs can learn the examples fine, and could even learn to interpolate across unseen durations within the bounds of the training examples. However, networks with full-rank connectivity have a hard time extrapolating, whereas networks whose recurrent weight matrix is low-rank could extrapolate (with some help from a context cue). Looking at the dimensionality of the network dynamics, it appears that the low-rank networks essentially keep to a low-dimensional manifold (!!) whose geometry is fixed, but the speed at which the dynamics unfold along the manifold changes for the different durations. Relating back to the overall theme, Manuel's talk suggests that connectivity constraints could be a useful thing for networks to be flexible and generalizable in tracking time(scales), instead of falling into tailored solution for individual timescales. Note that the full-rank networks probably **can** find the same solutions, since they are a superset of the low-rank networks, but the latter have an easier time reaching these solutions (for whatever reason). [The preprint][beiran_2021] has all the information plus more. I also have to mention Manuel's [older paper][beiran_2019] on disentangling the contribution of adaptation vs. synaptic filtering to network timescales, which I only learned about recently, but has some quite nice (and surprising) results.

- Going from rate to spiking networks, [Alex van Meegen][tw_avm] delved further into mechanisms by providing a theory to predict single neuron timescales. I'm not gonna lie, there was a lot of **heavy duty** math that I basically have no hopes of understanding, probably ever. But Alex framed his talk quite intuitively, and the question he poses is deceptively straightforward: can we predict the timescale of single neuron spiketrains given neuronal parameters and network connectivity, as well as the temporal statistics of the external input? In particular, he highlighted the question of how neurons embedded in a network can acquire much longer timescales than set by their membrane time constants. The contribution of the work is that, instead of brute-forcing it numerically, he worked out a theory that *analytically* connects underlying parameters to observables. If I understand correctly, the dynamical mean-field theory "squishs" the network of neurons into one "big neuron" whose distribution of input and output statistics (e.g., mean firing rate, ISI, and autocorrelation) can be described by stochastic differential equations, and squishing them this way is okay because the input and output are "self-consistent" (I'll stop here before I embarrass myself more). He applies it to several different types of neuronal models, including generalized linear models with various nonlinearities, as well as leaky integrate-and-fire neurons with different connectivity structures, and finds good agreement with simulations. All the findings are in the recently [published paper][vanmeegen_2021], which he did not have time to fully cover in a 20-min talk. One point he stressed was that the theory only predicts the average timescale (and distribution of quantities) across all the neurons in the population, *not the timescale of the average population activity*, which are markedly different (e.g., Fig. 10 e vs. f in the paper). I found this particularly interesting because the former corresponds to single-neuron timescales measured via spiketrain autocorrelations (e.g., in Murray et al., 2014), while the latter resembles something more like the LFP, or more directly, summed pre-synaptic input into the neuron. But these two quantities are suppose to be self-consistent, so the synaptic or neuronal membrane filter is doing a ton of work to make this happen? 

- [Nicolas Perez-Nieves][wb_npn], heroically stepping in remotely an hour before his supervisor, Dan Goodman (who seemed to have had just the worst luck in the days leading up to Cosyne), was schedule to talk, took yet another complementary modeling approach. In his work, he trains spiking neural networks— a difficult feat in of itself that requires a clever surrogate gradient descent technique—to do a range of classification tasks with temporal structures. Typically, training such networks, whether spiking or rate-based, means adjusting only the connection weights in order to optimize for task performance, but not here. Nicolas asks whether heterogeneity in single-neuron parameters, specifically their membrane time constants, can improve task performance. In this context, heterogeneity means that every neuron in the recurrent network is allowed to take on a different value for its time constant, instead of all being the same. This heterogeneity is implemented either through initialization alone (and then fixed), through learning, i.e., the neuron-to-neuron weights, along with single-neuron time constants, are learned through back-propagating the task performance loss, or both. This technically sophisticated (and apparently computationally intensive) setup lead to some very intuitively satisfying results: learned heterogeneity in single neuron time constants led to better performance in all the tasks, and their distribution roughly matches experimentally observed gamma distributions of membrane time constants measured in real neurons. In addition, a small number of neurons seem to consistently acquire very long timescales, e.g., 100ms compared to the median of ~10ms. There are a lot of interesting things to consider when extrapolating from these results to the real brain. For example, neurons in the brain probably don't (and can't) tune their membrane time constants for each task they face, but the heterogeneity already exists, or at least changes on an evolutionary timescale—so then how is this heterogeneity taken advantage of per task? Does this help explain functional specialization of different brain areas, since neuronal (and network) properties in different regions are more or less defined after early development? They touch on both points (and more) in the recently [published paper][pereznieves_2021].

- Last, but not least, [Vy Vo][tw_vo] and [Shailee Jain][tw_jain] gave a jointly pre-recorded talk about a set of very fascinating and interdisciplinary works they did on augmenting machine learning-style RNNs (i.e., LSTM) (**with timescales!**) to better perform language tasks, and then using them as a model of natural language processing in the brain. They start from the observation that natural languages, like many other processes in our environment, has a hierarchy of timescales (e.g., phonemes to words to sentences). RNN-based language models usually use a gated "neuron", like the long short-term memory (LSTM) unit, which has an internal memory that decays with some time constant, which is useful for capturing long-range dependencies. Because I'm learning the German language, it's the only example I can think of, where a question usually has the verb—a **critical** piece of information—at the very end. To parse a question, you would then need to "hold onto" information, like the subject and object and wheres and whens, until the very end, when it's clear what the requested action is. In the first part, [they show][vo_2020] that when LSTM units are assigned a distribution of timescales mimicking that of natural language (i.e., power law), these "multiscale" networks result in better language modeling performance, especially for rarer words. Pretty cool already, but [it gets better][jain_2020]: they can now give the same text input to the network as what a human reads inside a MRI scanner, and compare network activation to brain activity. Specifically, they try to predict BOLD timecourse at each voxel in the human brain using a weighted combination of the activation of all the units in the RNN. Surprisingly (at least to me), they find that activations of voxels in the temporal lobe (language-y bits?), prefrontal cortex (think-y bits?), and the precuneus can be predicted particularly well. What's more, they extract an "effective timescale" for each voxel by taking a weighted average of the timescale of the units in the RNN: if a unit is particularly predictive of the activity in a voxel, that unit's timescale would be weighted high. Through this procedure, they find that the auditory cortex has short timescales, which increases as you move towards parietal areas (TPJ), and there's a similar gradient along posterior-to-anterior PFC. Some natural questions arise here: how do the RNN-derived voxel timescales compare to those computed based on the BOLD ACF (like in the previous talks), and specifically, how are they different, and also how does the resting state timescale compare with these task-derived ones? Anyway, I think this is a super cool intersection of machine learning and neuroscience research (as well as collaboration between academic and industry labs), where the ML model actually benefited from a brain-inspired architecture while also assisting in explaining brain data.

---
### A lot of questions moving forward
I think my brain was properly scrambled, or rather, has exploded and smeared the workshop meeting room  after that whole day of talks (and I had said about as much in our final discussion). It's the feeling of suddenly experiencing so many new things that one has a tough time keeping track of any single discussion, let alone how they intersect with one another. Fortunately, it seemed like I was not the only one, and people mostly shared the sentiment that this was a *good thing*: that the talks and discussions at the workshop generated so many new leads and potentially new perspectives that it's hard to consolidate them into a coherent sequence of thoughts. In the two weeks following, I was able to take some time to digest these new ideas, and going over my notes for all the talks (and their associated papers) for this blog post really helped in picking up the pieces (of my brain) after the dust settled, and I find them falling into the same themes we've set out in the beginning. I hereby present you the pieces of my brain, which, of course, are heavily inspired by the workshop speakers and attendees, and in particular recent conversations with [Matteo Saponati][tw_saponati] and [Alana Darcher][tw_darcher]:

- **methods for measuring timescales:** implicit but central in all the discussions we've had is a reliance on faithful measurements of timescale, and we already had a very concrete demonstration (in Roxana's talk) about how naive loss-minimizing estimates from fitting an exponential decay function to the autocorrelation may be biased, as well as how more sophisticated methods (or fitting multiple timescales) could lead to different and novel interpretations of the same data. This is a huge issue, because if the ruler you're using to measure stuff is messed up, then any conclusions you drawn about whatever you're measuring is bound to be messed up. This much is obvious. But what's not obvious is the extent to which this is a "problem", and if so, how to fix it. Like I said before, different data modalities (spike times vs. continuous voltage recordings) has a different set of challenges for doing this right, even if we keep to the same approach of estimating an empirical autocorrelation function and then fitting a (or several) decay time constant(s). In this particular case, there's probably enough statistical theory to guide unbiased measurements, but it's the wild wild west when you start folding in other metrics like the integral under the ACF, full-width half maximum, or time-to-first-zero-crossing, let alone more complex measurements based on, e.g., delayed mutual information. It's likely that these metrics will partially overlap in what they're trying to measure, and it's a certainty that they will give different answers when applied to the same data. The question is, do we need a set of guidelines for what to use when, a "dictionary" that maps the relationship between these metrics under different scenarios (empirical or analytically derived), or otherwise some kind of sensitivity or surrogate analysis procedure that quantifies how differently the conclusion could be. Further complications arise when you have to make model decisions without knowing the underlying generative process, e.g., whether you fit a cosine term to the ACF will depend on whether you think there's an underlying oscillation. I don't know what the right answer is, but I feel like this should be *priority 1A* before we accumulate too much literature based on shaky timescale estimates without good error bounds. A related issue is just being aware of what exactly is the process whose timescale you're measuring. Spikes are different from LFPs are different from calcium dynamics are different from fMRI BOLD signals, and it would be great if we can avoid using "neural timescales" as some nebulous catch-all thing to loosely support theories at vastly different scales without specifying the plausible bounds of these modalities.
  
- **static or dynamic, when and where:** when I first got interested in timescales, it was kind of "obvious" in some sense what to expect, namely, cortical timescales are static properties of neurons (or neural populations), and should follow a cortical hierarchy that increases from sensory to association areas (minus the functionally dynamic bit...). This had enough empirical evidence from several (sparsely sampled) single-unit studies, as well as task fMRI studies, but more convincingly (for better or worse), agreed with a simple and intuitive idea. But in the last few years, even the idea of cortical hierarchy has evolved significantly, partly driven by our ability to measure *large-scale cortical gradients* in many modalities, including structural and dynamical brain variables. I think the timescales gradient concept still holds, but is likely to be the zeroeth-order approximation to reality. That was a long winded way to say, at this point, **I have no idea nor ideological commitment** to where, when, and how neural timescales change, whether the underlying covariate is spatial location, (interally generated) behavioral state, task-associated demands, or some other thing. Well, I guess what I can say for certain, especially after the workshop, is that timescales of *anything* is likely to be dynamic than static, especially when you throw enough different contextual covariates in the bag. Is this surprising? Not really, because the brain is a non-stationary dynamical system that has to somehow reflect, process, or otherwise be coupled to another non-stationary dynamical system that is our world, and static timescales would pose quite a limitation in its representational capacity. A more fruitful question might be: **under which scenarios *are* timescales static (or invariant)?** In other words, if timescale is the ruler, then which *things*—and things can include any abstract quantity such as brain regions, cell-types, or tasks—are the "same", and by extension, which things are "similar"? This is simultaneously a trivial and a complex question: trivial because if you take the perspective that timescale is just another characterization (or summary statistic) of neural dynamics, then we already know what to do with it. Afterall, systems neuroscience has lots of such measurables. Take the simplest one: stimulus-evoked firing rate. When a neuron increases in firing rate, we say that it's *tuned* to (properties of) the stimulus that triggered that response, and this tuning is often graded. From this perspective, we can churn through our normal pipeline, and the only difference is, instead of characterizing how strongly a neuron is responding to something—which has an intuitive though arguably questionable interpretation—we are talking about *how long* it's responding to something. You can even go second-order and characterize how timescales change, or its rate of change. As long as we can be relatively certain that our timescale estimates are good, we can do this ad-nauseum and still provide scientific value by gathering a collection of such data. But as much as I'd like "timescales" to be the 2020s equivalent of early-2000s-fMRI, the complex part of that question is: wtf does it actually mean?

- **linking measurements to theories:** well, wtf **does** it actually mean? In other words, having gathered all this data about how timescales vary under which circumstances, what do we learn about the brain? My default philosophical response is: what does measuring firing rates teach us about the brain? My default cynical response is: we just need a catchy one-liner theory about what timescales "represent" for it to take off. But seriously, what use is it? Well, with my small ideological bias, I think measuring the time constant of dynamics available to a dynamical system is a necessary step in characterizing and understanding this dynamical system, but we don't even really have to drink the dynamical systems Kool-Aid. If you stick with the broader perspective of system identification, we can cross reference a much older body of literature that looks at neural systems as input-output transformations. In that framework, looking at the timescale (of the impulse response, for example) is as natural as characterizing its resonance frequency or spatiotemporal filter, and I'm almost certain Walter Freeman III or somebody has done this 30+ years ago. My secret agenda is to take computational neuroscience **back** to the golden ages of FFTs and linear systems analysis, but if you want to go the modern route, here's something for you too: computation-through-dynamics requires, well, dynamics, and the characteristic timecourse of a particle traversing through the neural population state space is **probably** quite important. I mean I didn't come up with this just now, there is a body of literature on this as well (e.g., [Runyan et al., 2017][runyan_2017]), and it's also something that some of our workshop speakers are actively looking into. For a much more eloquent and broad coverage of specific examples related to this issue, please read this [great paper][gjorgjieva_2016]

- **mechanisms: biological and otherwise:** my personal philosophy here is that timescales is a readout (or observable) of a system whose latent (and biologically mechanistic) variables are the quantities we're truly interested in. What does this mean, exactly? Practically, it means using the more nebulous "neural timescales" measurements—specifically, the decay constant of spike train and LFP autocorrelations—to understand parameters of the neural circuit that *you cannot measure*. This can include time-related quantities such as membrane timescales, synaptic timescales, and timescales of other processes like adaptation driven by a combination of physical processes (like calcium fluctuation), but also network topologies, ratios of cells with different timescales, etc. Again, I'm not saying this is **the right way** to think about it, I just personally tend to gravitate towards these measurables as "**physiologically interpretable biomarkers**", because it's easy to imagine how such indirect inferences (when valid) can be of great value downstream, both scientifically and clinically. One could also ask what are the **mechanisms** that gives rise to the timescale observations we have, which is a different but very much related question. More strictly speaking, if we want mechanisms, an observable like neural timescales should be used to constrain (or even more explicitly, rule out) theories about plausible biological mechanisms, i.e., the timescales we measure in the brain are what they are, but which of the hypothesized mechanisms are implausible in generating these observations? This is a much broader point about science, and it's quite a big ask, but in the ideal case, seeing observations that are plausible under one hypothesis does not (and cannot) confirm that hypothesis, even though that's what we (myself very much included) often do. That aside, we can also use timescales to constrain non-biological hypotheses, and I'm thinking along the lines of information propagation models that Lucas and Brandon talked about. Going this route, we can sketch a model of the brain that is totally non-biological (I guess you can say a model at the algorithmic or computational level, if you want), but nevertheless use timescales and any other readout you can get your hands on to constrain the model, or plausible parameter regimes of the model. As an example, if we model spike propagation as a branching process, are our observations consistent with a system near criticality? Does it *rule out* sub- or supracritical regimes? From that perspective, timescales are not any more special than any other readout of the circuit, certainly not epistemologically (looking at you, firing rate), and by analogy, it could be as simple as the higher moments of a distribution: when mean and variance are not enough, maybe the skew would be useful in distinguishing between two hypothesized distributions, that's all.

- **consequences (and the f-word):** the f-word here being *function*, which I would like to avoid because it presupposes some kind of intention (but it's in the workshop title, I know...). It's possibly (*probably*) an inconsequential semantic debate, I acknowledge this, but it's my blog, and if you don't like it, you can function-off (just kidding). I am happy to talk about consequences, however, both in terms of biology, as well as computations and behaviors. We saw many examples in the workshop where neural timescales could potentially implement some computation (the most explicit one being Manuel's timing networks), and it's easy to imagine hypothetical scenarios where certain autocorrelation structure in the spike train might selectively activate downstream neuronal or network processes that are "tuned" to a timescale (think spike-timing dependent plasticity). Actually, correlated neural code is a thing, so why not *autocorrelated* neural code (credits to Matteo for co-coining the term, though it's possibly [a re-invention][holden_2004]). Here, the difficult part is determining whether a particular observation about timescales is necessary to induce the downstream consequence, or whether it's merely a reflection of something else. As a simple concrete example, are single neuron spiketrain timescales important given the population spiketrain timescale? In other words, it's difficult to isolate a single aspect of the spike train statistics (or LFP, or whatever), be it firing rate or timescales, while holding all else constant, and argue that *that* was really the crucial ingredient. This strict counterfactual requirement is basically impossible to realize in-vivo, so we try to make inference from evidence in the reverse direction, which is less powerful but obviously a lot more feasible, i.e., perturbing behavior through task design and seeing how it affects timescale measurements, and in stronger cases, even perturbing the brain directly (like Lucas' experiments in the first talk). I can sit here and write for the next 3 hours potential ways how cognitive and behavioral variables could be correlated to or be affected by neural timescales. Many of those will be proven wrong, but a small subset of *a lot* is still a lot, and I'm not sure if that's the regime we should head towards, i.e., correlate timescale changes with everything under the sun, as we're prone to do (oscillations and fMRI are just two examples that come to mind). At the same time, I don't have a better idea of what to do, other than perhaps supplementing behavioral experiments with building explicit dynamical models of computation, in which one *can* control neural timescales (whichever measure you choose) while holding all other constant...
  
- **understanding through building:** ...which brings me to the point of computational models. Maybe it's the engineer in me, but I really enjoyed all the different flavors of models in the afternoon session of the workshop, perhaps as a result of the sense of control they give us. Models give us the ability to operationalize these hypothetical counterfactuals in a straightforward way, i.e., if we have a model that performs some computation through its dynamics, then does (for example) the presence of heterogeneous timescales matter? In both Nicolas' spiking networks, as well as Vy and Shailee's LSTMs, the answer seems to be a resounding yes, since one can simply setup the alternative by giving the units homogeneous membrane timescales. Does that mean a spiking network or LSTM cannot do the task equally well without heterogeneous timescales? No. But we could potentially say something like, given the same learning mechanism or even the same learned connectivity, one architecture is superior than the other in the context of a specific task. Of course, when possible, one should also give the model *other* mechanisms that can plausibly result in similar computational benefits. Even though this is not necessarily always the practice in modeling, I think it could lead to some very interesting research questions, i.e., are there degenerate mechanisms that can complement or replace heterogeneous timescales in making network dynamics and computations robust? A further difficulty, in this particular case, is that constraining the timescale parameters of the model is easy, even if it's biologically unrealistic, because they're often explicitly parameterized (like, it's an array you put int). However, constraining the timescales of the *outputs* of the model, which is the quantity we can observe in our experiments, is more difficult. This would be easier if we knew which model parameters affects which aspects of model output, but that takes us back full-circle to the question of mechanisms. Regardless, it's important to differentiate the timescales of the inputs, the model parameters, and the model's output, and be vigilant about possibly confusing one another (I know I've made this mistake several times).

- **scalefull or scalefree; exponential or power law?:** as an 1/f afficionado, the final point I want to explicitly bring up is this tension between characteristic timescales and scalefree/self-organized criticality theories of the brain. I'm not sure if I'm hitting a wall where there is none, and it's not really a well-defined question, but I have a funny feeling in my stomach when I think about harmonizing these two conceptual frameworks (see this [review paper][marom_2009] for a very nice treatment of this discussion): on the one hand, exponential decay is a very common phenomenon in nature. Or, rather, many natural processes are well approximated by an exponential decay, that being the stable solution to a system of linear differential equations. On the other hand, many processes in nature are fractal, or scale-free, and can be captured by power law distributions of sizes, durations, or energy, while the generative model that produces power law observatons—systems at or near criticality—is theoretically attractive for many reasons, such as the balance between sparsity and robustness. Are these two perspectives at odds with each other? Not necessarily, as one could simply take a power law distribution of timescales to realize scale-freeness (similar to the multi-timescale LSTMs), which amounts to stacking up a lot of exponential decays of various time constants. While this is one plausible explanation, and it works in parameterizing the system in the way we want independently of the potential mechanisms that give rise to it, it feels a little "engineered" or contrived as a theory. Afterall, who's behind the scenes setting up such a nice distribution? Alternatively, there are theories (mainly from statistical physics and the complex systems tradition) that could more parsimoniously explain the power law observations, by which I mean with fewer hand-tunable degrees of freedom in the generative model. But in that realm, my impression is that people often use power law distribution (of, for example, timescales) as *evidence* of their claim that the system is at criticality, which would be circular if we then used that as an explanation. Lucas' generative model and Brandon's coarse-graining analysis came together nicely to give this perspective some weight, but I wonder how it would connect with the rest of the (more vanilla, if you will) observations? Maybe our theories should specify when and where the brain is at criticality, and when it is, what kind of timescale (and other) observations we should expect? How many neurons should we record to have confidence in falsifying the theory one way or another? But more importantly, do any of the entities we measure, like a neuron, actually *have a "characteristic timescale"*? Sure, the RC-circuit equations capture single neuron membrane dynamics well, from which a time constant just falls out, but perhaps the multitude of network mechanisms then ensure that the neuron never "operates" at its fixed timescale? 

---
### Some (potentially obvious) reflections as a participant and co-organizer
Alright, I think I'm just throwing out wild thoughts at this point, and I've covered most of the scientific points I wanted to cover. If you have thoughts on any of these issues, I would be very happy to hear about them. I just want to end this blog post with some final thoughts about the workshop experience. 

![fig_larry][fig_larry]

Hands down, our workshop was the best part of Cosyne for me. It kinda started out as a joke, but I truly believed it by the end. It's not that the main conference experience was bad by any means, it was just...looser. The talks were interesting, and so were the interactions at the posters, but there wasn't a narrow thematic constraint. This is obviously by design, and in some sense I'm stating the obvious: broadly interesting conference is broad. Wow, much insight dude. 

Yes, it is obvious, and I would not be saying this if not for the fact that the workshops, as a whole, were equally diverse in topics, and one does not have to sit in the same room the entire day. In fact, in my previous years at the Cosyne workshops, I've always hopped from one room to another constantly trying to see talks that were maximally overlapping with my interest, or was the hot topic du jour. Obviously, I couldn't do that this year as a co-organizer sitting in the front, but I think it actually worked out for the better. Perhaps this is also obvious, but I think the workshops is truly a scenario where the **whole is more than the sum of its parts**. When you see individual talks, you hear about the specifics of the experimental design or model implementation, and some background information to motivate the work. When you see a whole set of talks on the same topic, even if some of those talks lie squarely outside your domain of interest and expertise, you still get to see much more clearly the things that were in common, i.e., the shared motivation and context, the things that are possible, as well as what's **not** talked about, i.e., gaps in thinking or even opportunities for future research. Clearly, the panel of speakers did not include everyone in the world working on timescales. But if your sample contains 10 people that come from vastly different perspectives, and 8 of them mention the same stuff or acknowledge the same issues, or, maybe are even puzzled by the same observations and questions, then there's something interesting going on. Furthermore, the fact that all the talks are on the same day makes it just much more likely that interesting stuff will get into your head by osmosis (at least that was my experience). I would say that attending the entirety of a workshop is the most efficient way to get familiarized with a subfield or topic as a novice, except that wouldn't even be true: I think even "experts"—actually, especially experts—would benefit greatly from seeing a familiar issue from a different perspective. I definitely don't consider myself an expert, but I learned something new from basically every talk, and I don't just mean a piece of scientific finding, but more so a new way of thinking about things, timescales and beyond. I suppose this could partially be due to our explicit intention in sampling a diverse range of perspectives when selecting the speakers, but my guess is that even if you sat through 2 days of people going through competitive models in the BrainScore workshop (no shade), you'd still learn something on the meta-level. 

On top of that, some things I had to do as a co-organizer ended up really adding to the experience. Beforehand, Brad gave me some really helpful advice that, as an organizer, I should take notes, always prepare a few questions for the speaker in case nobody else asks questions, and to otherwise appear engaged. These are all sensible and obvious things to do, and I probably would have tried to do them anyway if for nothing other than the optics, but to no one's surprise, appearing engaged (and not being on my phone) actually makes you engaged, and when you're engaged, you take away a lot more. In addition, when you then try to prepare questions that are not completely superficial, it requires actively listening to the talk and thinking beyond what the speakers have presented. Obviously, nobody prevents you from doing this even if you're not an organizer, but this is extremely difficult to sustain throughout the whole day, and if I didn't have to, I probably wouldn't have, just because it's quite easy to slip into "well let me just check Twitter for a second". Actually, as an audience, I feel like sometimes you don't want to be *that person* that has a question at the end of every single talk, which is exactly who you want to be as the organizer. Another thing that helped was that Roxana and I had to come up with good questions or prompts for the two discussion periods, and that required another round of active thinking and bouncing ideas off of each other, as well as generally trying to tie everything together. Thankfully, this brain expenditure happened against the backdrop of beach and sun. 

One last thing I want to mention, and probably the most important: being a part of the same workshop for the entire day gives you a chance to interact with the speakers and other participant in a way that's more natural than asking a question at the end of a talk, at least to me. I'm probably not in the minority when I say that I feel very awkward approaching somebody I don't know to have a conversation at a conference, even more so when it's not about the immediate scientific subject at hand. At the same time, the conversations I enjoyed the most were not really about the talk we just heard, or even timescales, but about life, family, career in academia, the war in Ukraine, film, surf, or random tidbits of culture, be it about Canada, California, or Tubingen. For me, there's really just no good way to transition from "I have a follow up question about something you mentioned in your talk" to, well, I don't know what, but something unrelated and spontaneous. When you hop in and out of workshops, you're one of many faces that somebody sees. When you stay at a workshop and repeatedly ask questions or engage in the discussion, you become somebody I know. I realize much of this is due to the fact that I was in the privileged position of a co-organizer, it was within our roles to mingle with speakers, as well as get everyone to meet and talk to each other. Because of this, we had the opportunity to ask everyone to have lunch together, so we could at least get to know each other a bit outside of the "classroom", so to say. I really enjoyed this casual interaction, as well as dinner and drinks, and follow-up conversations I had with them afterwards. I think we could've done a better job in involving the non-speaker participants (who seemed clearly engaged and interested) in the more casual follow-up interactions, like going out for dinner, so I think I will be mindful of that if I was in such a position again, both for professional networking and just connecting potentially like-minded people.

Anyway, I obviously realize that not everyone will have an opportunity to be accepted as a workshop organizer, and I don't want to necessarily prescribe this approach for anyone. But I think from now on, as a participant, I will try to pick a single workshop for the entire day, and just stay there and learn as much as possible.



[tw_pinto]: https://mobile.twitter.com/lucasmpinto
[tw_manea]: https://mobile.twitter.com/anamanea10
[gs_rudelt]: https://scholar.google.com/citations?user=wPpGhg4AAAAJ&hl=de
[tw_munn]: https://twitter.com/DrBMunn
[tw_zeraati]: https://twitter.com/roxana_zeraati
[tw_beiran]: https://twitter.com/mBeiran
[tw_avm]: https://twitter.com/AlexVanMeegen
[wb_npn]: https://neural-reckoning.org/nicolas_perez.html
[tw_vo]: https://twitter.com/vvobot
[tw_jain]: https://twitter.com/shaileeejain
[tw_saponati]: https://twitter.com/matteosaponati
[tw_darcher]: https://twitter.com/AlanaDarcher

[murray_2014]: https://www.nature.com/articles/nn.3862
[soltani_2021]: https://pubmed.ncbi.nlm.nih.gov/34026949/
[cavanagh_2020]: https://www.frontiersin.org/articles/10.3389/fncir.2020.615626/full
[pinto_2019]: https://www.sciencedirect.com/science/article/pii/S0896627319307317
[pinto_2021]: https://www.biorxiv.org/content/10.1101/2020.12.28.424600v2.abstract
[manea_2022]: https://elifesciences.org/articles/75540#content
[wilting_2019]: https://academic.oup.com/cercor/article/29/6/2759/5476016
[rudelt_2021]: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008927
[siegle_2021]: https://www.nature.com/articles/s41586-020-03171-x
[munn_2020]: https://physoc.onlinelibrary.wiley.com/doi/full/10.1113/JP278935
[muller_2020]: https://www.sciencedirect.com/science/article/pii/S1053811920307102
[zeraati_2022]: https://www.nature.com/articles/s43588-022-00214-3
[zeraati_2021]: https://www.biorxiv.org/content/10.1101/2021.05.17.444537v1
[cranmer_2020]: https://www.pnas.org/doi/10.1073/pnas.1912789117
[beiran_2021]: https://www.biorxiv.org/content/10.1101/2021.11.08.467806v1
[beiran_2019]: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006893
[vanmeegen_2021]: https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.043077
[pereznieves_2021]: https://www.nature.com/articles/s41467-021-26022-3
[vo_2020]: https://iclr.cc/virtual/2021/poster/3095
[jain_2020]: https://proceedings.neurips.cc/paper/2020/hash/9e9a30b74c49d07d8150c8c83b1ccf07-Abstract.html
[runyan_2017]: https://www.nature.com/articles/nature23020
[gjorgjieva_2016]: https://www.sciencedirect.com/science/article/pii/S0959438815001865?via%3Dihub
[holden_2004]: https://www.nature.com/articles/428382a
[marom_2009]: https://pubmed.ncbi.nlm.nih.gov/19836433/

[yt_cosyne]: https://www.youtube.com/channel/UCzOTbZTHTubFNjANAR33AAg
[wwn_virtual]: https://www.world-wide.org/cosyne-22/
[tw_talukder]: https://twitter.com/SaberaTalukder/status/1507183050177884162?s=20&t=K17HvJ49jTwOujZuVSzuqA
[tw_zeraati]: https://twitter.com/roxana_zeraati

[fig_speakers]: /assets/images/blog/2022-03-28-speakers.png#center
[fig_wstopics]: /assets/images/blog/2022-03-28-wstopics.png#center
[fig_larry]: /assets/images/blog/2022-03-28-larry.png#center

---
<iframe width="560" height="315" src="https://www.youtube.com/embed/eSRNzNF9rgM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
---
title: 'The Year is 2023'
tags: [Reflections, General Science, Science Communication, Data & Technology]
status: publish
type: post
published: False
header:
  overlay_image: /assets/images/blog/default_header.jpg
  overlay_filter: rgba(0,0,0,0.7)
  teaser: /assets/images/blog/default_header.jpg
classes:
  - wide
permalink: /blog/:year/:month/:day/

excerpt: ""
---

Being an immigrant family in Canada, mine doesn't make a big deal out of Christmas other than the fact that it's a time that I'm always home. My dad makes a big dinner for the 24th, the family will do something nice together on the 25th, like go out for a hike or watch a movie. The gift-giving actually happens on the 26th, where we all go out for Boxing Day shopping and everybody gets something new, and for a massive discount (this year I got the Levi jeans I've always been eyeing, half price). Win-win. 

Nevertheless, I rack my brain every year for a more personal gift I can give my family, like, I don't know, the same flavor of Body Butter my grandma likes. In 2022, my parents took a holiday trip through Germany and a bit of France, and just about visited every single museum and historical artifact there is to offer (of course, in addition to my village here in TÃ¼bingen). I joined them for part of the road trip, and one day ended up in some other random countryside village an hour and some outside of Paris, whose claim to fame is basically that, for its size, a disproportionate number of famous artists had once resided there. Among them was Vincent Van Gogh, who actually died here after shooting himself in the stomach...3 days later.

For people with absolutely zero formal art or art history training, my dad, and by osmosis, my mom, could probably serve as museum curators in a pinch (he did, in fact, educate the jewel room tour guide in one of the German castles about an obscure piece of imperial history). So it was by no coincidence that we paid 5 euros each to be inside the 5 square meter room where Van Gogh lived out his last few days. Although I could not have cared less to walk the fields that our artist dragged his bloody semi-corpse back to his lodge, this little road trip did inspire me in my quest of a personal gift: a canvas painting of my parents, plagiarizing the style of our Post-Impressionist master. Most of you (under 45 and on Twitter) probably know where this is going, and no, I did not paint it myself.

This blog post briefly details the 5-8 hours I spent in the process of accomplishing a task that is, depending on whose perspective you take, completely magical or completely trivial, dumb, even. It is this intensely paradoxical feeling that motivated me to document this in the first place. The fact that you can do this at all---re-creating a personal photo to match the artistic style of a painter who died more than a century ago---is mind blowing for probably 95% of the world. On the other hand, this exact fact, and that it was produced by a machine learning model from 2015 (a mere 7 years ago, which might as well be a hundred in the ML world), would struggle to evoke a response that is anything other than a lukewarm "oh cool" from 95% of the people I work with everyday, especially considering how long it took me.

But the "tutorial" is only a minor part of it. This paradoxical exercise, though unintentional, reminded me yet again just how large a chasm of technological competency there exists in this world, as well as my place as an imposter perpetually existing somewhere in between. This is a story where I, the bumbling protagonists, grapple with my place in this world in 2023. A story from which I emerge uncertain whether my supporting cast---ChatGPT, Stable Diffusion, the broader ecosystem of artificial intelligence and its human caretakers---are friends or foes.

---
### Step 1: Defining the problem
Neural style transfer

### Step 2: Try the new shiny thing
[Stable Diffusion][SD_release] was all the rage in 2022. Again, it's strange writing about this: if you are reading this, you most likely came from Twitter, in which case you've probably seen millions of these SD-generated images over the last 6 months. On the off chance you didn't (hi mom), Stable Diffusion is a piece of (AI?) software that takes in a prompt sentence ("dog riding rocket to the moon") and generates incredibly detailed and realistic images based on that prompt.

Well, technically it's one specific implementation (and commercial product) by [StabilityAI][stability_ai], of a image-generation model that uses ["denoising diffusion probabilistic models"][diffusion_wiki], whose details I definitely will not get into here today. While it generates images of mind-blowing quality, it's "just" the latest and greatest of a series of such diffusion-based image-generation models, following DALLE and DALLE-2, Midjourney, etc. Essentially, these all share the similarity of using 1) a complicated thingy that turns plain English words into numbers ("embeddings"), even better if it has some understanding of images as well (i.e., [CLIP][clip_openai]), and 2) another complicated thingy that uses those words-turned-numbers to make guide image-generation, which, in this case, is the diffusion model. Hence, Stable "Diffusion". 

I'm something of an AI enthusiast myself, so this was finally my chance to try it on an application I found personally useful (as opposed to, say, spamming Twitter). Poking around Google a bit, it turned out to be quite easy to get started: I had no idea how there now exists a variety of ways and tutorials to use [Stable Diffusion][sd_guide_all], depending on how hands-on you want to be with fine-tuning the model, exploring hyperparameters, etc., as well as a vast and general ecosystem surrounding the development and usage of state-of-the-art AI models. 

I didn't want to download and locally run a trained model or bother with setting up a GUI (e.g. [AUTOMATIC1111][gh_automatic] & [sygil][gh_sygil]), because I thought that start-up cost was too high for a very simple, 5-minute thing I wanted to do. Then I realized that there are so many web-hosted solutions out there now, that you can basically fire up a trained model and generate images from the web browser. The most flexible (but also complicated) being running the model on Google Colab (e.g., [here][colab_SD]), which is basically the same as downloading it to your own computer, but you run it on Google's Cloud. On the other hand, Replicate, for example, is a very nice resource that hosts a collection of models with great GUIs that you can run for free for a while, after which it's pay-per-use, and includes the latest [Stable Diffusion][replicate_SD] releases, and even a [separate img2img interface][replicate_img2img]. Finally, you can simply create a [Dreamstudio][dreamstudio] account, which is the web interface hosted by StabilityAI itself.


---
### Step 3: Going back to what works, fueled by old-I and AI

---
### Post-mortem
Let the absurdity of all this sink in: 

Sitting in my parent's basement over the holidays, I fired up an *ancient* pre-trained neural network model from 7 years ago (which I had to pick over the shinier, killer of a neural network model from 2022), on Google's free python computing platform on the "Cloud" physically located in god-knows-where. And with some help from ChatGPT, the closest thing we've ever seen to an "AI Assistant" (sorry Siri) that was barely 3-week old, I had to write a for-loop for bash, a thing that existed since like a million years ago. All this, just to transform a crystal clear, high-resolution photo that I took with my shiny iPhone into a blurry, tri-colored painting in the style of someone who lived and died in a different century. Then I paid and sent it off online for printing, and picked it up at Walmart in Scarborough a week later.

This is the year 2023, and I guess I'm a bit amused by all of this. 

Is this AI? Is this *A-General-I*? Compared to the alternative I had imagined, which was to simply go to a website, upload my photo, and type in "change this photo to look like the style of Van Gogh's Church at Auvers", and bingo, out comes exactly what I had wanted...was this closer or farther away? Would it have been closer if I could have used my brain to communicate this wish directly, instead of typing it on a keyboard like a pleb? Or was it somehow **my** fault for not having been a bigger tech wiz? *Am I already a boomer???*

There is a strange feeling that my place in this chasm is intimately knotted with that of "AI", in some grotesque way. This feels like the **Uncanny Valley of (Artificial) Intelligence**: it's so close you can feel it because you're doing the absolute impossible, by standards from even 12 months ago, but it's just far enough to feel impossibly inconvenienced because it didn't just understand exactly what you had wanted. I often have this relationship with my computer, and technology at large: why couldn't you just move this fucking file from and to the exact folder I wanted...as I watch shows on Netflix from all over the world while running actual computing tasks on my 2-pound laptop that would have required a whole room of cables and racks at a time as recent as when I was born.

Obviously this is not a criticism of the progress in AI: we are coexisting with signs of artificial intelligence that are, well, *more intelligence* than ever. Even when they are not sold as "intelligent", many of these things have invisibly transformed our life today. At the same time, we only notice their existence when they become an inconvenience, or cause some kind of social backlash.


dedicated porn models
my place as a writer?


maybe you expected "AI" to be able to create this version of the photo of my parents with just a single command, something like, "edit. Maybe you, like the average person on Earth, expect this to be either fairytale


uncanny valley of (artificial) intelligence


Neural style transfer is out, stablediffusion is in.

- Neural style transfer
- Diffusion models and dreamstudio
- Img2img
- Model libraries
- Mangled faces and confused races

Neural style transfer is back in

- old models
- New models
- Old models plus human limitations
- Replicate and Google colab
- Shellscripting
- Gpt3

The uncanny valley of AI

General state of the world

Hello 2023

https://github.com/crowsonkb/style-transfer-pytorch
https://colab.research.google.com/drive/1Tmuwmncao5E3D-5tTIVQjRy2YQ8JdpoB?usp=sharing

https://github.com/nkolkin13/NeuralNeighborStyleTransfer
https://replicate.com/nkolkin13/neuralneighborstyletransfer

---
[stability_ai]:https://stability.ai/
[SD_release]:https://stability.ai/blog/stable-diffusion-public-release
[diffusion_wiki]: https://en.wikipedia.org/wiki/Diffusion_model
[gh_automatic]:https://github.com/AUTOMATIC1111/stable-diffusion-webui
[gh_sygil]:https://github.com/Sygil-Dev/sygil-webui
[sd_guide_all]:https://andrewongai.gumroad.com/l/stable_diffusion_quick_start?_gl=1*u73vfg*_ga*MTE4ODg5NTY3NC4xNjc1NTI2MDY4*_ga_YHRX2WJZH7*MTY3NTUyNjA2OC4xLjEuMTY3NTUyNjI5My41OC4wLjA.
[replicate_SD]: https://replicate.com/stability-ai/stable-diffusion/versions/f178fa7a1ae43a9a9af01b833b9d2ecf97b1bcb0acfd2dc5dd04895e042863f1
[colab_SD]: https://colab.research.google.com/drive/1wriLAuWlY1xlfq_Zpq0Fk5WgFTRFyyOG?usp=sharing
[dreamstudio]: https://beta.dreamstudio.ai/
[stability_dreamstudio]:https://dreamstudio.com/create/
[replicate_img2img]: https://replicate.com/stability-ai/stable-diffusion-img2img
[clip_openai]:https://openai.com/blog/clip/
[img2img_guide]: https://stable-diffusion-art.com/stylize-images/
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-11-12T19:01:28-08:00</updated><id>http://localhost:4000/</id><title type="html">Mind From Matter</title><subtitle>Seeking the mind in the brain. Posts about computational neuroscience, cognitive science, and random things in my life.</subtitle><author><name>Richard Gao</name></author><entry><title type="html">SFN2018</title><link href="http://localhost:4000/sfn/" rel="alternate" type="text/html" title="SFN2018" /><published>2018-11-03T00:00:00-07:00</published><updated>2018-11-03T00:00:00-07:00</updated><id>http://localhost:4000/sfn</id><content type="html" xml:base="http://localhost:4000/sfn/">&lt;p&gt;Day 2:
LFP biophysics&lt;/p&gt;

&lt;p&gt;Cameron McIntyre&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;subthalamic DBS implant LFP - beta correlates with units&lt;/li&gt;
  &lt;li&gt;subthalamic cells have different morph and configuration from ctx cells&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Andreas Kuhn&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;clinically relevant features in STN DBS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Helen Bronte-Stewart&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;resting state DBS LFP recording in STN&lt;/li&gt;
  &lt;li&gt;spectrum is ‘stationary’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Day 1:&lt;/p&gt;

&lt;p&gt;Ken-ichi Amemori (Graybiel):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;beta stimulation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scott Cole (Voytek):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;waveform shape in time domain
Alex Gramfort:
  DAR:&lt;/li&gt;
  &lt;li&gt;Driven AutoRegreassive models for search&lt;/li&gt;
  &lt;li&gt;driver signal (low-freq) serves as time-varying AR parameters for the high frequency osc.&lt;/li&gt;
  &lt;li&gt;Robust to bandwidth/frequency of driving signal&lt;/li&gt;
  &lt;li&gt;can vary various parameters and compute likelihood
  CSC:&lt;/li&gt;
  &lt;li&gt;convolutional sparse coding&lt;/li&gt;
  &lt;li&gt;topography x waveform (conv) activation&lt;/li&gt;
  &lt;li&gt;alphaCSC extracts kernels
  MNE:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jaana Simola (M. &amp;amp; S. Palvas)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;measuring network connectivity via MEG &amp;amp; phase/amplitude synchrony&lt;/li&gt;
  &lt;li&gt;visual alpha lateralization &amp;amp; network synchrony (graph strength) predicts Posner task performance&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lobier et al., NI 2017&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hyeyoung Shin (Jones Lab)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;beta, MEG&lt;/li&gt;
  &lt;li&gt;beta power predicts inhibited performance across tasks, species and modalities&lt;/li&gt;
  &lt;li&gt;measuring beta transient events (number, power, duration, freq-span) from spectrogram in a per-trial fashion&lt;/li&gt;
  &lt;li&gt;nice complement to time-domain (bycycle) measures&lt;/li&gt;
  &lt;li&gt;dynamic (beta-length vs. pause) vs. burst generator: found evidence for stochastic durations, i.e. random pause times&lt;/li&gt;
  &lt;li&gt;event number correlates the most with prestimulus beta power and non-detection&lt;/li&gt;
  &lt;li&gt;prestim beta event &lt;strong&gt;closer&lt;/strong&gt; to stim predicts non-detection&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;would this explain stochastic distribution of per-trial power in visual gamma? Should check motor beta as well in ECoG&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;good Q: does power, event number, and duration etc explain different portions of performance variance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jason Samaha (Samaha/Postle)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;quantifying alpha oscillatory frequency&lt;/li&gt;
  &lt;li&gt;phase matters, does frequency?&lt;/li&gt;
  &lt;li&gt;MX Cohen instantaneous frequency method (MXC JN 2014)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;‘instantaneous’ is wrt to the complex sinusoid, but integrated over many samples during filter and hilbert stage&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;FT based method cannot resolve 0.2Hz difference, MXC method can but under-estimates freq in noise, improves when SNR increases&lt;/li&gt;
  &lt;li&gt;inter-subject alpha freq inversely correlates with two-flash fusion threshold (better temporal resolution), works within subject on a per-trial basis too&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ali Mazaheri (Mazaheri)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Corcoran et al. 2018&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;alpha frequency &amp;amp; pain&lt;/li&gt;
  &lt;li&gt;slow alpha &amp;lt;&amp;gt; longer pain duration (years)&lt;/li&gt;
  &lt;li&gt;9-11Hz alpha CoG (power-weighted freq)&lt;/li&gt;
  &lt;li&gt;capsicin induced pain: slow alpha &amp;lt;&amp;gt; more pain &amp;amp; duration, also slowing down &amp;lt;&amp;gt; pain&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Andrew Watrous (Watrous/Josh Jacobs)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MODAL - time-frequency detection of oscillations&lt;/li&gt;
  &lt;li&gt;rate and theta spike-phase coding neurons in the human MTL&lt;/li&gt;
  &lt;li&gt;probabilistic detection&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tom Donoghue (Voytek)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;FOOOOOOOOOOF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Josh Jacobs (Jacobs)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;traveling waves (theta/alpha) in temporal cortex&lt;/li&gt;
  &lt;li&gt;circular-linear model &lt;strong&gt;(Fisher 1995)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;also exist elsewhere and at other freqs&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;referencing&lt;/strong&gt;?&lt;/li&gt;
  &lt;li&gt;wave collision between occipital alpha and MT/frontal theta?&lt;/li&gt;
  &lt;li&gt;consistency of traveling direction correlates with performance&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ermentrout &amp;amp; Kleinfeld 2001&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;whole cortex vector field show posterior to anterior propagation&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;higher freq oscillations are also travelling faster&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;neuronal population time constant across cortex?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Zhang et al 2018, Neuron&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lyle Muller (Sejnowski)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;generalized (wideband) phase&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Muller et al 2016, elife, Nat Comm 2014&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;does traveling wave depend on recording depth&lt;/strong&gt;?&lt;/li&gt;
  &lt;li&gt;spontaneous and non-isotropic waves, phase locking&lt;/li&gt;
  &lt;li&gt;impacts visual perception threshold&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Saskia Haegens ()&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;oscillations implement low-level functional blocks (e.g. inhibition, ensemble formation, sampling, etc)&lt;/li&gt;
  &lt;li&gt;frequency variability between subjects&lt;/li&gt;
  &lt;li&gt;peak freq increases with cognitive demand&lt;/li&gt;
  &lt;li&gt;inverse correlation between alpha power and broadband (HFA) power&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;general thoughts&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;lots of complementary packages but unclear how to use all of them&lt;/li&gt;
  &lt;li&gt;fuller characterization of oscillations (frequency, 1/f, travel)&lt;/li&gt;
  &lt;li&gt;loglog plots&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Richard Gao</name></author><category term="Mind &amp; Brain" /><category term="Data &amp; Technology" /><summary type="html">.</summary></entry><entry><title type="html">Roemer Has It: The Hilbert Transform, Instantaneous Power/Phase/Frequency, and Negative Frequencies in Neuroscience</title><link href="http://localhost:4000/roemerhasit_Hilbert_Transform/" rel="alternate" type="text/html" title="Roemer Has It: The Hilbert Transform, Instantaneous Power/Phase/Frequency, and Negative Frequencies in Neuroscience" /><published>2018-11-02T00:00:00-07:00</published><updated>2018-11-02T00:00:00-07:00</updated><id>http://localhost:4000/roemerhasit_Hilbert_Transform</id><content type="html" xml:base="http://localhost:4000/roemerhasit_Hilbert_Transform/">&lt;p&gt;If you work with oscillations in the brain, or any sort of time-frequency analysis of neural (or other) time series, you’ve probably used the Hilbert transform at some point. Depending on your background and level of comfort with signal processing, it may be just a scipy or MATLAB function call to you, or you might understand the deeper mathematical theory behind it. What you might not appreciate (I certainly didn’t), however, is one of the important assumptions behind it, specifically in the context of applying it to brain data. In particular, it’s an important assumption in interpreting instantaneous oscillation power and phase in brain rhythms, especially if you’ve ever thought to yourself, “what does &lt;strong&gt;instantaneous&lt;/strong&gt; power mean anyway?”&lt;/p&gt;

&lt;p&gt;In the following post, I delve into the math behind the Hilbert Transform and provide some visualizations that will hopefully make these ideas clearer. When I was researching for this, seeing the complex sinusoids represented as vectors really helped me in understanding it better, as well as in seeing the elegance of the math. I will also clarify what scipy.signal.hilbert() is actually doing (hint: it’s not computing the Hilbert Transform). You should have a basic exposure to frequency analysis (Fourier Transform) and filtering, i.e. know what they do. But no more than that is necessary, as this post will unpack some fundamentals of the Fourier Transform and complex sinusoids.&lt;/p&gt;

&lt;p&gt;Note: the most confusing part about this post is probably the switching of representation between time domain and frequency domain, and at times, time-frequency domain. Be wary when this happens, and I will clearly signpost when I make a jump (thanks for the feedback, Tammy!)&lt;/p&gt;

&lt;p&gt;I’m uploading this as a blog post mainly to see the swagged out markdown embedded code blocks, but notebook is identical and can be found &lt;a href=&quot;https://github.com/rdgao/roemerhasit/blob/master/RHI_Hilbert_Transform.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;instantaneous-measures-and-the-hilbert-transform&quot;&gt;Instantaneous measures and the Hilbert Transform.&lt;/h1&gt;

&lt;p&gt;If you work with (M)EEG/ECoG/LFP, or even EMG, you may have computed instantaneous power. There’s a long list of studies that looks at stimulus-evoked and time-resolved measures of, for example, oscillatory and high-frequency power (Crone 1998, first example that comes to mind.) If you’re in the even more niche community of people for whom “phase-amplitude coupling” rings a bell (hello friends), then you’re definitely familiar with these instantaneous measures.&lt;/p&gt;

&lt;p&gt;Typically, how you arrive at these measures is through 1) narrowband filtering your signal, 2) get its complex representation via the Hilbert transform, and 3) compute squared magnitude/phase/phase difference. 1) and 2) can be combined via a complex wavelet filter, or obtained via FFT, but I will stick with the above because I want to write about the Hilbert Transform.&lt;/p&gt;

&lt;p&gt;In particular, when I first started writing this post, I wanted to address two thoughts that always vaguely bothered me, but could never figure out why or where I got them from:&lt;/p&gt;
&lt;h5 id=&quot;1-instananeous-measures-are-suspicious-because-how-do-you-measure-powerfrequencyphase-truly-at-an-instant-can-you-look-at-a-single-point-on-a-sine-wave-without-the-neighboring-points-and-determine-the-amplitude&quot;&gt;1. “Instananeous” measures are suspicious, because how do you measure power/frequency/phase truly at an instant? Can you look at a single point on a sine wave (without the neighboring points) and determine the amplitude?&lt;/h5&gt;

&lt;h5 id=&quot;2-the-hilbert-transform-is-mathematically-well-defined-only-for-narrowband-signals-and-breaks-down-when-the-signal-is-wideband-energy-at-many-frequencies&quot;&gt;2. The Hilbert Transform is mathematically well-defined only for narrowband signals, and breaks down when the signal is wideband (energy at many frequencies).&lt;/h5&gt;

&lt;p&gt;Turns out, both of the above were false beliefs. So I’m passing on my experience so you don’t have to go through the struggle.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So what does the Hilbert Transform (HT) do? HT, in the most pragmatic sense, is a function that creates information. I know this sounds sacriligeous, but stay with me. If you read the docstring for scipy.signal.hilbert(), it says&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;Compute the analytic signal, using the Hilbert Transform&quot;.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In other words, it turns a real-valued signal into a complex-valued signal - it’s &lt;strong&gt;analytic&lt;/strong&gt; form - which means turning a 1-D time series (real) into a 2-D time series (complex: real + imag).&lt;/p&gt;

&lt;p&gt;For purpose of demonstration, I will use a snippet of rat hippocampus local field potential data (courtesy of CRCNS, dataset hc2), because of its strong theta oscillation. The following few steps may look familiar, as I will first be filtering the raw signal for its theta component, then computing the Hilbert Transform.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;axes.labelsize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;font.size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcParams&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;font.family&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Arial&quot;&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# our open source lab package, free for all to use/contribute :)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# https://github.com/voytekresearch/neurodsp&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;neurodsp&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ndsp&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# loading the test data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadmat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data/sample_data_2.mat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;squeeze_me&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# filtering and hilbert transform&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_filt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ndsp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'bandpass'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remove_edge_artifacts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hilbert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_filt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# plotting&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Raw Signal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_filt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Filtered Signal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Real Signal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_filt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'--r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Filtered Signal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hilbert Real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hilbert Imag'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time (s)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Complex Signal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# selecting 2 slices of data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2650&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2700&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'g--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Transition bandwidth is 2.0 Hz. Pass/stop bandwidth is 8 Hz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-1.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the first plot, you can see the raw hippocampus LFP (black) and the filtered theta component overlaid. If you comment on how non-sinusoidal the oscillation looks without provocation, your IP will be banned forever (joking but also don’t).&lt;/p&gt;

&lt;p&gt;In the second plot are the filtered theta (dashed red), and the real (blue) and imaginary (orange) components of the analytic signal. Notice that the filtered signal and the real component of the analytic signal are completely overlapping, which is the first hint of what the Hilbert Transform does: it retrieves an imaginary component. This is what I meant by “creating information”, but hold that protest still.&lt;/p&gt;

&lt;p&gt;Also, notice that the imaginary component is a) almost identical to, and b) slightly lagging the real component - by exactly 90 degrees, or pi/2 rad, in fact. At this point, those of you familiar with this analysis pipeline should anticipate the next few functions calls, which computes the “instantaneous” power and phase from the filtered theta signal.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x_power&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_phase&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# plotting&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Power'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_phase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Phase'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time (s)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'g--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-2.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Like I mentioned above, having worked on these methods for some time now, I’ve sometimes felt an air of mysticism, and sometimes, skepticism, about how instantaneous measures could be computed on a signal. This is because I had an intuitive notion of what instantaneous amplitude (power) and phase are: amplitude is roughly the signal envelope of the sinusoid, and phase is the, well, phase of the sinusoid in that instant.&lt;/p&gt;

&lt;p&gt;Implictly, my notion of amplitude and phase is in accordance with the definition of a sinusoid as x(t) = Asin(wt+$\phi$), where A is the amplitude and $\phi$ is the phase lag. But given just a single point in a sinusoid-like real-valued signal, how does one decide what its instantaneous power is, if it’s at any point in time other than the peak or trough, such as the two instants I’ve highlighted above in black and green?&lt;/p&gt;

&lt;p&gt;But as you might have inferred from the above plots, that is actually a misconception. The instantaneous power is not computed on the filtered signal, but the analytic signal, which is the whole point of using the Hilbert Transform. Namely, it is the squared magnitude of the complex vector at an instant in time. Similarly, the instantaneous phase is the phase angle between the vector and the real axis (0 deg) in the complex plane. Mathematically, this refers to the complex sinusoid, i.e., x(t) = Ae^(iwt) in Euler form, or x(t) = A(cos(wt)+isin(wt)) in rectangular form.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: now switching from temporal representation (time on x-axis) to the complex plane, where each dot is an instant (sample) in time.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k.-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quiver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quiver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Imaginary'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'off'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Vector 1(k) phase: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f, vector 2(g) phase: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_phase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;360&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_phase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;360&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-3.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above plot, I’ve traced out the first 5 seconds of the analytic signal in the complex domain, where each small dot represents a vector in an instant in time (with its arrow-tail removed), and the black and green arrows refer to those two moments marked in dashed lines in the previous plot. What you can see is that the signal traces out a relatively smooth circular trajectory, with a varying radius. At any moment in time, the radius squared is the signal power, and its angle from the positive real line is the phase: so the black arrow is at about -30 degrees (or 330), and the green one at 75 degrees.&lt;/p&gt;

&lt;p&gt;Hopefully, this clears up any confusion on how instantaneous power and phase are computed. I have to emphasize here that these are not mystical or vaguely intuitive measures at all, as I had thought, but mathematically well-defined quantities of every complex signal. Which now brings us back to the original question: how does the Hilbert Transform create the imaginary component of a real signal, out of thin air?&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;your-hilbert-transform-is-a-lie&quot;&gt;Your Hilbert Transform is a lie.&lt;/h1&gt;

&lt;p&gt;To understand how the Hilbert Transform produces the analytic signal, we have to delve into the Fourier Transform and the (complex) sinusoidal bases. But first, a clarification.&lt;/p&gt;

&lt;p&gt;If you read through the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_transform&quot;&gt;Wikipedia article on the Hilbert Transform&lt;/a&gt;, it says:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The Hilbert transform has a particularly simple representation in the frequency domain: it imparts a phase shift of 90° to every Fourier component of a function.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Technically, the Hilbert Transform as defined mathematically is &lt;strong&gt;not what scipy.signal.hilbert() or hilbert() in MATLAB does&lt;/strong&gt;. What those functions do is to return the analytic signal, which is a complex signal defined as:
x_a(t) = x(t) + iHT{x(t)} = Re(x_a(t)) + Im(x_a(t)).&lt;/p&gt;

&lt;p&gt;In other words, the mathematical Hilbert Transform produces the imaginary component of the analytic signal, which is not what scipy.signal.hilbert() returns. See the full documentation here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-sp1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Ignore the middle part of that formula for now.)&lt;/p&gt;

&lt;p&gt;The conflict is somewhat confusing, but was implemented as such probably because the HT is used almost exclusively in the context of computing the analytic signal. From this point on, I will be precise and use “analytic signal” for x_a(t), and “Hilbert Transform” (noun) for y=HT{x(t)} (note, without the ‘i’ attached), “Hilbert Transform” (verb) for the operation HT{ }, and hilbert() for the scipy function. This is the mathematically correct usage, but contradicts our everyday colloquial usage in neuroscience (and more broadly, scientific computing).&lt;/p&gt;

&lt;p&gt;EDIT: it is at this point that I went back and changed all my variable names from &lt;code class=&quot;highlighter-rouge&quot;&gt;x_ht&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;x_a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Noting the Wikipedia quote, this is consistent with what we saw in the second subpanel of the very first figure above: the imaginary component was about 90 degrees phase-lagged to the filtered theta oscillation. However, this is not always the case. Let’s see what happens when we compute the analytic signal of the raw wideband signal.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# plotting&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hilbert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hilbert Real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hilbert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hilbert Imag'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time (s)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-4.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Back to time domain!)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I’ve zoomed in on a shorter segment of the signal for clarity. You can see that, roughly speaking, the imaginary component (orange) is still lagged about 90 deg compared to the real component, and this is due to the phase shift of the dominant theta frequency. However, it’s not a literal time shift of the whole signal, since many of the more squiggly high-frequency ripple components are not exactly copied over but remain locked to the same theta phase (e.g., see ripple at 2.3s). As the Wikipedia article states, the Hilbert transform imparts a 90 degrees phase shift to &lt;strong&gt;every&lt;/strong&gt; Fourier component, not just the strongest (theta) component. In other words, every complex Fourier coefficient in the raw broadband signal is delayed by 90 degrees.&lt;/p&gt;

&lt;p&gt;Why does introducing a 90-degree lag produce the imaginary component of the signal? Finally, we are ready for the part that I enjoy the most: the relationship between the Hilbert Transform and the Fourier Transform, in vectors. Bonus, we make our own DIY Hilbert Transform.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;negative-frequencies&quot;&gt;Negative Frequencies&lt;/h1&gt;

&lt;p&gt;Going back to the beginning of the tutorial, I mentioned that the Hilbert Transform creates information, which is in the form of the imaginary component of the analytic signal. That’s actually a lie, technically. From the perspective of the time-domain signal, indeed, it seems to have generated a 2-dimensional (complex) signal from a 1-dimensional (real, scalar) signal. The information it “creates”, however, &lt;strong&gt;comes from a specific assumption&lt;/strong&gt;, and when viewed in the frequency (or Fourier) domain, it’s actually not creating information at all, but splitting the original signal in two. I will illustrate this with a perfect sine wave first.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# create a sine wave for 5 seconds at 14Hz, with slight phase delay&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t_trunc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_sin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_trunc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;F_sin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ortho'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# time domain plot&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_trunc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time (s)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Real Signal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# frequency domain plots&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F_sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ko-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Power'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Fourier Power'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F_sin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ko-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Fourier Coeff. Real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F_sin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ko-'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Fourier Imag'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Fourier Coeff. Imag'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Frequency (Hz)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-5.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Warning: jumping between time (1st plot) and frequency (2nd-4th plot) domain.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first plot shows the sine wave in time domain, and the next three show the Fourier power (squared amplitude), as well as the real and imaginary components of the Fourier coefficients. A few things will jump out at you. First, Fourier power is zero everywhere except two points, which are at 14 and -14 Hz. In neuroscience, it’s almost always the case that power spectra are plotted with only the positive frequencies, i.e., the right half. This is because for any real-valued signal, as all brain recordings are, the power spectrum is symmetrical about 0Hz, so plotting the left side is just wasting space. &lt;strong&gt;But sometimes we’re so used to looking at the positive half of the spectrum that we forget the other half exists, or matters at all.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here’s why it matters. The 14 Hz power we see &lt;strong&gt;does not&lt;/strong&gt; represent the squared magnitude of our scalar sinusoid at f=14 Hz, i.e. x(t) = cos($2\pi$ft), which one might assume by looking just at the positive frequencies. Instead, it’s the squared magnitude of the &lt;strong&gt;complex sinusoid&lt;/strong&gt; at 14 Hz, in the form of z(t) = Aexp(i$2\pi$ft). This is because the basis functions for Fourier decomposition are not real sinusoids, but complex sinusoids.&lt;/p&gt;

&lt;p&gt;Ugh, basis functions - it got a little gnarly, so I think you’ll like this next part.&lt;/p&gt;

&lt;p&gt;In rectangular form, the complex sinusoid looks like z(t) = Acos($2\pi$ft) + iAsin($2\pi$ft). Wait a minute, that’s just our original signal (cos($2\pi$ft)) with an unwanted imaginary part tacked onto the end! The original signal is purely real, so how do we get rid of this imaginary sine component? The answer is in the negative frequency. Specifically, at -14Hz.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fc_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F_sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fc_neg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F_sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axhline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quiver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'+ 14Hz'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headwidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quiver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_neg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_neg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'- 14Hz'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headwidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quiver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Sum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headwidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Real'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Imaginary'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;box&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'off'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-6.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Back to complex representation!)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The three vectors above represent the complex sinusoids at 14Hz (blue) and -14Hz (red) in the complex plane, and their vector sum (black). They’re not exactly like the black and green ones from a few plots above. Those previous ones represent two instants of the same complex sinusoidal function as it evolves in time, while these represent two complex sinusoids extracted from our made up signal, and their sum. There is one thing in common, however, which is that you could imagine the blue and red vectors here rotating about the origin as progression of time, which corresponds to phase advancing the time series. The only extra rule is that the &lt;strong&gt;blue vector rotates counterclockwise&lt;/strong&gt;, since it’s a positive frequency, and &lt;strong&gt;the red vector rotates clockwise but at the same speed&lt;/strong&gt; (their frequency (14Hz) in radians).&lt;/p&gt;

&lt;p&gt;It’s no coincidence that the negative frequency vector is reflected about the real axis from the positive frequency - this is how the imaginary component is cancelled out to form our real-value only signal. Since the two colored vectors always mirror each other about the real axis as they rotate, their summation (black vector) will &lt;strong&gt;always&lt;/strong&gt; fall exactly onto the real axis. As the colored vectors rotate about, the black vectors will shrink and grow, representing the value of the real sinusoidal function in time.&lt;/p&gt;

&lt;p&gt;In math, this cancellation of imaginary components amounts to:
z(t) + z’(t) = A(cos($2\pi$ft) + isin($2\pi$ft)) + A(cos($2\pi$ft) - isin($2\pi$ft)) = 2Acos($2\pi$ft) = 2Ax(t),&lt;/p&gt;

&lt;p&gt;and thus, we get back our signal (with a scaling constant of A=1/2 on the complex sinusoids). To further verify that this is accurate, we can check that the energy from the two sinusoids sum to be equal to the energy of the time series, which is just the sum of squares, or &lt;code class=&quot;highlighter-rouge&quot;&gt;var(signal)*len(signal)&lt;/code&gt;, in this case (Parseval’s theorem).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Signal energy=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f, Fourier energy=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%.2&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_sin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Signal energy=2500.00, Fourier energy=2500.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;hilbert-transform-revisted&quot;&gt;Hilbert Transform, revisted.&lt;/h1&gt;

&lt;p&gt;Armed with the concept of negative frequencies, let’s work out what &lt;code class=&quot;highlighter-rouge&quot;&gt;scipy.signal.hilbert()&lt;/code&gt; is doing. Following the perfect sinusoid example, the goal is to transform x(t) = cos($2\pi$ft) to its analytic signal, x_a(t) = x(t) + iy(t) = cos($2\pi$ft) + isin($2\pi$ft). Graphically, this amounts to getting the blue vector from the black vector. Without looking at the math, we should have an idea of what to do here, given the forward steps we took from the blue and red vector to the black vector - we simply need to erase the red vector and multiply the blue vector by 2. Turns out, this is exactly what scipy.signal.hilbert() does:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-sp2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Briefly, the positive step function U retains the positive frequencies of the Fourier Transform, while multiplying by 2.&lt;/p&gt;

&lt;p&gt;“The negative half of the frequency spectrum is zeroes out.” In other words, deleting the negative frequency complex sinusoid(s) transforms the real-valued signal to the analytic signal. I thought this shortcut is honestly pretty cool, and makes a lot more sense after understanding the geometrical interpretation of the negative frequency sinusoids. Also, this probably points to the real reason for implementing scipy.signal.hilbert() as getting the analytic signal directly - it’s a lot easier than getting the actual Hilbert Transform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;More importantly, nothing about how the Hilbert is defined restricts its usage to a narrowband signal, debunking misconception #2 (mentioned at the beginning).&lt;/strong&gt; The complex (analytic) signal generated by HT is perfectly valid and well-defined at all points.&lt;/p&gt;

&lt;p&gt;Finally, let’s see if we can implement our own Hilbert Transform function by following these steps, with the &lt;strong&gt;correct&lt;/strong&gt; name.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;analytical_sig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# get fft&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fftfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Fx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# zero out negative frequencies&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# return 2x positive frequencies&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hilbert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analytical_sig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'scipy hilbert'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_a2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'custom hilbert'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Time (s)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-11-02-RHI_HT-7.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Perfect overlap, passes the sniff test.&lt;/p&gt;

&lt;p&gt;That is all. I hope this provided some intuition for what the Hilbert Transform is and isn’t doing, debunked some (I hope) common misconceptions, as well as explained what instantaneous power/phase is in brain signals.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;some-last-thoughts&quot;&gt;Some last thoughts:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Remember that x_a(t) = x(t) + jHT{x(t)}, so how do we actually compute the Hilbert Transform without going directly to the analytic signal, and how does the Hilbert Transform negate the negative frequency component? This &lt;a href=&quot;https://ccrma.stanford.edu/~jos/mdft/Analytic_Signals_Hilbert_Transform.html&quot;&gt;tutorial&lt;/a&gt; does an in-depth explanation, which has some really simple but elegant complex math, as well as neat visualizations that should be accessible if you’ve been following up to this point.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I think it’s hilarious that scipy.signal.hilbert() not only does not return the Hilbert Transform, as its function name would suggest, it doesn’t even compute the Hilbert Transform at all. The last part of the docstring basically says you can GET the Hilbert Transform of the signal yourself if you want to, by just taking the imaginary component, but why would you want to? There’s gotta be a joke about programmers and engineers being practical here.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I’m repeating myself here, but it’s worth it: I had a misconception that the Hilbert Transform only “existed” for narrowband signals. This is not true, as we clearly demonstrated above. HT is a well defined mathematical operation regardless of the bandwidth of the signal. The issue here is more so in the context of neuroscientific data, i.e., how to interpret the Hilbert Transform of a signal that doesn’t have a dominant oscillatory mode, specifically its instantaneous power and phase. If we are not careful here and just apply the method, we can extract a phase time series for all signals, but may or may not be valid. In the last 5 years, we’ve started to notice that oscillations are highly non-stationary, and sometimes not even present at all in the purport canonical bands (e.g., alpha). This gets at the broader question of bandpassing a signal for an oscillatory component when there is none, but that’s discussion for another day. In short, the HT is not to blame, but narrowband filtering. (See &lt;a href=&quot;https://voytekresearch.github.io/bycycle/auto_tutorials/plot_1_bycycle_philosophy.html#sphx-glr-auto-tutorials-plot-1-bycycle-philosophy-py&quot;&gt;bycycle&lt;/a&gt; and &lt;a href=&quot;https://voytekresearch.github.io/fooof/&quot;&gt;fooof&lt;/a&gt; for more indepth exposition of the issues and our methods of dealing with them.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Actually, I will just say one more thing about that, and this is worth &lt;strong&gt;emphasizing&lt;/strong&gt;. There is a fine but ideological difference between generative model and descriptive analysis. Using Fourier analysis to compute the band-limited energy does not mean you subscribe to the model that the signal is generated by sinusoids, as its simply a description (or representation) of the signal. The time series itself is another representation - you can represent that same signal in infinitely many ways. Using the Hilbert Transform is also not subscribing to a particular generative model - you don’t have to presume a complex sinusoidal generator to describe it as one. However, there is a fine line you cross when you interpret the transformed representations as being generated by an implicit model, like claiming the band-limited phase time series as the phase of a sole oscillator at that frequency. This does subscribe to the model that there &lt;strong&gt;is&lt;/strong&gt; an underlying oscillator in the signal, which is not at all implied by the descriptive analysis. Fourier analysis is very useful in many aspects, especially sparse representation of oscillatory signals. But it’s not as useful in faithfully (and sparsely) describing non-sinusoidal (nonlinear) oscillations and non-oscillatory phenomena.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I’ve actually never used complex sinusoidal or wavelet filters, only real-valued ones. But all the above holds, as filtering with a complex wavelet bypasses the need to apply the Hilbert Transform, since it’s, in a sense, transformed (into the complex time series) as it is filtered. More details in the Mike X Cohen ref below.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As with almost all neuroscientific time series analyses - (the late) Walter Freeman has written about it. I stumbled upon this as I was finishing up this post. Had I seen it earlier, I might not have even written it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sources&quot;&gt;Sources:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ccrma.stanford.edu/~jos/mdft/Analytic_Signals_Hilbert_Transform.html&quot;&gt;Phase Quadrature&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.dsprelated.com/freebooks/filters/Plotting_Complex_Sinusoids_Circular.html&quot;&gt;Complex Sinusoids&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.dsprelated.com/freebooks/mdft/Symmetry.html&quot;&gt;Symmetry&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?list=PLn0OLiymPak3jjr0hHI9OFXuQyPBQlFdk&amp;amp;v=VyLU8hlhI-I&quot;&gt;Mike X Cohen lecture&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.hilbert.html&quot;&gt;scipy doc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_transform&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.scholarpedia.org/article/Hilbert_transform_for_brain_waves&quot;&gt;Walter Freeman on HT for brainwaves&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;gratitude&quot;&gt;Gratitude:&lt;/h3&gt;
&lt;p&gt;Thanks to Tammy, Scott, Paolo, and Brad for reading an earlier version of this and providing feedback.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/aY5EvDhPWfo&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Richard Gao</name></author><category term="Mind &amp; Brain" /><category term="Data &amp; Technology" /><summary type="html">The one where I find out scipy has been lying to me this whole time.</summary></entry><entry><title type="html">Reference Manager Showdown: a full review of Papers 3, Zotero, F1000, and Paperpile (+ ReadCube)</title><link href="http://localhost:4000/reference-managers/" rel="alternate" type="text/html" title="Reference Manager Showdown: a full review of Papers 3, Zotero, F1000, and Paperpile (+ ReadCube)" /><published>2018-09-19T00:00:00-07:00</published><updated>2018-09-19T00:00:00-07:00</updated><id>http://localhost:4000/reference-managers</id><content type="html" xml:base="http://localhost:4000/reference-managers/">&lt;p&gt;How does the saying go? “You can’t paint a Picasso with a paint roller?” I’m sure that’s not how it goes, but the point is, having good tools is the foundation of doing good work. Today, I’m writing about one of the most commonly used tools across academia - the reference management software.&lt;/p&gt;

&lt;p&gt;Long story short, I’ve been using Papers 3 for about 2 years now, and I’ve been getting increasingly annoyed by it over the last few months. Naturally, I flipped through the internet to see if there exists a comparison of the more recent tools available to help me make a decision. To my extreme surprise and exasperation, I found none. So I decided to bite the bullet and try all the ones I could get my hands on to see which one I liked more. This took me a good month or so, but hey, now that I did the hard work, you don’t have to.&lt;/p&gt;

&lt;p&gt;This post is rather comprehensive, so I’ve collapsed the detailed comments for each app, leaving just the intro and summary. These are aimed at helping you make a decision as quickly as possible, based on your needs. You can find the detailed notes for each by clicking on “Click to expand” in each section, which I highly recommend if you’re considering a particular app based on the feature highlights.&lt;/p&gt;

&lt;p&gt;One last thing before we get to the good stuff: my preferences are specific to my literature reading workflow, which might be different from yours. In particular, I do most of my reading electronically (and on a mobile device), and I often receive/find one-off recommendations from the lab or Twitter, which is a pain to manage without disrupting my day. With that in mind, I drew a flowchart of my workflow (programatically generated in Markdown, with mermaid!).&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Click to expand workflow flowchart.&lt;/b&gt;&lt;/summary&gt;
  &lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;400&quot; src=&quot;/assets/images/blog/2018-09-20-refman-workflow.png&quot; /&gt;
&lt;/p&gt;
&lt;/details&gt;

&lt;p&gt;Briefly, the steps are: get paper &lt;strong&gt;(Scan/Import)&lt;/strong&gt;, file paper &lt;strong&gt;(Organization)&lt;/strong&gt;, read paper &lt;strong&gt;(Reading/Note-taking)&lt;/strong&gt;, and later, find paper &lt;strong&gt;(Storage/Search and Citing)&lt;/strong&gt;, with the additional requirement of syncing &lt;strong&gt;(Offline/Sync)&lt;/strong&gt; between two devices - I do all my writing on my laptop, and ideally, I will be doing all my reading on the iPad. Each software is judged based on those categories, so that if you skip some of those steps in your workflow, hopefully the other sections will still be informative.&lt;/p&gt;

&lt;p&gt;If you’re just starting your research career, be it graduate or undergraduate, I hope this guide will inform you on which app you want to use for the next few years based on your needs. And if you’re a seasoned academic that just happens to be displeased with your reference manager, keep on reading.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;tldr&quot;&gt;tl;dr:&lt;/h1&gt;

&lt;p&gt;Most of these apps are on an equal footing with regards to the core functionalities: importing, filing, reading, and creating citations, though the small features that do differ can make or break it for you. Other than that, it’s a question of whether you prefer a traditional (offline) app or a web app, whether you will read on a mobile device (phone/tablet), and making the trade-off between feature-rich but clunkier, or spartan but lightweight.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Papers 3 &lt;em&gt;was&lt;/em&gt; great, but it’s going downhill fast and you should not buy it.&lt;/li&gt;
  &lt;li&gt;ReadCube doesn’t have tagging (wtf) and, while the other necessary features are there, there are some superfluous ones slowing things down. Probably the most complete ecosystem (desktop, web, and mobile app), and may improve with the merged ReadCube-Papers app.&lt;/li&gt;
  &lt;li&gt;Zotero is free, open source, and NOT owned by Elsevier. If you need a desktop-only (offline) app that works well with Word, with the potential of useful future add-ons (e.g., Zotfile), this is the one for you.&lt;/li&gt;
  &lt;li&gt;Mendeley is not here. Just use Zotero instead.&lt;/li&gt;
  &lt;li&gt;F1000Workspace is a free web-app, very functional but a little claustrophobic. It only works online and has great integration with Word and Google Doc. Good pick if you rarely need to read or write offline and don’t mind bad UI design.&lt;/li&gt;
  &lt;li&gt;Paperpile is also a web-only app, but is beta testing an iOS app that has full offline functionalities. Lightweight, functional, and pleasantly designed app interface, but only works with Google Doc (for now).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spoilter alert: I ultimately went with Paperpile, but it was not by any means a decisive victory. This really depends on your own workflow.&lt;/p&gt;

&lt;p&gt;Go to: &lt;a href=&quot;#papers-3&quot;&gt;Papers 3&lt;/a&gt; | &lt;a href=&quot;#readcube&quot;&gt;ReadCube&lt;/a&gt; | &lt;a href=&quot;#zotero&quot;&gt;Zotero&lt;/a&gt; | &lt;a href=&quot;#f1000workspace&quot;&gt;F1000&lt;/a&gt; | &lt;a href=&quot;#paperpile&quot;&gt;Paperpile&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;papers-3&quot;&gt;Papers 3&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Price&lt;/strong&gt;: $50, one time&lt;/p&gt;

&lt;p&gt;I’ve been using Papers 3 for the last 2 years, and I really liked it. I paid $50 for it, so you bet I’m gonna like it. Overall, the desktop (Mac) and iOS apps were great - rich features, automatic syncing over Wifi and Dropbox, and serviceable customer support. But the entire reason for this deep dive on reference management software is that I wanted to switch from Papers, and it’s for two reasons: first, various aspects of the app are getting buggy, ranging from general slowness of the desktop app during searching/accessing pdfs, consistent crashing of the Citation plug-in, and weird sync issues with the iOS app; second, Papers is now merged with ReadCube, and they are to rollout a new ReadCube app soon under a monthly subscription model. Papers 3 users can upgrade at a discount, and the old app will (so they say) be supported indefinitely, but let’s be real here: it’s getting killed. Ultimately, it was a great piece of software that, I felt, focused on the right things. But given the state of the merger, I would not recommend shelling out $50 for it.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Click to expand for details and screenshots.&lt;/b&gt;&lt;/summary&gt;

  &lt;p&gt;&lt;strong&gt;Scan/Importing (6/10):&lt;/strong&gt; Scanning for articles in the Mac app itself is easy, and I’ve actually used it a bunch for doing literature searches prior to starting a project. Importing a references from the search interface is just one button-click, but getting the pdf itself can be a pain with paywall journals, as you have to click through multiple pages in this clunky web browser inside the app. You can also just drag and drop pdfs, and reference matching for published articles, for the most part, is very accurate. Duplicate reference detection is automatic, and clearly displayed as a column. The one thing that really annoyed me was that there was no easy way to import a paper while tagging and filing at the same time. I would often read a paper’s abstract online, scan the figures, and decide that it could be a useful reference for the future. From that point, with Papers 3, I would have to download the pdf into the designated watch folder, and open the app to tag or write down my immediate thoughts. Since the app is so bulky, I usually just save that step for later (read: never), except this introduces unnecessary work later because I already know how I want to file this paper at this moment. I hated this so much that I started to append what I would have tagged as the file names when I saved them. I thought this problem was unavoidable until I tried the other apps on this list, which all have Chrome extensions for direct import into library and easy tagging - life-changing. Having a sensible organizational system is great, but its usefulness is directly tied to how easy it is to actually use it.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-4.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-4.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-5.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-5.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-6.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-6.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-7.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-7.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-8.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-8.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Organization (9/10):&lt;/strong&gt; Papers offer a large set of organizational tools, including tags, collections (folders), colors, flag (for importance), and read/unread marker. I used tags to tag permanent properties, like the method used in the paper or the model organism, folders for each of my ongoing projects, colors for denoting the level of detail to which I read the paper (green for abstract, red for full text, etc), and flag for marking really relevant papers that I will probably revisit. I’ve gotten used to these different markers, so it feels natural, though my biggest complaint is probably that there doesn’t seem to be a way to hierarchically organize tags, and finding articles under multiple tags is needlessly difficult (compare to Zotero).&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Reading/Note-taking (9/10):&lt;/strong&gt; Papers offer its own reader, which opens the pdf inside the app. Highlighting and text-box comments are easy to make, and these annotations show up on the righthand side toolbar during reading (reader-view) and preview (library-view). You can also write plaintext notes within the toolbar. It looks like highlights made in Preview are visible in the Papers reader, but the other way around doesn’t always work until the library (and pdfs) are formally exported out of Papers, which show up as external notes in all the other reference management programs. Minor inconvenience.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-app1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-app1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-app2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-app2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pa-app3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pa-app3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Mobile/Offline Syncing (5/10):&lt;/strong&gt; Because the Mac app runs locally, and all pdfs are stored locally, reading offline on the computer is not an issue. In contrast, while the iOS app attempts to mirror the same kind of functionalities on the iPad, it falls short. Theoretically, this is the perfect iPad companion app. Practically, it’s unreasonably buggy and slow, so much so that I actively don’t want to use it. Again, the tool is as great as how often you actually use it. For some reason, syncing changes to &amp;lt;10 pdfs take multiple minutes, especially annotations and tags. I don’t know if it’s because I’m doing it over Wifi, as opposed to over Dropbox, but I suspect it’s because of the virtual disk system Papers sets up to manage and store the pdfs (don’t care enough to dive into, and I don’t quite understand why it needs to be that complicated). To be honest, if there was any prospect of this thing getting better through updates, I would probably happily suffer for a few more months. But as it stands, unlikely.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Storage/Search (4/10):&lt;/strong&gt; Papers used to have great Spotlight integration, such that both titles and text within the pdf can be searched. For some reason, that functionality is gone. No matter, Papers comes with a tool called Citations, which serves as the citation insertion tool, but also a private Spotlight. It works really well, I almost always find the paper I’m looking for within a few words. Huge caveat here, though: from around 6 months ago, Citation has been crashing 75% of the time when called up via hotkey. I’ve emailed support and they say other customers have been experiencing similar issues, and basically told me to reinstall as a solution. Well, I tried, and it didn’t work, which makes me more skeptical of continued support (though the support staff I interacted with were all very pleasant and tried to be helpful.) Searching within the Papers app, though, gives you the joyful experience of traveling back in time, like if you were using a Win95 PC, especially when searching by author. For some reason, this takes literally forever (I have about 1200 pdfs), to the point that it lags my computer as I type in the author’s name in the search bar. Again, not exactly sure why, but I think it relates to the way Papers implements its virtual file storage architecture. All that complexity is fine if things worked well under the hood, but it doesn’t, so I’m very disappointed about what was otherwise an amazing tool.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Citing (6/10):&lt;/strong&gt; As I mentioned earlier, Citation is the magic tool that integrates search and citation embedding in one go. Citation integration with Word is fantastic, where inline citations are inserted as citekeys (placeholder tags). Once you’re done writing, it’s a one-button click to convert all the inline refs into whatever format you had declared, and there are plenty of styles available, including all the common ones (APA, IEEE, etc.). It also automatically populates a reference section at the end of the document, in the said format. In contrast, it’s not at all integrated with Google Docs, and the magic formatting doesn’t work there. However, you can paste the full formatted version of any reference directly into a editable text box anywhere, including Google Docs. It’s much less handy for keeping track of the inline citations, as you’d have to manually edit those and manually keep the correspondence between inline citations and formatted reference.&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;In Summary:&lt;/strong&gt; It was good while it lasted - Papers was a great tool for the last few years. It seemed like the savior of academic reference management in its earlier days. As it grew over time, however, the software just got more bloated and fragile. Since the merger with ReadCube, it feels like they’ve abandoned the current app and are actively funneling users to the subscription-based service, which constantly reminds me of the intrusive and cluncky web pdf reader adopted by the Nature family journals. Well, if the developers themselves are telling you to use another service, who am I to say otherwise?&lt;/p&gt;

&lt;p&gt;Go to: &lt;a href=&quot;#papers-3&quot;&gt;Papers 3&lt;/a&gt; | &lt;a href=&quot;#readcube&quot;&gt;ReadCube&lt;/a&gt; | &lt;a href=&quot;#zotero&quot;&gt;Zotero&lt;/a&gt; | &lt;a href=&quot;#f1000workspace&quot;&gt;F1000&lt;/a&gt; | &lt;a href=&quot;#paperpile&quot;&gt;Paperpile&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;readcube&quot;&gt;ReadCube&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Price&lt;/strong&gt;: $36/year (free with institutional subscription)&lt;/p&gt;

&lt;p&gt;I originally wasn’t even going to give ReadCube a try, out of principle, due to its relationship with Nature Publishing Group and the paper rental service model. But then I decided that I should really be comprehensive. So I begrudgingly downloaded it and imported my entire library into it, which was easy, if a little slow. I grabbed the Chrome extension as well, which let’s you import papers directly from a journal html page. There is a desktop app, a web app, and an iOS app, which means preserved functionalities while offline. I poked around both the web and Mac app, the interface design is consistent between the two and pleasing to the eye, with most of the space dedicated to the library view itself. It even has a sidebar that displays the citation history and all papers citing the current one you’ve highlighted. As much as I hated the pdf viewer when it randomly popped up on Nature pages, I actually thought it was really polished in the context of the app. Syncing between the web app and Mac app is automatic and painless, and I have no reason to expect otherwise for the iOS app, though it’s unclear where the pdfs are stored.&lt;/p&gt;

&lt;p&gt;It has all the ingredients to be an useful and dependable tool, and surprisingly, I think I would have had a hard time not choosing ReadCube, had it not been for one feature that made me want to banish this thing into oblivion (and hence not write the full review): there are no tags. Okay, that’s not &lt;strong&gt;strictly&lt;/strong&gt; true. You can tag an article, but only as hashtags in the notes area. The hashtags are used to aggregate articles into lists, which is the basic (and only) unit of organization in ReadCube. Hashtags cannot contain space, has no color coding, and all of them are flaccidly listed in the left sidebar with no way of organizing them otherwise. I don’t know what made me more upset: that this app is missing such a simple but key feature, or that it’s a great app in every other dimension. My only other complaint is that it feels a bit clunky and slow at times. This may be due to a combination of the unnecessarily rich visual design for the interface and the nice but superfluous features, like loading the entire citation list in the sidebar.&lt;/p&gt;

&lt;p&gt;And so with that, I end my full review of ReadCube: if you don’t use tags or just plain hate organizing your papers in a sensible way, give this a try, it’s pretty good in every other way. By the looks of it, the new (beta) &lt;a href=&quot;http://support.readcube.com/support/solutions/articles/30000026748-faqs-for-the-upcoming-readcube-papers-app-&quot;&gt;ReadCube-Papers&lt;/a&gt; app retained a lot of Papers 3 organizational features, so that might be worth a try when it’s available for the public. Although, it’s more likely than not that it will still be slow and clunky, especially compared to the more light weight apps like Paperpile, or even Zotero.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Click to expand for screenshots.&lt;/b&gt;&lt;/summary&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-rc-1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-rc-1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-rc-2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-rc-2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-rc-3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-rc-3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;
&lt;/details&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;zotero&quot;&gt;Zotero&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Price&lt;/strong&gt;: $0/$20/$60/$120 (300MB/2GB/6GB/Unlimited Storage)&lt;/p&gt;

&lt;p&gt;Zotero, like Papers 3, is a desktop app that primarily runs offline. The functionalities are very similar to Papers, and so is the interface. The main difference here is that 1) it’s (mostly) free, 2) it’s an open source project, and 3) it doesn’t have a companion mobile app. In exchange, however, there are a few very handy tools that make life easy, which I will highlight below. Note that I haven’t used Zotero or the other apps extensively, like I have with Papers. I have, however, imported my entire library into it and spent a few days working with it AS IF I was using it, by going through all the steps in my workflow.&lt;/p&gt;

&lt;p&gt;One note about &lt;strong&gt;Mendeley&lt;/strong&gt;: I used Mendeley when I first started graduate school, and it was serviceable. I would say that Zotero is basically a faster (and less evil) version of Mendeley, without a mobile app. If you’re happily using Mendeley, all the power to ya. If you are considering switching to Mendeley, I would give Zotero a try first to see if your needs are met.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Click to expand for details and screenshots.&lt;/b&gt;&lt;/summary&gt;

  &lt;p&gt;&lt;strong&gt;Scan/Importing (10/10):&lt;/strong&gt; pdf drag and drop works smoothly, and you can add items by DOI, so it’s very arXiv/bioRxiv friendly. The most convenient add-to-library function, though, is the Chrome extension. Clicking the toolbox on any page will import that reference into your Zotero library, and the transfer between Chrome and the Zotero app itself is seamless. This works for both the html and the pdf page, and it automatically grabs the pdf into your library. Not only that, you can tag the newly added reference in the extension toolbar itself, as well as direct it into the relevant folder, so you don’t need to leave Chrome at all. The only thing it lacks is a confirmation button, as clicking the extension button automatically adds the page you’re on, and I’ve accidentally done it a few times. I looked around, but didn’t find an integrated database scan option, though there is an automatic “import from feed URL option”. Either way, I think the Chrome extension more than makes up for it while using Google Scholar or PubMed. Finally, batch import via .ris files exported from Papers 3 works perfectly, even getting all the tags and notes right.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-zo-1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-zo-1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-zo-2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-zo-2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-zo-3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-zo-3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-zo-4.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-zo-4.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-zo-5.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-zo-5.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Organization (10/10):&lt;/strong&gt; The basic features are all there, very much like Papers. You can tag a reference, as well as assign references into non-exclusive folders for different projects. There are less options than Papers, though. For example, it lacks flag/no flag and star rating. But those can easily be compensated for with a few custom tags. I do really like the fact that you can assign groups of tags to the same color, which makes browsing tags a lot easier. Also, as per standard, main library view offers many columns (author, title, etc.) and you can sort by any. In short, nothing to write home about here, but that’s arguably a good thing because everything just works well and quickly.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Reading/Note-taking (6/10):&lt;/strong&gt; Zotero does not ship with an in-house pdf viewer, so on a Mac, it’s automatically opened in Preview. I’ve never quite decided whether I like native pdf readers more or embedded pdf readers, so your personal preferences may vary. Aside from aesthetics, the main issue is how well your pdf highlights/comments get parsed as notes, and later, how well it transfers out of the reader app. In this case, Preview keeps the highlights, which are visible in other pdf readers. Extracting annotation from the pdf is not supported by Zotero itself, but there’s a handy open source add-on called [Zotfile][zotfile] that can perform the extraction and turn highlighted words into notes. You can run this operation in a batch fashion, but it is not automatic, so this whole operation is slightly inconvenient. Adding notes is simple enough in the side bar, though you may have to switch windows constantly if your screen can’t fit the pdf and Zotero together.
[zotfile]:http://zotfile.com/&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Mobile/Offline Syncing (5/10):&lt;/strong&gt; Since Zotero lives locally on the machine, there’s no syncing necessary if you read on your laptop, and you can use it wherever you are, no Wifi required. My main (and only real) complaint about Zotero, and ultimately the reason I did not go with it, is that it does not have a companion mobile app. There used to be a third party app called Papership that basically did all the file management, but it’s been abandoned as of a few years ago. Zotfile, once again, offers a reasonable but inconvenient solution: if all the pdfs live on Google Drive, it’s able to pull any annotations and updates and sync with the local files. This means you can read the pdfs in your preferred pdf reader app on your tablet, highlight and annotate, and Zotfile will make the transfers. It works, but it’s clunky. You can’t use the functionalities of Zotero while on the iPad, meaning no tagging or categorization, or search, for that matter. Also, Google Drive, for whatever reason, does not allow entire folders to be downloaded offline, so you’ll have to manually “make file available offline” anytime you add new papers. It’s not a huge deal either, but the Drive UI doesn’t really make this an easy job.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Storage/Search (10/10):&lt;/strong&gt; All pdfs can be stored in a single folder, be it a local one or a synced folder, so file management is painless. If you choose to shell out some money, I think your pdfs are synced with Zotero’s server. Or you can use your free 15Gb from Google. Another feature Zotfile offers is batch renaming of the pdf files based on some arbitrary rule you can define, like author_title_year.pdf. This means relevant search words in the title can be picked up by Spotlight, though whole-text search is less dependable. Search within the app is fast, which should not come as a surprise for a piece of modern technology managing a few thousand pdfs (&lt;em&gt;ahem Papers 3&lt;/em&gt;). Anyway, my &lt;strong&gt;favorite&lt;/strong&gt; Zotero feature is the ability to select multiple tags in the toolbar as an “AND” search: when you click on one tag (“oscillation”), all tags except the ones co-tagged to “oscillation” are eliminated, so you can iteratively click on overlapping tags to filter your search. This is an awesome feature and I don’t know why none of the other apps have adopted it, it makes searching so easy when you can’t think of any search words. Granted, you need to have a reasonable tagging system.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Citing (7/10):&lt;/strong&gt; Zotero lacks a magic genie citation insertion tool that runs in the background like Papers 3, so you can’t search for references unless actively writing inside a Word document. The Microsoft Word add-on is minimalistic but fully functional, though it requires Zotero to be running as well. In any case, all you need here are two buttons: “Insert Inline Citation” and “Format Bibliography”, with some way of controlling the citation style, and Zotero does this fine. When searching for the reference, it takes you into the main app and opens up a sleek search bar similar to Papers. Disappointingly, Zotero does not have Google Doc integration, and its &lt;a href=&quot;https://www.zotero.org/support/google_docs&quot;&gt;documentation&lt;/a&gt; suggests dragging and dropping from the app, though that will of course require manual shuffling when you need to change some references. Alternatively, you’re offered the option of downloading and uploading the GDoc file as a Word doc, or manually insert cite keys when you write and run format once in the end. Any changes made online will have to be taken offline and synced. This mirrors the situation with mobile reading - you can make it work if you want to, but will be a pain in the ass if Google Doc is your main tool.&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;In Summary:&lt;/strong&gt; Zotero is an excellent (and free!) tool that I would highly recommend if you mostly read on your computer and write in Microsoft Word. You’re essentially getting the same organization, functionality, and aesthetics out of an open source piece of software as Papers 3 (or Mendeley), except you get to keep your money (or soul). In this day and age, and with Zotero’s sizeable existing user base, it seems that being open source is not at all disadvantageous to continued development. If anything, it takes a smaller community of dedicated users, or even a single individual, to add or improve some functionalities, as was the case with Zotfile. But if you depend heavily on Google Doc for writing, or read primarily on a synced mobile device, then unfortunately it will be difficult to use.&lt;/p&gt;

&lt;p&gt;Go to: &lt;a href=&quot;#papers-3&quot;&gt;Papers 3&lt;/a&gt; | &lt;a href=&quot;#readcube&quot;&gt;ReadCube&lt;/a&gt; | &lt;a href=&quot;#zotero&quot;&gt;Zotero&lt;/a&gt; | &lt;a href=&quot;#f1000workspace&quot;&gt;F1000&lt;/a&gt; | &lt;a href=&quot;#paperpile&quot;&gt;Paperpile&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;f1000workspace&quot;&gt;F1000Workspace&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Price&lt;/strong&gt;: Free, $10/month for additional features and storage (free with institutional subscription).&lt;/p&gt;

&lt;p&gt;F1000Workspace is the reference manager in the F1000 ecosystem (with F1000Prime and F1000Research), short for Faculty of 1000. I have no idea what that means, or why someone decided they had to use those same words to name the services as well. F1000Workspace is a web-based app, and it’s honestly the quintessential embodiment of what the public thinks of scientists (and engineers): functionally, it really does its job, and that’s that. It has no fluff, it doesn’t care what anyone thinks about how it looks (perhaps to a fault), or what its name is.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Click to expand for details and screenshots.&lt;/b&gt;&lt;/summary&gt;

  &lt;p&gt;&lt;strong&gt;Scan/Importing (9/10):&lt;/strong&gt; Importing references is easy: from the web app interface, you can drag and drop local pdfs from your computer or import from various cloud storages. Alternatively, importing batch-wise via a .ris file worked just as well, along with my notes, but tags did not import. It seemed to have a little trouble matching references that were sections from a book, compared to Papers at least. But for all my article pdfs that were indexed in PubMed or otherwise had an easy to access DOI, it worked just fine. Like Zotero and Paperpile, it has a very convenient Chrome extension that allows you to grab references from the journal site you’re currently on, or the pdf link itself. It even offers tagging and filing from the extension, which I love. This means you can read through once, import and organize in one go without ever visiting the web app itself, and forget about it until you need to cite it. &lt;strong&gt;Other apps should take note&lt;/strong&gt;: it’s a small convenience, but would save the user a ton of time over the years if they didn’t have to visit the main app to file and tag. You can also download the offline companion app and set up a to-watch folder on your computer, where pdfs are automatically imported.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-4.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-4.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-5.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-5.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-6.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-6.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-7.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-7.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-8.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-8.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-9.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-9.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Organization (5/10):&lt;/strong&gt; F1000 has the standard pair of organizational tools: folders and tags (tags can have colors as well). Instead of “Folders”, F1000 calls them “Projects”, which serves the exact same role as far as I can tell. The only difference being that you can file references into a “Shared Project”, which makes collaboration easier than exporting a folder of references and emailing it, though I’m not sure if the recipient also needs to be registered with F1000. The downside to the free version is that you can only have 3 projects at a time, which is a pretty small number, but this limit is bypassed with institutional or paid subscription.&lt;/p&gt;

  &lt;p&gt;One thing I really don’t like is the app interface itself in Library view. There is a ton of dead space you can’t control, and with a reasonably sized screen, the actual reference list panel is tiny and you’re limited to looking at ~10 references at a single time. What’s worse is that the fields are truncated to fit more of them. Sure, it’s not like you will actually be searching through references by scrolling all the time, but any keyword that returns 10+ results will be a pain to scroll through. Oh yeah, and scrolling does not auto-populate the next set of references, so you need to click the “next page” button. This certainly makes me feel a little claustrophobic, and I fear that some references will be buried forever because I will never click through 20 pages to explore refs I might have missed while writing.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Reading/Note-taking (10/10):&lt;/strong&gt; F1000 offers a web pdf reader app that is packed with note-taking features. You can add highlights, sticky notes, and even notes to your highlights. From your library, you can also directly access the article html site, and even make notes and highlights on there that syncs to the pdf itself when the little ‘F’ pops up. That’s pretty cool, and is great for that first-time read through. Bonus if you like to read the html version. Overall, I think this should be the gold standard of reading and filing articles on the web.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-app1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-app1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-app2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-app2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-f1-app3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-f1-app3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Mobile/Offline Syncing (2/10):&lt;/strong&gt; The biggest weakness with F1000, as you might expect, is its (in)ability to function offline. If you’re not connected to the web, the web app obviously will not work. This means you cannot offline search, read, file or &lt;strong&gt;even cite&lt;/strong&gt;, because you cannot access the database. F1000 offers one work around: while you’re still online, you can add references to a “reading list”, which the offline companion app can access and open the associated pdf. Key caveat here is that you every time you quit the app, you have to sign in the next time you open it, which you cannot do offline. So overall, this is rather pointless, especially if you save your pdfs locally as well. F1000 has an iOS and Android app that looks identical to the web app and pdf reader, and poetically, it suffers from the same offline issue. There is no way to locally save pdfs on the iPad, and if you’re offline, the app is basically a dud. In other words, the mobile app turns your tablet into a screen that you can take with you but only when there’s Wifi. The upside is that syncing works quickly and flawlessly. I suppose this is not unexpected if the app basically opens a tablet version of the web app. In the end, I like writing and reading on the bus/plane/subway, so offline access is crucial for me, especially on mobile. But depending on your needs, this might not be a huge issue.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Storage/Search (8/10):&lt;/strong&gt; Since F1000 exists as a web app, you can only search in the browser, i.e., no magic tool. Nevertheless, keyword matching is satisfactory, as it searches in title, abstracts, and highlighted pdf text/annotations. The downside, like I mentioned above, is that you cannot search offline, and any search result that returns more than 20 reference will be painful to work through, especially if you don’t know these references by the first 5 words of the title already. The upside to F1000 is that tag searching works conjunctively, like in Zotero, so clicking on multiple tags will return the intersection. I &lt;strong&gt;really&lt;/strong&gt; love this feature, since my tag system includes multiple taxonomies. So it will be incredibly easy to find studies that look at, for example, human + oscillation + ECoG + attention, based on my own tags, instead of relying on text search. Storage-wise, all the pdfs seem to be magically stashed somewhere, perhaps on an F1000 server. Paid (and institutional) subscriptions offer unlimited storage, so this will never be a problem. Batch exporting pdfs is straightforward as well so no need to worry about losing all your references into the abyss.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Citing (10/10):&lt;/strong&gt; F1000 actually offers the most comprehensive integration with both Word and Google Docs. The Word add-on includes a full suite of tools, including the ability to directly submit your manuscript to F1000, though I cannot imagine that was by popular request. The standard ones, however, work well, and you can actually search by tags, projects, and search Google Scholar and PubMed from the toolbar in addition to your F1000 library. Since it doesn’t work at all offline, it offers the ability to suggest references for manually inserted citekeys, so that you can still write offline uninterrupted, and add in the references when you’re online again. The Google Doc extension works just as well, though my one small complaint is that the citation tool (on both platforms) is a bit slow, as it seems to load the entire library database when it’s first opened. That is a small price to pay for both GDoc and Word compatibility.&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;In Summary:&lt;/strong&gt; F1000 is kinda like the web app version of Zotero. It’s a great tool, and aside from offline access, it prioritizes all the key features (importing, filing, reading) and it does them well. Equally important, it doesn’t offer useless features, well, except for aspects of the Word plug-in. The full features may be accessible to you for free if you work at a North American or European institution, and it even has both Word and GDoc integration. If you read and do most of your work while online, and don’t mind the awful interface too much, I would highly recommend trying this tool. But if you need any offline access at all, say, while commuting or during field work in remote places, then you’re out of luck.&lt;/p&gt;

&lt;p&gt;Go to: &lt;a href=&quot;#papers-3&quot;&gt;Papers 3&lt;/a&gt; | &lt;a href=&quot;#readcube&quot;&gt;ReadCube&lt;/a&gt; | &lt;a href=&quot;#zotero&quot;&gt;Zotero&lt;/a&gt; | &lt;a href=&quot;#f1000workspace&quot;&gt;F1000&lt;/a&gt; | &lt;a href=&quot;#paperpile&quot;&gt;Paperpile&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;paperpile&quot;&gt;Paperpile&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Price&lt;/strong&gt;: $36/year&lt;/p&gt;

&lt;p&gt;The best way to describe Paperpile is that it’s like the minimalist hipster child of Papers 3 and F1000Workspace. It’s a web-only app with a sleek interface that is, in my opinion, a lot more pleasing to the eye. The strength (and limitation) of Paperpile is that it links to (and is locked to) a Google account and takes advantage of your Drive space and organization. You’re limited to online use on the computer, but the iOS app is fully functional offline. This is ultimately the one I ended up going with, as it fits my workflow well and I really like the minimalist design of both the web and iOS app.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Click to expand for details and screenshots.&lt;/b&gt;&lt;/summary&gt;

  &lt;p&gt;&lt;strong&gt;Scan/Importing (9/10):&lt;/strong&gt; Drag and drop pdf importing, .ris (and other reference file format) batch importing, and direct DOI importing are all available and work well. It also has a Chrome extension that snags the reference and its associated pdf from the journal html website. The weird thing is that the extension cannot import when you’re on the pdf page instead of the html page - mildly disappointing. Also, I wish the Chrome extension supported filing and tagging on the spot, instead of having to do it from the web app (though dev forum suggests this is in the works). One cool thing is that it is totally integrated into Google Scholar (and other databases, like JStor), so that you can grab references directly from search results.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-4.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-4.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-5.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-5.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-6.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-6.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-7.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-7.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-8.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-8.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-9.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-9.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Organization (10/10):&lt;/strong&gt; Paperpile offers tags, folders, and star labeling, which can all be accessed from the left sidebar. Tags can be colored for easier visualization, and folders can have an infinitely deep subfolder structure, which I believe is unique to Paperpile. Shared folders can also be made, so it’s designed with collaboration in mind. As for the app itself, it displays an even smaller number of references than F1000 at full window size (6.5 on my screen). But for some reason, I don’t mind it so much. I think it’s because the list view actually takes up more than half the screen, and all relevant information for the displayed articles are actually there, with full bolded titles. On top of that, scrolling is smooth and automatically populated as you reach the bottom, so it’s a much more pleasant experience. Another nice touch is the gmail-style keyboard shortcuts, including Shift-Arrow to select multiple, and Cmd-C/B/K to copy as full reference or citekey. I wonder how many people made it to this point without rolling their eyes and saying “just use latex”.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Reading/Note-taking (10/10):&lt;/strong&gt; Paperpile’s old web pdf viewer was atrocious. It basically used the native Chrome pdf viewer, which means no highlighting, no annotations, no nothing. Recently, they’ve rolled out their standalone pdf viewer app called “MetaPDF”, which can be selected as the default reader opened by Paperpile. This is the polar opposite of that old pdf viewer. First of all, and most importantly, it’s pretty. Second of all, you can make highlights, notes, and notes for your highlights. You can even draw shapes and stuff, and all these are embedded into the pdf itself and readable in other viewers. This very much rivals F1000’s online pdf viewer, and pretty much all the embedded pdf viewers of the offline apps on this list. Taking longer notes can be done from Paperpile’s library view itself, but not from within the pdf viewer.&lt;/p&gt;

  &lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-app1.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-app1.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-app2.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-app2.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2018-09-20-refman-pp-app3.png&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2018-09-20-refman-pp-app3.png&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

  &lt;p&gt;&lt;strong&gt;Mobile/Offline Syncing (7/10):&lt;/strong&gt; Being a web app, Paperpile suffers the same fate as F1000 when you’re offline, since you can’t even access the URL. Again, this means no reading, writing, or filing. However, there is a beta version of the iOS app that you can request access for and download via TestFlight. The app is fully functional offline, meaning you can tag and annotate, and changes will be synced once you’re back online, as long as the pdfs are also downloaded onto the iPad beforehand. In my experience, the syncing has been painless, both for tags and annotations made in the pdf. There’s no batch download yet, but at least the download button for each reference is huge. This capability somewhat offsets the lack of offline functionality for the web app, as personally I’m much more likely to read offline on my iPad than on my computer. Still, if you must use it offline on your computer, then look elsewhere. They did not pay me for this post. Sponsorship please?&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Storage/Search (7/10):&lt;/strong&gt; Paperpile is linked to your Google account, and it uses Google Drive storage space to store the pdfs. This is potentially very nice or very limiting, depending on how capped your Drive space is, though my library of 1500 pdfs only takes up about 3Gbs. The convenient thing about having the pdfs on Drive is that they can be synced to your computer as a Drive folder, so the pdfs can be accessed offline. Although, you need to be a bit careful about housekeeping since the developers recommend making changes to your library strictly through the Paperpile app, and not by moving files around in the Drive folder. Overall, this is an elegant shortcut, and it’s likely that Google Drive will be around for a while still. But depending on your stance on privacy, having yet another aspect of your life tied to Google may be unideal. Search bar searching within the app is satisfactory, and is limited to keyword matches in the title, abstract, authors, and notes. This is a step below the full text and within-pdf annotation search features of F1000 and Papers 3.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Citing (6/10):&lt;/strong&gt; Currently, Paperpile only has citation integration for Google Doc. It’s fast, accurate, and has a nice feature for searching directly online while inserting citations. If your “friend” is someone who writes down off-the-cuff claims and finds references later to back them up, this is an invaluable addition. On the other hand, it currently does not have a citation tool for Word, so you’re stuck with copying refs in the main web app and pasting them manually, or uploading your Word document and converting it to a Google Doc and adding all your references afterwards. Fear not, however, it looks like a Word plug-in has been in the works and will be pushed out very soon, as well as offline usage, so it may be worth to stay tuned.&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;In Summary:&lt;/strong&gt; Of the apps I reviewed, Paperpile seems to have the biggest potential to improve. The development team is aware and routinely engages with feature requests on the user forum, and my own experience with the in-app chat has been excellent as well. I sent a message to request the beta iOS app, and someone replied within 24 hours and also extended my trial by two weeks to make sure I had plenty of time to test with the app in my workflow. It really is nice to have that dependable human support in your products. Looking through the forum, my hope is that the crucial features (e.g., Word plug-in, better Chrome extension, offline capabilities) will be rolled out in 6-12 months. But as of now, Paperpile is a great fit for anyone that already works online and within the Google ecosystem, and enjoys the UI/UX it offers.&lt;/p&gt;

&lt;p&gt;Go to: &lt;a href=&quot;#papers-3&quot;&gt;Papers 3&lt;/a&gt; | &lt;a href=&quot;#readcube&quot;&gt;ReadCube&lt;/a&gt; | &lt;a href=&quot;#zotero&quot;&gt;Zotero&lt;/a&gt; | &lt;a href=&quot;#f1000workspace&quot;&gt;F1000&lt;/a&gt; | &lt;a href=&quot;#paperpile&quot;&gt;Paperpile&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;details&gt;
  &lt;summary&gt;&lt;b&gt;Bonus:&lt;/b&gt; code for generating the flowchart in mermaid-compatible editors.&lt;/summary&gt;
  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;```mermaid
graph TD;
    search(&quot;Scan
    &amp;lt;br&amp;gt;- search online database/catalogue
    &quot;)
    import(&quot;Import
    &amp;lt;br&amp;gt;- browser import data/pdf
    &amp;lt;br&amp;gt;- ref data matching
    &amp;lt;br&amp;gt;- detect duplicates
    &amp;lt;br&amp;gt;- cross-platform import
    &quot;)
    read(&quot;Detailed Reading
    &amp;lt;br&amp;gt;- reader interface
    &amp;lt;br&amp;gt;- annotating/notes
    &amp;lt;br&amp;gt;- highlighting    
    &quot;)
    file(&quot;Organization
    &amp;lt;br&amp;gt;- tags/categories/(un)read
    &amp;lt;br&amp;gt;- project folders
    &quot;)
    storage(&quot;Storage/Search
    &amp;lt;br&amp;gt;- pdf storage
    &amp;lt;br&amp;gt;- Spotlight search
    &amp;lt;br&amp;gt;- within pdf search
    &quot;)
    cite(&quot;Citation
    &amp;lt;br&amp;gt;- GDocs/Word integration
    &amp;lt;br&amp;gt;- ref formating (MLA, APA, etc)
    &quot;)
    sync(&quot;Sync
    &amp;lt;br&amp;gt;- pdf sync/offline access
    &amp;lt;br&amp;gt;- annotation sync
    &amp;lt;br&amp;gt;- organization sync    
    &amp;lt;br&amp;gt;- mobile/web app
    &quot;)
    other(&quot;Other Factors
    &amp;lt;br&amp;gt;- longevity
    &amp;lt;br&amp;gt;- cost
    &amp;lt;br&amp;gt;- collaborative
    &quot;)

    search--keep for ref--&amp;gt;import
    import--read abstract--&amp;gt;file
    file--read figures/text--&amp;gt;read
    read--&amp;gt;storage
    storage--writing--&amp;gt;cite
    file--mobile/offline--&amp;gt;sync
    sync--upload--&amp;gt;storage
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;You made it to the end! Now pick your starter Pokemon and go forth into the wild world of reading lots of papers you will never remember!&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RKHswIdTdVk?rel=0&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Richard Gao</name></author><category term="Science" /><category term="Data &amp; Technology" /><summary type="html">Naturally, I flipped through the internet to see if there exists a comparison of the more recent tools available to help me make a decision. To my extreme surprise and exasperation, I found none. So I decided to bite the bullet and try all the ones I could get my hands on to see which one I liked more. This took me a good month or so, but hey, now that I did the hard work, you don't have to.</summary></entry><entry><title type="html">CCN2018: Thoughts and Hot-Takes</title><link href="http://localhost:4000/CCN2018/" rel="alternate" type="text/html" title="CCN2018: Thoughts and Hot-Takes" /><published>2018-09-09T00:00:00-07:00</published><updated>2018-09-09T00:00:00-07:00</updated><id>http://localhost:4000/CCN2018</id><content type="html" xml:base="http://localhost:4000/CCN2018/">&lt;p&gt;This is arguably a pretty lukewarm hot-take, but I slept for 12 hours last night after bombarding my brain with brain stuff (and beer, and Philly foods) for 4 days straight, and finally felt like I could string together coherent words again. No promises, though.&lt;/p&gt;

&lt;p&gt;Immediate post-conference thought: there were a lot of man-buns, and one Philly cheesesteak was enough for me.&lt;/p&gt;

&lt;p&gt;In all seriousness, this was my first CCN and I had a lot of fun in Philly. The smaller scale of the conference and the organizers’ effort to emphasize socializing made the experience a lot more personal, especially compared to SfN. It was eye-opening learning about all sorts of things that I’m not immediately exposed to, though there was way too many deep neural networks for my personal taste. That being said, I really hope this conference continues for years to come and crafts an identity complementary to NIPS and COSYNE. Here are some thoughts in 4 sections:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#some-favorite-talks&quot;&gt;personal favorite talks&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Alison Gopnik, Ryan Adams, Dani Bassett, Ugurcan Mugan&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#on-mind-matching-and-socializing&quot;&gt;thoughts on the mind-matching session&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;awesome idea, would love to see this evolve. Even randomly matching would be fun, with some suggested ice-breakers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#on-the-panel-discussion-and-debate&quot;&gt;on the panel debate and what question should we be asking ourselves&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;spoiler alert, it’s not “when would we be done with neuroscience?”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#on-the-conference-content-as-a-whole&quot;&gt;thoughts on the research presented (poster and talks) as a whole and suggestions for the future&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;too much deep learning ~ brain and not enough LFPs, oscillations, dynamical systems, and embodied cognition/computation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;some-favorite-talks&quot;&gt;Some favorite talks&lt;/h3&gt;
&lt;p&gt;I enjoyed &lt;strong&gt;Alison Gopnik&lt;/strong&gt;’s opening keynote, because it injected the conference with a good dose of experimental &amp;amp; developmental psychology but also drew from learning algorithms, nicely rounding out the Venn diagram in CCN. She posed the question of why it would be beneficial for animals with higher adult intelligence to have such a long developmental period with relatively poor cognitive abilities. The punchline is that, instead of thinking about the long developmental period and playing as defective or yet-to-be matured intelligence, that we should think about it as the exploration phase during reinforcement learning (explore vs. exploit strategies), or the high-temperature phase during simulated annealing. In plain English, it means children mess around and choose suboptimal choices not because they’re stupid mini-adults, but that our species’ brains had evolved such that it values trying out different options even if with it comes a short-term cost. She went through some experiments that I won’t repeat here, but the takeaway is that children were quicker in learning rules in lab games (e.g., Blickets game) that would otherwise contradict learned experiences of, say, an undergrad or an adult.&lt;/p&gt;

&lt;p&gt;The talk appealed to my intuition on how children behave and gave a plausible reason from the perspective of evolutionary biology, and it has potential implications on early education and parenting. My biggest question here is: what is it that the children are actually learning during this period of high-temperature annealing and hypothesis-space exploration? All the experiments demonstrated that they were able to flexibly learn constrained game rules during childhood, and suppose it generalized to real-life problems, it’s not clear how humans would benefit from this period of learning unless they were able to take some of the principles into adult life, and I’m not sure what those principles are. One plausible idea is that children, during play/exploration, learn that making mistakes is sometimes good, and the memories of these rewarding mistakes could carry into adulthood such that it motivates them to do “noisy gradient descent” as a policy during adult life, in order to avoid locally optimal solutions. If you don’t learn this as a child, you might eventually learn this as an adult and end up packing up your life during a mid-life crisis. Too real.&lt;/p&gt;

&lt;p&gt;Side note: it was awesome that she made a point to ask the women in the audience for questions first, setting up the tone for the rest of the conference, and indeed there were some great questions.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;I liked &lt;strong&gt;Ryan Adam&lt;/strong&gt;’s keynote as well, as it was a very digestible overview of deep learning and probabilistic graphical models for those of us that are not in the thick of things in ML. &lt;strong&gt;Tim Lillicrap&lt;/strong&gt;’s talk was the complement of this, meaning a lot of mathematical details for a specific learning algorithm, namely, combining reinforcement learning with an indexable memory element. Both talks appealed to neuroscience at a very cursory level (e.g., sparse memory in the hippocampus), and focused on the advances it made in ML, which I didn’t mind because they were educational for an outsider trying to understand the state of the art in this related but tangential field. One theoretical takeaway that’s consistent with the rest of the conference was that people are really looking for modularity and structure in learning algorithms, as the dense and homogeneous architecture of most neural networks simply do not give us the interpretability we want as humans, especially those that want to understand some aspect of the brain as well. At the end of the day, scientists like low-dimensional abstractions as a measure of understanding, and I tend to agree here.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;One of my favorite talks was a contributed talk by &lt;strong&gt;Ugurcan Mugan&lt;/strong&gt;, who argued that animals who could see farther (i.e., those that have bigger eyes and live on land) have a better ability to plan in a long term fashion, and therefore developed a better ability to plan in a long term fashion for survival (continuation of &lt;a href=&quot;http://www.pnas.org/content/early/2017/03/09/1615563114.short&quot;&gt;this work&lt;/a&gt;). If you’re a fish that lives in the sea - an environment with very low visibility - there’s no point for you to be able to predict what will happen 20 meters out because you will never acquire the information required to update those plans. Note that this argument only applies for vision, and it’s possible that it will break down when you account for other senses, like olfaction in sharks. Nevertheless, it’s a great example of considering environmental factors AND our physical hardware when taking into consideration the task the brain has to solve in the context of the only thing that matters during evolution - survival. Another exciting element about this work is the potential implications about memory: space and time is inseparable, and if you can see further, then you need to plan further in time. And if you need to plan further in time, it stands to reason that you need to remember further back in time. It’s a conjecture, and I’m not well-versed enough on the embodied cognition literature to know whether this is old news or a dumb idea, but it would be cool to artificially extend the physical range of an animal’s sensory organs (arm reach or visual range) and see whether it will impact it’s ability to consider information at longer timescales. Maybe we can prevent climate change by circumventing the literal shortsightedness of human beings and give everyone VR goggles that display what’s going on in a 10km radius.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Dani Bassett&lt;/em&gt;’s concluding keynote was simply masterful. I know very little about network science, but she went through a comprehensive body of work in the lab, starting with human learning of graph structure from sequential sampling, to graphs in the brain and how cognitive flexibility relates to dynamic flexibility in brain networks. Aside from the science, I loved the structure of the talk, as well as the transitions with quotes from poets and historians. If nothing else, her talk serves as a great template on how to give an entertaining science talk, which is hard to do as the ending keynote of an intense 12-hours-a-day conference! My lingering question from this talk is the role of subcortical neuromodulatory structures on graph control in the brain. She talked about how certain nodes in the (brain) graph had better controllability by electrical stimulation, and it would be interesting to see what native mechanisms the brain has for this type of large scale control, especially when switching tasks. I think we call this thing “attention”, and it’d be incredibly cool if we could infer subcortical projection of norepinephrine or dopamine neurons based on a node’s controllability.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;on-mind-matching-and-socializing&quot;&gt;On mind-matching and socializing&lt;/h3&gt;
&lt;p&gt;I really appreciated the mind-matching session. I think it’s a fantastic new idea for conferences, especially if it’s ever implemented at larger ones like SfN. Aside from being matched with people in their own labs, which seemed to be the main complaint, it was just a great opportunity to meet people. To be honest, I think whether being matched with someone in or outside my subfield doesn’t really matter. Getting matched with someone outside my field meant learning something new and establish potential collaborations (which I think was the intention), and getting matched with people in my own field meant geeking out over similar advances and problems.&lt;/p&gt;

&lt;p&gt;Let’s face it, scientists are not the greatest at socializing (myself included), and one of the most valuable things at a conference is meeting people (shout out to &lt;a href=&quot;https://twitter.com/neurograce&quot;&gt;Grace Lindsay&lt;/a&gt; for organizing the Twitter meet up! Even though I didn’t actually end up meeting you…) By the looks of how many people signed up and vigorously participated in the mind-matching, people really wanted to talk to each other, but perhaps it’s just difficult to get over the initial awkwardness of making the first introduction without any real reason to. Poster sessions are great for socializing, but obviously it’s restricted to people doing similar things, though sometimes I will literally go to a random poster that’s empty just to talk to people. Also, having this session relatively early in the conference was also a great idea, because it allowed for repeated interactions afterwards just by virtual of bumping into each other over the course of the next few days. Shout out to &lt;a href=&quot;https://twitter.com/bob6294&quot;&gt;Jason Kim&lt;/a&gt;, who I matched with and later waved me over to introduce me to a few other Bassett lab members at the poster session.&lt;/p&gt;

&lt;p&gt;I think one potential improvement, aside from not getting matched with people in the same lab, is to provide non-science related starter topics. Or maybe science questions at a very high-level, but not too vague so there can be some concrete discussion. The point is just to make a real connection with someone else, over whatever thing you have in common, and sometimes talking about our specific academic interests may feel a little restrictive and forced, especially after doing the elevator pitch 6 times in a row. Some easy ones:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if you weren’t a cognitive/computational neuroscientist, what would be your dream job?&lt;/li&gt;
  &lt;li&gt;who is your celebrity doppleganger?&lt;/li&gt;
  &lt;li&gt;what are your top 3-5 non-science related books?&lt;/li&gt;
  &lt;li&gt;what are the things you spend the most time doing, other than science?&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;on-the-panel-discussion-and-debate&quot;&gt;On the panel discussion and debate&lt;/h3&gt;
&lt;p&gt;I didn’t love the “Challenges and Controversies” discussion panel, and it’s because I wasn’t sure what the purpose of such a discussion is, particularly with regards to the question of “What would it mean to understand the brain?” I don’t NOT like the concept of such a discussion. I do. I was at CNS earlier this year where Jack Gallant really got into it with Gary Marcus (at the expense of Eve Marder and Alona Fyshe, which is a shame), and if nothing else, it was thought provoking and entertaining. I liked all 3 individual short talks prior to the discussion: each of their preferred research program is interesting and answer complementary and important questions. I think the problem in this case was that the debate was too agreeable to inspire new ideas (or to be entertaining), but at the same time, nobody really conceded ground with regards to their commitment to their individual methodological and philosophical ideologies. I’m not saying the discussion has to be rude, which, to the panelists’ credit, it wasn’t. But perhaps it would’ve been better to play the devil’s advocate a little? At a conference like this, I think everyone understands the value of interdisciplinary and complementary approaches, so it’s no surprise that nobody claimed “my way is the best way”. But perhaps it would’ve have been better to force that particular line of discussion, maybe by constraining the question, e.g., “if we had only a billion dollar left to invest in brain and cognition research over the next 25 years, who should we give this money to and why?”&lt;/p&gt;

&lt;p&gt;Which brings me to the second point: “when will we be done neuroscience” is not a good question to ask neuroscientists. Why? Because I can tell you the answer in one word: never. Tal Yarkoni wrote a great &lt;a href=&quot;https://www.talyarkoni.org/blog/2018/08/18/if-we-already-understood-the-brain-would-we-even-know-it/&quot;&gt;blog post&lt;/a&gt; very recently on this topic of “understanding”. The job of scientists, by definition, is to perpetually ask questions about the thing they are studying, both because that’s what they love doing, and because that’s what they’re paid to do. Modern academia makes it seem like the role of scientists is to answer questions, and that’s partly true, especially in clinical and engineering contexts. But I would argue that the most important thing a neuroscientist does is to ask (good) questions. If that’s the case, the search will never end, as long as a single neuroscientist is alive and still wants to be paid to do what they do. We’ve been doing physics for 400 years, and we still don’t “understand” the world/universe/atoms/anything at 100%. I would say “we” understand the world, because F=ma and I don’t fear for my life whenever I drive across a bridge, but a physicist would find that notion of understanding hilarious and incredibly inadequate. Along that vein, someone had a great comment at the end of the debate (anonymous shoutout because I forgot his name), which was something along the lines of “I would say I knew how a car worked, until my car stopped working and I had to go to my mechanic.” Incidentally, I’ve been in several Uber rides where, upon telling the driver what I do for a living, they responded with “don’t we already know how the brain works?” Maybe I happen to stumble into cars with especially educated Uber drivers, but I really doubt that.&lt;/p&gt;

&lt;p&gt;So the real question here, I think, is “who or what should we prioritize the understanding of brain for”, which can also be reformulated into the above constrained optimization question, i.e., if resource was extremely limited, what’s the minimally viable product we, the neuroscientific community, should aim to provide? Let’s just cut the shit for a second and honestly ask ourselves that question, and I think for a lot of us, the answer would be “ourselves”. Because we are curious and it’s a great job and I would love to ask and answer questions about the brain and tinker with cool toys and computer simulations for the rest of my life, and there’s no shame in that. But if that’s the case, the answer to the original question is clearly “never, or for as long as I’m alive.” But let’s step outside of the neuroscience bubble for a second: many of us have written grants claiming that our science may improve understanding and treatment for those with neurological and psychiatric disorders, which is not false, because in the limit, all these small pieces of knowledge do indeed contribute to the answer. But if we’re truly aiming to help patients as quickly as possible, the immediate best thing to do is probably say “fuck understanding the brain”, and come up with therapies and interventions that would improve behavior and quality of life - for example, DBS. Another practical example: should understanding of the brain equate to successful manipulation of behavior? If the latter is the goalpost, then we should also stop doing neuroscience and start working for Facebook because folks out here are already manipulating &lt;strong&gt;important&lt;/strong&gt; behavior, and neuroscience really has nothing to offer to counter that threat.&lt;/p&gt;

&lt;p&gt;Okay, mini-rant over, but I think these are problems we should think seriously about as a community, because that money river is not going to flow forever, nor should it, and theoretical neuroscience might be the first one to dry up.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;on-the-conference-content-as-a-whole&quot;&gt;On the conference content as a whole&lt;/h3&gt;
&lt;p&gt;In terms of the scientific content as a whole, personally, I felt that there were too many “deep learning ~ brain” comparisons. I’ll preface this with my acknowledgement that I might be on the Hatorade because I don’t work in this field, and if I did, I’d probably find it to be just right, so I’m sorry if I am offending any individual work - please excuse my ignorance! It’s not to say that I find the deep learning and brain comparison uninteresting - quite the contrary, I think there are a lot of interesting ideas to work with here, both in terms of improving architectures with known biological constraints, as well as understanding the brain with the help of a (more or less) functional brain model. For example, the contributed talk by &lt;strong&gt;Bashivan, Kar &amp;amp; DiCarlo&lt;/strong&gt; was very insightful in terms of how we can use neural network models as a testbed to make novel predictions about the brain in processing visual stimulus, which they then confirmed with in-vivo experiments and even worked out independent cell control with visual stimulus! Imagine that, SfN can finally stop sending me emails about optogenetics workshops (joking but also please stop).&lt;/p&gt;

&lt;p&gt;My complaint here is two-fold. First, I felt that this line of work, on average, lacked explicit acknowledgement of the goal. Is it to build a better artificial system by including biological realism, or is it to understand the brain better with a model, or both? I’m not saying the whole field has to subscribe to one or the other, but I think individual works should be know where it sits. And because many weren’t explicit about it, and this is perhaps due to a lack of intimate understanding of the background on my part, a large part of the conference felt like “let’s take this wet squishy thing we don’t understand and correlate it to this dry hard thing we also don’t understand.” Like the above work I cited, if we’re interested in the theoretical link, then I think the true value here is to learn something we didn’t know about the brain by using predictions from deep learning models. It’s really fascinating that certain aspects of deep neural nets mimic the real brain, and perhaps we’re just on the initial wave of mining the similarities, but if we just talk about algorithmic similarities, I’m sure you can find aspects of SVM or even linear regression where weights mimic neural activity in some tasks, but it would be a mistake to say that we’re learning something about the brain.&lt;/p&gt;

&lt;p&gt;My second complaint, which may also have contributed to my philosophical disagreement, is simply the fact that (in my personal opinion) there were just too many posters &amp;amp; talks on this topic. Again, not saying any of them were uninteresting or undeserving, but that I would’ve liked to see more of other things in computational or cognitive neuroscience. One of those things being, obviously, population dynamics and computation observed in LFP and ECoG, and the role of oscillations or high-gamma/multi-unit activity in computation. As far as I know, there were 3 posters on oscillations - one on spontaneously emerging alpha oscillation from a predictive coding network, one on behavioral oscillation in the theta range in a memory task, and the other being my own. There were a handful of other ECoG/EEG/MEG works, but those, while all fascinating in other ways, simply used those signals as an observable. Other than that, and this sounds like a stupid thing to say about a cognitive and computational neuroscience conference, but I felt like there was too little representation of works not focusing on the brain. The talk on visual range I mentioned above being the exception, I think there is a lot of interesting work on computational ethology and embodied cognition that wasn’t here, especially in the area of simulated or physical robotics given the current capabilities of the state of the art deep learning architectures. Maybe this is the West Coast cognitive science Kool-Aid talking, but I’d hate to see CCN fall right back into the “brain is a computer” hole that the previous iteration of cognitive science fell into.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;That’s all for the conference reflection, hope everyone there had a great time. Comments are still down but would love to know what you thought of it on Twitter. And lastly, here’s some vibes to shake off the edge from all that excitement:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lrpm7wyQzNw?rel=0&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Richard Gao</name></author><category term="Reflections" /><category term="Mind &amp; Brain" /><summary type="html">Great, diverse talks; Loved the mind-matching; Panel discussion question was too unconstrained; Could use more non-deep learning stuff as a whole.</summary></entry><entry><title type="html">New (static) website with Jekyll and GitHub Pages</title><link href="http://localhost:4000/jekyll-website-blog/" rel="alternate" type="text/html" title="New (static) website with Jekyll and GitHub Pages" /><published>2018-07-21T00:00:00-07:00</published><updated>2018-07-21T00:00:00-07:00</updated><id>http://localhost:4000/jekyll-website-blog</id><content type="html" xml:base="http://localhost:4000/jekyll-website-blog/">&lt;h1 id=&quot;tldr&quot;&gt;tl;dr:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;SquareSpace is great for bloggers that don’t want to mess with the internal components of website-generation, but has a somewhat slow and clunky interface, especially for markdown and embedding images/links/code.&lt;/li&gt;
  &lt;li&gt;Jekyll + Github is a great alternative, and learning how to use it serves as a friendly intro into using these development tools (git, shell, etc).&lt;/li&gt;
  &lt;li&gt;Blogging workflow, in markdown, in a text editor is SO much smoother, in terms of writing, formatting, and embedding stuff.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;After using &lt;a href=&quot;https://www.squarespace.com/&quot;&gt;SquareSpace&lt;/a&gt; for my personal website for 4 years, I decided to say goodbye and switch to something else, and that something else is this: a &lt;em&gt;static website&lt;/em&gt; built using &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; and hosted on &lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub (Pages)&lt;/a&gt; for free, with my old domain name purchased through &lt;a href=&quot;https://domains.google/#/&quot;&gt;Google Domains&lt;/a&gt;. In this post, I will briefly document the process of setting up the new website and migrating over content (mainly blog posts), as well as some interesting things I learned about websites and the interwebs from this process. For another perspective on migrating from SquareSpace to Jekyll, see &lt;a href=&quot;http://www.practicallyefficient.com/2016/04/03/static-and-free.html&quot;&gt;here&lt;/a&gt; for a very clear and concise exposition.&lt;/p&gt;

&lt;h3 id=&quot;why-i-left-squarespace&quot;&gt;Why I left SquareSpace&lt;/h3&gt;
&lt;p&gt;SquareSpace, for the most part, was a great service. Having used Wordpress and Tumblr for blogging, SquareSpace just felt like a sharper, classier, and better put-together platform. Most of that was probably due to the well-designed templates, which were fewer in number compare to Wordpress or Wix, but every one of them was good. It also costs $100 a year, which, factoring in hosting, domain name registration, various integrated services (e.g., analytics, commenting) and the platform itself, was not unreasonable. I would still recommend SquareSpace to anyone that wants to have a beautiful personal website, especially if 1) coding ability is an issue, 2) you have a lot of multi-media content and/or transactions, and 3) having a custom domain name is a priority.&lt;/p&gt;

&lt;p&gt;So why did I switch? Well, having gone through the last 4 years of my PhD doing scientific programming everyday, I felt that I wanted my website to reflect my new-found skillset and “values”, if that’s not too strong of a word? That also means having a fuller control of my blog content and using a set of open-source tools that the computational sciences and software engineering community embraces (with the plan of embedding code or full-fledged Jupyter Notebooks into my blog in the future). When I built my website 4 years ago on SquareSpace, designing the visual experience was 90% of the work, partly because that was a priority of mine, but also because that was all I had control over. It was a beautiful website, if I may say so myself, and I had even injected custom CSS code. Too bad I didn’t take a screenshot of it before closing my account.&lt;/p&gt;

&lt;p&gt;Anyway, my one real problem with the platform was that it felt bulky, and writing a blog article inside a dynamic Javascript editor box that had serious word processing capabilities seemed like an overkill, if not downright slow and a waste of resources. Most of the time I would write in an editor offline anyway. Also, I think the main bulk of the platform went to supporting the visual design aspects of the website, and after having seen hundreds of developers’ personal/project sites with the same 4 or 5 fonts and layouts, I was sold: sign me up for uniformity, please! That was the last straw for me, and I knew I no longer wanted to pay $100 a year for a beautiful but slow and bulky platform to make a website that looked like it was threw together for $0 and in 10 minutes.&lt;/p&gt;

&lt;h3 id=&quot;static-vs-dynamic-websites&quot;&gt;Static vs. Dynamic Websites&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-07-21-jekyllfb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Note that I’m not a web-developer, and some of the comments below may be my own - incorrect - inferences.)&lt;/p&gt;

&lt;p&gt;Before delving into Jekyll, I want to talk about the difference between static vs. dynamic websites, which I think would be interesting for the broader audience as well. Prior to migrating, I thought all websites were the “same” and never really appreciated the fact that some sites have lots of dynamically responsive elements. An example of that is the status update box on Facebook, which opens up when you click it, and once you hit “Post”, that new status immediately updates your home page, and the “master copy” of Facebook as well, so that all your friends can see your fancy dinner. Same goes for Twitter in the browser. This is usually enabled in the form of Javascript (I think?), which communicates in realtime with the backend infrastructure of Facebook to make sure all these changes are updated simultaneously. Clearly, this requires a lot of resources, both in terms of backend infrastructure, hosting, and bandwidth, and makes sense if you have content that should be updated in realtime.&lt;/p&gt;

&lt;p&gt;In contrast, static websites don’t update in realtime, and have very few responsive elements. If you’ve ever visited a Math/CS professor’s website, you’ll know what a static website of the times past looks like. Static websites are easy to manage, because it’s just a bunch of text/data files on a server, literally like a folder on your computer. It’s also faster if all you have are html files that need to be rendered and displayed. For most people, this suffices as a personal website. I guess that’s the reason why we were taught how to make static sites in high school, but what was a pain in the ass then is still a pain in the ass with creating static websites now: writing HTML and CSS from scratch. I think this is the main barrier for people who want a personal website but don’t need to know html/CSS, i.e., not a web-developer.&lt;/p&gt;

&lt;h3 id=&quot;jekyll--github-pages&quot;&gt;Jekyll &amp;amp; GitHub Pages&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-07-21-jekyll2.png&quot; alt=&quot;&quot; /&gt;
Enter: Jekyll. The graphic above pretty much says it all. Jekyll is a tool that transforms (basically) text files into static, blog-enabled websites, which is then hosted on and automatically rendered by GitHub. To build a website in less than 5 minutes, you pretty much just need to follow &lt;a href=&quot;https://pages.github.com/&quot;&gt;this guide&lt;/a&gt;. GitHub provides a number of templates, and you can look externally for open-source themes made by people in the community, like the one I’m using here (&lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot;&gt;Minimal Mistakes&lt;/a&gt;). It wasn’t too much work to get it up and running, and there is plenty of support in the repo issues (for this particular theme, and most of the more popular ones). You can also have fancy features like aggregate blog posts by topic, a search bar, embedded photo galleries (like &lt;a href=&quot;/our-thin-layer-of-existence/&quot;&gt;here&lt;/a&gt;), etc.&lt;/p&gt;

&lt;p&gt;To use an analogy, if building a website is like cooking a meal, where your content is the raw ingredients and the recipe is the theme/features, then using Wordpress/SquareSpace is like mailing your raw ingredients to this “cooking company”, along with the specific recipes you want to follow, plus maybe some $$$, and you get your meal back. It’s hard to get the ingredients back in a useable form, and it’s more difficult to tweak the process itself, but it is relatively painless if you just want the meal itself. Using Jekyll, the “cooking machine” is in your own home, and you get to keep the ingredients, as well as the ability to tweak the recipe to a much greater degree, instantaneously. In this case, the “recipe” lives in the pre-designed theme, but also in the _config.yml file, where theme-specific settings, like color scheme, can be changed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All those factors, among other things, satisfies a particular niche where the user not only wants the finished product, but wants to be engaged with the process itself, at least insofar as to learn how it works.&lt;/strong&gt; Bonus: if you’re familiar with git/GitHub at all, this will feel like a &lt;em&gt;very&lt;/em&gt; natural process. (To take the analogy a step further, Gordon Ramsey has this saying in a lot of his videos: “let the knife do the work.”) &lt;strong&gt;And if you’re unfamiliar with git/GitHub/version control, this is a GREAT way to pick it up.&lt;/strong&gt; In a nutshell, I feel like this is a very good compromise between having a sense of how web-development works (hosting, resource management, etc), and not having to do a lot of the grunt work yourself (coding html/CSS, designing every single aspect of the visual experience). You can find a collection of themes &lt;a href=&quot;https://github.com/topics/jekyll-theme&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://themes.jekyllrc.org/&quot;&gt;here&lt;/a&gt;, &amp;amp; &lt;a href=&quot;https://jekyllthemes.io/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;markdown--atomio-workflow&quot;&gt;Markdown + atom.io Workflow&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-07-21-jekyll3.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;This is what being empowered looks like.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Having control/possession of the content and learning how to build things is nice, but ultimately, I want my working experience to be as painless as possible. This is what markdown + a good text editor really offers - I’m only half-way done writing this blog piece, and I have to say, this is by far the smoothest workflow I have ever experienced while blogging on any platform. I use &lt;a href=&quot;https://atom.io/&quot;&gt;atom&lt;/a&gt;, but I imagine Sublime to be just as capable. In the screenshot above, you can see the 3 split panels: files on the left, main content (this article) in the middle, and setting files on the right - and this is on a single laptop screen. You know when you first learn to program, and it’s like “holy crap I can do anything I want?!?” This is like that, but for blogging.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-07-21-jekyll5.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Square brackets for inline referenced links and images, then dump all the addresses at the bottom.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A huge part of the experience is that markdown files make embedding and formatting almost painless. Of course, you have to manage your files yourself, like the images you’re embedding into the article. But there are no more clunky [UPLOAD IMAGE] interfaces you have to deal with, nor button-clicking in a slow online editor for formatting text. Plus, it’s much faster, especially having gotten used to the markdown formatting shortcuts (## bold ##, etc. See &lt;a href=&quot;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&quot;&gt;cheatsheet&lt;/a&gt;). I forgot to mention above, GitHub automatically builds your site when you push changes, presumably by feeding your code through Jekyll on their server, but if you have Jekyll running locally, you can preview your site and content on the fly, not connected to the web at all!&lt;/p&gt;

&lt;p&gt;I believe both Wordpress and SquareSpace are able to accept markdown-style texts, which is great for external links, but is still a pain to manage images, since you have to upload and figure out the address of those images. Plus, no formatting-aware pretty colors. Oh and, of course, you get all the nice tools of the editor itself as well, one of which is “snippet” in atom, where I can type “blank_header” and that will expand into the blog article settings header you see in the first screenshot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2018-07-21-jekyll4.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Terminal on the left running Jekyll, Chrome on the right to preview (note the address bar).&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;migrating-from-wordpresssquarespace&quot;&gt;Migrating from Wordpress/SquareSpace&lt;/h3&gt;
&lt;p&gt;If I’ve convinced you to move your existing blog/website from Wordpress or SquareSpace, great! It took me a couple of months on the weekends, but a lot of it was figuring things out by myself and also manually reformatting a lot of old articles to be markdown compatible. Essentially, the process is to export your existing content from &lt;a href=&quot;https://en.support.wordpress.com/export/&quot;&gt;Wordpress&lt;/a&gt; or &lt;a href=&quot;https://support.squarespace.com/hc/en-us/articles/206566687-Exporting-your-site&quot;&gt;SquareSpace&lt;/a&gt; as XML files, and change them to be compatible with Jekyll &amp;amp; markdown. &lt;a href=&quot;http://www.practicallyefficient.com/2016/04/03/static-and-free.html&quot;&gt;This&lt;/a&gt; has some practical advice for the migration process, including a super helpful &lt;a href=&quot;https://gist.github.com/evanwalsh/6131008&quot;&gt;Ruby script&lt;/a&gt; (by Evan Walsh) that converts the html exports into markdown files. I’ve built some &lt;a href=&quot;https://github.com/rdgao/rdgao.github.io/tree/master/extras/helperscripts&quot;&gt;helper scripts&lt;/a&gt; as well, one of them (html2markdown.ipynb) is a Jupyter Notebook that fixes up some of the mangled exports, and another (make_thumbnail.ipynb) to resize images into the same size, either via padding or cropping and down/up-sampling, to make gallery-compatible thumbnails.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;html2text&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;post_folderpost_fol&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'../SS_export/SS_posts/'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;export_folder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'../SS_export/exported_md/'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;html_files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post_folder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_ind&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post_folder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;new_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;export_folder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'.md'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# read ...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readfile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# and write...&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# first 3 characters is always '---', so just need to find the second instance&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;yaml_end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'---'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;newdata&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'---&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;html2text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html2text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yaml_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;writefile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;writefile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newdata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Holy crap embedded python code! (Same as in html2markdown.ipynb, need the html2text package.)&lt;/p&gt;

&lt;h3 id=&quot;extra-goodies&quot;&gt;Extra Goodies&lt;/h3&gt;
&lt;p&gt;Two more points I wanted to mention: first, I’m very much looking forward to integrating Jupyter Notebooks into the blog, or at least have code snippets available embedded within tutorials. Specifically, I want to give &lt;a href=&quot;https://github.com/choldgraf/textbooks-with-jupyter&quot;&gt;this&lt;/a&gt; a go at some point. God willing that I don’t change the whole infrastructure of my website again in a few years, this site will be more than a blog, but also my “business card” or a curated gateway into my code and publications.&lt;/p&gt;

&lt;p&gt;Second, migrating my blog made me aware of something unexpected sinister that I had never thought about before: &lt;strong&gt;comments&lt;/strong&gt;. Specifically, using comment services like Disqus, which I think was enabled on my old SquareSpace blog. Disqus is super convenient as a service - I mean, clearly, I didn’t even realize that it was an independent service on my old blog. But like the saying goes, if you’re not paying for it, you’re the product. Turns out, even if your site does not serve ads or track visitor information for ad purposes, by enabling Disqus, you allow it to send that user info to their &lt;strong&gt;customers&lt;/strong&gt; - buyer of your information (see &lt;a href=&quot;https://fatfrogmedia.com/delete-disqus-comments-wordpress/&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://replyable.com/2017/03/disqus-is-your-data-worth-trading-for-convenience/&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://chrislema.com/killed-disqus-commenting/&quot;&gt;here&lt;/a&gt;). So as of right now, this blog is comment-less (by choice! Not that it was getting lots of comments before..) There are some really interesting/creative alternatives, like &lt;a href=&quot;https://staticman.net/&quot;&gt;staticman&lt;/a&gt;, which accepts comments and push them to the GitHub repo of your website. I spent quite some time getting it to work, but was unable to for weird reasons.&lt;/p&gt;

&lt;p&gt;For now, just @ me on Twitter.&lt;/p&gt;</content><author><name>Richard Gao</name></author><category term="Data &amp; Technology" /><summary type="html">I felt that I wanted my website to reflect my new-found skillset and &quot;values&quot;, if that's not too strong of a word? That also means having a fuller control of my blog content and using a set of open-source tools that the computational sciences and software engineering community embraces.</summary></entry><entry><title type="html">Year 3 (and a half): TIL my PhD is Flappy Bird.</title><link href="http://localhost:4000/year-3-and-a-half/" rel="alternate" type="text/html" title="Year 3 (and a half): TIL my PhD is Flappy Bird." /><published>2018-01-16T00:00:00-08:00</published><updated>2018-01-16T00:00:00-08:00</updated><id>http://localhost:4000/year-3-and-a-half</id><content type="html" xml:base="http://localhost:4000/year-3-and-a-half/">&lt;p&gt;It’s January of 2018. 40 months after I started my PhD. Wow. I know people say
that the years feel shorter as you get older, but these days are just zipping
by way too fast. I realized somewhere in October that I forgot to write one of
these for my third year, and all of a sudden, we’re in 2018. So I’m late, what
else is new.&lt;/p&gt;

&lt;p&gt;This post is broken into two parts. The first part is about a recent shift in
attitude I had on how to process failures and other “negative” emotions
throughout the PhD, during which I also realized that getting a PhD is more
like staying alive in Flappy Bird than running a marathon. Between when I
started writing this post and now (~10 days), I’ve had two major rejections
already (one paper, one conference submission). I’m still learning how to
process these things, but what you will read below is my take on some wisdom I
received through reading. The second part contains habits and lifestyle
changes I’ve acquired over the last year or so, that I think contributed
positively to my general well-being.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: none of what you’re about to read are meant to be advice, they’re more or less just entertaining chronicles of my graduate school adventure. But if you happen to find them useful or comforting, that’s all the better!&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;500&quot; src=&quot;/assets/images/blog/2018-01-16-year3-flappybird.png&quot; /&gt;
&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;PhD - Perpetual heap of Discomfort&lt;/strong&gt;&lt;br /&gt;
I thought PhD was like a marathon, and in some ways, it is - it’s grueling,
long, and most of the time you’re by yourself. But in a marathon, every step
you take contributes to getting you to the finish line. In a PhD, not so much.
It’s more like this: imagine stepping off a cliff way above the clouds, and
you’re falling through a mysterious unknown. Your trajectory has been
determined, and many, many things will come straight at you, largely outside
of your control. And you know, with every passing second, that you’re falling
(probably to your death), and it’s scary as hell. That being the case, your
attitude through the fall could be one of two things. The first is to
completely reject the reality that is the unstoppable force of gravity and
helplessly flail, feeling strong emotions at every possible opportunity. That
could be a positive or negative emotion: maybe you see a branch that might
stop or slow your fall, or maybe you hear the snap of the branch that you
thought would save you. All of these little events in their individual moments
seem disconnected and unrelated. From that perspective, their causes could
either be that you personally are doing something right/wrong, or that life is
being especially gratuitous/unfair. Through these three and half years,
sometimes I blamed life and the people around me, and often I blamed myself,
which starts the vicious cycle of unhappiness: “I seem to be falling. I see
and grasp at every little thing, but things often don’t work out. Now I’m
still falling and even more stressed out. Am I not learning quickly enough?
How is everyone else doing this so gracefully? I wish I wasn’t falling. Oh
shit, I’m still falling. Ugh… can’t I get a break or do something right?”&lt;/p&gt;

&lt;p&gt;In hindsight, that’s not the only way to experience this perpetual fall and
those emotions along the way, but I didn’t even consciously realize that that
was a choice I was making, or that I even had a choice. A few months ago, I
read “Man’s Search for Meaning” by Victor Frankl, and in it I found a piece of
wisdom that would go a long way in helping me unlock and process my feelings
of free-fall during my PhD, as well as life as a whole during these few years.
In it, he says something to the effect of: &lt;strong&gt;in this day and age, people have
a tendency to not only put themselves in situations where they are unhappy,
but they then become unhappy about their unhappiness, because they have a need
to be happy all the time.&lt;/strong&gt; After chewing on this for some time, I asked
myself: have I been unhappy during these years, and have I been unhappy about
my unhappiness, especially those unhappiness that stemmed from factors that
were a natural progression of being in graduate school in a foreign country
(which also happened to be a dumpster fire with a steady stream of shitty news
to look at on Twitter)? I think the answer was yes.&lt;/p&gt;

&lt;p&gt;All of the shitty stuff - work failures and personal failures - are real, and
in a way, it’s not my fault. It’s not anybody’s fault. I don’t mean that life
for graduate students can’t be made better by the institution as a whole - it
certainly can. Nor am I deflecting responsibility from the things that I could
have done better, like starting a deliverable early to avoid stressing myself
out last minute. But I think a part of me put such a heavy expectation on this
graduate school experience to be positive that I rejected the possibility that
my unhappinesses were a product of the situation I put myself in, but rather
time and time again blamed it on how much life sucks or how badly I’m doing as
a researcher. This doesn’t mean I haven’t been happy at all. Quite the
opposite, actually: I regularly find joy in the work I do and the progress
I’ve made, the new friends I’ve made and the experiences we’ve shared, and the
beautiful city and its ocean breeze I now think of as my new home, as well as
the cherished opportunities I get to see my old home and everything it
embodies. But these moments of happiness are fleeting, as all moments are.
Except, their endings only thrust me back to the reality of my falling,
against the expectation that each happy moment was suppose to mean that I have
figured life out, and that the feeling of falling should have disappeared.&lt;/p&gt;

&lt;p&gt;The realization that many aspects of my current situation made me unhappy was
powerful, and quite a relief. It’s difficult to put them into the right words,
but there’s a profound difference between the mindset that things are really
great overall and there are just unexpected wrinkles along the way, versus the
mindset - the more accurate one I’d say - that this process as a whole is
challenging and frustrating, and inevitably will make me unhappy, and that’s
perfectly fine, and I shouldn’t be too hard on myself or anyone around me
because of it. When I left for San Diego three and a half years ago, I kind of
just hopped on a plane and left. I didn’t think about how hard I made it for
myself to see my family and old friends, experience the comfort of home in
Toronto, and in general the challenges of starting life in a completely new
place. On top of that, never did I really think about the challenges of doing
a PhD, the real and daunting challenges that many before me have faced, which,
summed in one phrase, is a relentless feeling of failure and uncertainty, both
real and perceived.&lt;/p&gt;

&lt;p&gt;So what now? In Frankl’s book, one of the things he talks about, within the
framework of logotherapy - or logically looking at ourselves - is that we have
a choice in changing our situations. Taking responsibility for my own
happiness has always been my mantra, which, I think, was why I felt inadequate
or that I always have to do more to improve my own situation. But to change my
situation, I have to first accurately assess my situation, and these short-
term ups and downs, in a way, were largely predetermined by the choice of
being in graduate school 3 years ago. Yes, specific failures and disappoints
may be avoidable, but they will inevitably come in one form or another. Of
course, I can also choose to drop out at any time, and that would definitely
alleviate certain aspects of this feeling of falling. But if I wanted to stick
it through, I’d first have to acknowledge that failures, successes, more
failures and the feeling of being not good enough even after successes are all
a part of this process. A lot of people in academia, to their credit,
acknowledge the reality of anxiety, stress, and things like imposter syndrome,
for example. Every time I read a blog post on that, I think to myself:
“everyone feels it, it’s not a big deal, and it makes sense to feel it, so I
shouldn’t feel it now, because understanding it means I’m above it now.
Right?” Wrong. No matter how many times I’m confronted with this feeling, and
no matter how many times I feel satisfied with a piece of work I did, this
feeling has not gone away, and to be honest, I don’t think it ever will. So my
new resolution is just to embrace this feeling, and embrace the fall.
Something else that helped is the Buddhist perspective (or philosophy?) that
even “negative” emotions and events can be observed, appreciated, and
understood (I really like Thich Nhat Hanh’s writing about this in &lt;a href=&quot;https://www.amazon.com/Being-Peace-Thich-Nhat-Hanh/dp/188837540X/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;amp;qid=1515982322&amp;amp;sr=8-1)&quot;&gt;Being
Peace&lt;/a&gt;, and that’s an invaluable treasure in life, just as the happy moments are.&lt;/p&gt;

&lt;p&gt;(As I finish writing this, I am realizing that the falling process I described
as getting a PhD is basically &lt;a href=&quot;http://flappybird.io/&quot;&gt;Flappy Bird&lt;/a&gt; rotated 90
degrees: something is always propelling you forward, and you more or less
can’t control how fast you go forward, even though that’s what you’re measured
on. Instead, the effort you exert feels almost orthogonal to the direction you
want to move towards, but nevertheless, it’s those little actions that keep
you afloat and moving. Actually, that sounds like life in general. Holy shit -
life is Flappy Bird.)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;That being said, I think there were a few things I did over the last year or
so that definitely contributed positively to my mental and physical well-
being. Most of them fit in the larger theme of finding what works for myself,
so no guarantees that it will work for anyone else, but it might be worth a
try:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moved off-campus&lt;/strong&gt; : La Jolla, nice as it is, is not very good for living,
especially as a 25 year old with a diverse taste for cuisines (here comes the
Soylent jokes). More importantly, living on campus literally meant that I live
in the same place as I work, so I basically didn’t really live, and just
worked. Turns out, living in a place where I can walk for more than 15 minutes
and not having to stop because there’s nothing but highways is nice. Plus, I’m
now in an area much closer to my friends, and spending quality social time (or
get college-student-drunk) is definitely good for the soul! Also, having to
take the bus for 30 minutes to and from work means there is a significant
chunk of time away from my laptop, which brings me to…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Casual reading&lt;/strong&gt; : sometimes I still spend the whole bus ride thinking about
projects, but on most days I get a full hour of casual reading. I think I’ve
read more books this year than I have in the rest of my life combined, and
this sounds really stupid coming from a 26-year old, but books are great
stuff! On the surface level, being immersed in a really good fictional
narrative completely reenergizes my mind, and honestly on some days I look
forward to that bus ride more than actually going to the office (or anything
else in life) because of the book I’m reading. On a deeper level, the books
themselves, both fiction and non-fiction, have had transformative effects on
my life. I’d been meaning to put together a list of my personal favorites (who
knows when this will happen), but top of my list so far: the Glass Bead Game,
Dune, Half a Yellow Sun, All the Light We Cannot See, and I already mentioned
Man’s Search for Meaning. Is this always good for productivity? No, because on
some days, I would get to the office and read for another half an hour because
I just can’t put it down. But do I enjoy it? You’re goddamn right I do.
Shoutout to the people who have recommended or lent books to me, and my
bookclub buddy (spoiler: it’s my girlfriend) who patiently reads the weird
shit I want to read and picks books that I would’ve never read otherwise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mid-day workout&lt;/strong&gt; : I now go to the gym before lunch for an hour and half
every day, either to play basketball or to lift. I’ve even turned down free
lunch events because it overlapped with my gym time. &lt;em&gt;GASP&lt;/em&gt; I know, right? Why
was this helpful? For one, I feel like I’m in the best physical shape I have
been for a while, which definitely helps me feel good about myself, especially
during times where everything else is crap (running theme: how to have a good
day when life is shit otherwise). The other benefit is that taking that midday
break to do something active really helps the blood circulate to my brain
again, because if there’s anything I learned about myself during grad school,
it’s that I can’t sit still for more than 3 hours and still be fully there
mentally. I’ve tried various routines to overcome this, but working for 8
continuous hours a day always resulted in me dozing off for the second half of
the day, no matter how many hours I’ve slept before or how many coffees I
have. Midday nap doesn’t even do me as much good as exercise. I think the
combined effect of adrenaline and breathing more deeply is a much more natural
stimulant for the brain, and it has to come during the early afternoon for me,
because going to the gym at night after a long workday (during which I’ve
fallen asleep many times at my desk) is doubly ineffective.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Waking up earlier&lt;/strong&gt; : The awkward thing about exercising midday is that, of
course, I’m now working like 5 hours a day instead of 7, even though those 5
hours are all good, focused hours. To compensate for this, I usually try to
wake up early (like at 6 am) to put in two extra hours, have a late breakfast,
and go to the office. Because I woke up SOOO early (6 is hella early for a
grad student!), I get sleepy by 9pm, which means I’m in bed at grandma hours,
and the virtuous cycle continues. I’m really digging the tripartite workday
now (work, breakfast, work, gym + lunch, work), and the added benefit is that
even if I’m completely unproductive on campus for whatever reason (meetings,
teaching, etc), I still feel good about the 2 hours I got done before my day
even started. The only thing this isn’t good for is socializing on a
weeknight, because it screws up the next morning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Keeping a lab notebook&lt;/strong&gt; : working on computational projects means I usually
try many many things before something worthwhile happens, and sometimes a
whole day has to be spent on things like debugging code, or learning some
software package. All this means that a lot of work has to be done before I
see any real results, and that feels bad. Also, I find myself always referring
back to simulation or analyses I did in vague ways because I can’t remember
the details, but just remembering that it was interesting/important. So after
the Nth time of that happening, I decided to keep track of my daily
explorations and progress in Evernote. In hindsight, this makes a lot of
sense, because that’s what people in wet labs do. I guess I just felt that
computational stuff happens so quickly that it’s not worth writing down. But
hey, even if it doesn’t matter, seeing a full notebook definitely makes me
feel good even if nothing concrete came out of it yet, and that, my friends,
is the word of the day today.&lt;/p&gt;</content><author><name>Richard Gao</name></author><category term="Reflection" /><category term="Science" /><summary type="html">Year 3 is done! Here's to half a PhD.</summary></entry><entry><title type="html">Our thin layer of existence. (26/52)</title><link href="http://localhost:4000/our-thin-layer-of-existence/" rel="alternate" type="text/html" title="Our thin layer of existence. (26/52)" /><published>2017-10-16T00:00:00-07:00</published><updated>2017-10-16T00:00:00-07:00</updated><id>http://localhost:4000/our-thin-layer-of-existence</id><content type="html" xml:base="http://localhost:4000/our-thin-layer-of-existence/">&lt;p&gt;Hawaii was an absolute treat for the senses, where almost every single day we
witnessed something that was breathtaking: be it the sun setting into the
infinite expanse of the orange horizon, the warm saltiness of the sea water on
the skin that’s just cool enough to alert the peripheries, or the chirping of
countless &lt;a href=&quot;https://www.youtube.com/watch?v=TA_9_zAK5sA&quot;&gt;coqui frogs&lt;/a&gt; going in
and out of synchrony while the jungle leaves rustle peacefully under the
backdrop of the star-lit night. Words are not enough to really convey these
perceptions, and to be honest, neither are pictures, but &lt;a href=&quot;/gallery/hawaii-2017&quot;&gt;we tried
anyway&lt;/a&gt;. One idea that cannot be conveyed without words,
however, is the feeling of our minuteness I got - both me personally and human
lives in general - when I witnessed just a little more of the world around us.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2017-10-16-hawaii-hanauma.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/assets/images/blog/2017-10-16-hawaii-mauisunset.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/assets/images/blog/2017-10-16-hawaii-road2hana.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/assets/images/blog/2017-10-16-hawaii-haleakala.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Traveling to new places is always an eye opening experience, especially when
immersed in the ecology and culture of a foreign place. This was different,
though - it revealed to me something that I foolishly thought I already knew
intimately - the world in which I live. It’s like this: imagine you’re about
to visit someone’s house for the first time. It’s cool, because you know
you’ll see things you’ve never seen before, or at least how the same things
might be arranged differently, so it’s an expected kind of novelty. Now
imagine sitting on your own couch at home, with an abundance of familiarity
surrounding you. Everything was either put there by you, or it’s been there
for so long that it might as well have come with the place. All of a sudden,
someone whispers a few words into your ears and you watch the familiarity in
front of you unfold into an entirely new experience, realizing for the first
time that there is so much more to your home than meets the eye. Hawaii gave
me that feeling about this “world”, “my” world, and not just through its
exquisite wild life on the surface, but extending from the depth of the ocean
to the stars above. The whole two weeks were full of moments like those, but I
will just describe a few things that happened over a span of 48 hours on the
Big Island (Hawaii Island).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;The Earth shaping itself&lt;/strong&gt;&lt;br /&gt;
The state of Hawaii is a chain of islands formed by underwater volcanic
activity. I knew this, and it makes sense. How else does a chain of islands
emerge in the middle of the ocean? I didn’t know, though, that there are
&lt;strong&gt;active&lt;/strong&gt; volcanoes on the Big Island. That fact seems routine enough when
you read about it, but being there and witnessing it is another thing. We
visited the Volcano National Park, where the &lt;a href=&quot;https://en.wikipedia.org/wiki/Halemaumau_Crater&quot;&gt;Halemaumau
Crater&lt;/a&gt; spews out a thick stream of never-ending smoke during the day, and transforms into a scary demonic pit at night. The park itself is a huge area of land that surrounds the crater, as well as the aftermath of some of the more explosive eruptions
from a few decades ago. The landscape is incredibly eerie. It simultaneously
makes me appreciate the wrath of Mother Earth, fear her swiftness in taking
life away, and marvel at the incredible youth of the land beneath my feet and
its newly sprouted inhabitants. When I think of a young Earth, I think of
spring and budding greens. But here, youth is charred black, porous, and
honestly looks kinda deadly and downright alien.&lt;/p&gt;

&lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-crater.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-crater.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-infinitystone.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-infinitystone.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-craternight.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-craternight.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-dragonmei.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-dragonmei.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-meditate.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-meditate.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-freshlava.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-freshlava.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;The young Earth rages on.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The youthfulness of the land was further exhibited the next night, when we
went to the edge of the park where lava is pouring out down the slope of the
mountain and into the ocean. The feeling that we are standing there and
witnessing NEW EARTH BEING BORN is truly incredible. Land is literally being
formed around us, and the rocks we stepped on were younger than any of their
visitors (and there were some hardy toddlers braving the lava rock hikes in
the pitch black night). It was a sublime reminder that the world around us is
dynamic, constantly morphing, swallowing itself and rebirthing itself - not
only do plants and animals cycle through life and death, so too does the Earth
they stand on.&lt;/p&gt;

&lt;p&gt;Shit I never ever think about.&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;The world outside of our world&lt;/strong&gt;&lt;br /&gt;
I’ve always lived in crowded places with extremely dense light pollution.
Among those, La Jolla is probably the only place where I can regularly see
more than a handful of stars at night. I’ve heard of friends going out of the
city to star gaze, but have never done it myself, nor have I ever really
camped in my life (I know). So imagine my awe when we went three quarters way
up Mauna Kea (9300 ft), which is the dormant volcano on the Big Island and
peaks at almost 14000 ft!!! There a lot of cool little tidbits about this
mountain, one of which being that the base of it is actually &lt;strong&gt;deep&lt;/strong&gt; under
the ocean, so deep that if you measured from the base to its peak, it’s just
slightly taller than Mount Everest at 33,000 ft. Driving up to observatory
altitude is the embodiment of “0 to 100 real quick”. I think we went from
beach to 9300 ft in about an hour? On the way up, we had to drive through a
layer of super dense condensation (aka clouds), and it is a local saying that
many people hit the invisible cows on the way up and down the mountain because
visibility around the foggy area is no more than about 10 m ahead of you. But
beyond that, the sky above feels like it reaches the depth of the universe.
After nightfall, it’s as if we were transported to another dimension or planet
outside of our own, because I’ve never ever seen that many stars shining so
brightly. Apparently, from Mauna Kea, one can see every star available in the
northern half of the sky, and about 80-90% (?) in the southern sky, because,
you know, it’s a tall ass mountain.&lt;/p&gt;

&lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-maunakea.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-maunakea.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-inviscow.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-inviscow.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-maunakeaalt.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-maunakeaalt.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
&lt;/figure&gt;

&lt;p&gt;Standing under that diamond studded ceiling, we got to see a lot of
astronomical phenomena firsthand, through our own eyes (and sometimes through
a telescope). For example, there were guides at the visitor information center
that set up small telescopes for the crowd to take a closer look at the stars,
and I actually saw for the first time Saturn and its ring. It looks like a
&lt;a href=&quot;http://www.deepskywatch.com/images/articles/see-in-telescope/saturn-in-small-scope-n.jpg&quot;&gt;miniature, cartoon version&lt;/a&gt; of the Saturn I’m used to seeing in
books and films: a small tilted ring encircling a smaller dot, both unicolor
with a gray sheen. It was pretty neat. We also saw the ISS racing through the
night sky in a perfect broad curve, and several shooting stars. By far the
most indescribable feeling, though, was the smallness of humans and our planet
under such a majestic sky. Standing on top of the cold peak, it was like the
universe and all its mysteries were suddenly opened to me - I am directly
experiencing, for the first time ever, how vast the space is out there and how
little we really knew.&lt;/p&gt;

&lt;p&gt;It wasn’t quite a religious moment, but that was as close as I’ve ever gotten
to marveling in the creation of some higher being.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;The world within our world&lt;/strong&gt;&lt;br /&gt;
I’ve posted this before, and I have to post it again. I’ve watched this video
myself about 20 times now, and every time I do, I can’t help but have a big
stupid grin on my face. There are just so many completely spontaneous
opportunities to witness animals enjoying themselves, be it a manta ray
tumbling around, a family of sea turtle surfing the current, or a pack of
dolphins playing hide and seek with us in the bay.&lt;/p&gt;

&lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-yellowy.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-yellowy.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-dolphin.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-dolphin.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-sharp.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-sharp.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-turtle.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-turtle.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-beetle.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-beetle.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;http://localhost:4000/assets/images/blog/2017-10-16-hawaii-manta.jpg&quot;&gt;
        &lt;img src=&quot;http://localhost:4000/assets/images/blog/thumbnail/2017-10-16-hawaii-manta.jpg&quot; alt=&quot;&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;So much of our world - and our neighbors - is below us.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;As a surface-dweller, my idea of life is mostly concentrated around my
altitude and on dry land. Rarely do my thoughts venture out into the other 70%
of this planet. The waters around Hawaii, though, really made me feel that
there is life all around us. Perhaps they’re different, and look a little
strange, but life nonetheless. Breaking through the thin surface of the water
that separates two worlds, you are instantaneously immersed in another
storyline, like an invisible fly on the wall with the special privilege to
witness the completely normal lives of all its characters. In those moment, I
felt acknowledged and welcomed, and I hope I can do the same for them one day.
It really makes me question, even now, the extent of other kinds of cognition,
beyond our simply human ideals.&lt;/p&gt;

&lt;p&gt;After all, we are but a thin layer of existence in a much, much larger whole.&lt;/p&gt;</content><author><name>Richard Gao</name></author><category term="Travels" /><summary type="html">Hawaii was truly an eye-opening and life-changing experience.</summary></entry><entry><title type="html">First research paper published! (25/52)</title><link href="http://localhost:4000/first-research-paper-published/" rel="alternate" type="text/html" title="First research paper published! (25/52)" /><published>2017-09-18T00:00:00-07:00</published><updated>2017-09-18T00:00:00-07:00</updated><id>http://localhost:4000/first-research-paper-published</id><content type="html" xml:base="http://localhost:4000/first-research-paper-published/">&lt;p&gt;Because I’m quite short on my 52 posts this year, I’m stealing this from the
&lt;a href=&quot;http://voyteklab.com/inferring-ei-balance-from-lfp/&quot;&gt;lab blog&lt;/a&gt; (which I
wrote!). But hey! First research paper published, jeez what a long process…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Highlights (tl;dr)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The overarching goal of our recent &lt;em&gt;NeuroImage&lt;/em&gt; paper
(&lt;a href=&quot;http://voyteklab.com/wp-content/uploads/Gao-NeuroImage2017.pdf&quot;&gt;PDF&lt;/a&gt;) is to
make inferences about the brain’s synaptic/molecular-level processes using
large-scale (very much non-molecular or microscopic) electrical recordings. In
the following blog post, I will take you through the concept of excitation-
inhibition (EI) balance, why it’s important to quantify, and how we go about
doing so in the paper, which is the novel contribution. It’s aimed at a broad
audience, so there are a lot of analogies and oversimplifications, and I refer
you to the paper itself for the gory details. At the end, I reflect a little
on the process and talk about the real (untold) story of how this paper came
to be. &lt;em&gt;**&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Tale of Two Forces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inside all of our brains, there are two fundamental and opposing forces – no,
not good and evil – excitation and inhibition. Excitatory input, well,
“excites” a neuron, causing it to depolarize (become more positively charged
internally) and fire off an action potential if enough excitatory inputs
converge. This is the fundamental mechanism by which neurons communicate:
shorts bursts of electrical impulses. Inhibitory inputs, on the other hand, do
exactly the opposite: they hyperpolarize a neuron, making it less likely to
fire an action potential. Not to be hyperbolic, but since before you were born
these two forces were waging war with and balancing one another through
embryonic development, infancy, childhood, adulthood, and till death. There
are lots of molecular mechanisms for excitation and inhibition, but for the
most part, “excitatory neurons” are responsible for sending excitation via a
neurotransmitter called glutamate, and “inhibitory neurons” are responsible
for inhibition via GABA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2017-09-18-paper1-EI.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Like all great rivalries (think Batman and Joker, Celtics and Lakers), these
two forces cannot exist without each other, but they also keep each other in
check: too much excitation leads the brain to have run-away activity, such as
what happens in seizure, while too much inhibition shuts everything down, as
happens during sleep, anesthesia, or being drunk. This makes intuitive sense,
and scientists have empirically validated this “excitation-inhibition balance”
concept numerous times. This EI balance, as it’s called, is ubiquitous under
normal conditions, and has been proposed to be crucial for neural computation,
the routing of information in the brain, and many other processes.
Furthermore, it’s been hypothesized, with some experimental evidence in
animals, that an imbalance of excitation and inhibition is the cause (or
result) of many neurological and psychiatric disorders, including epilepsy,
schizophrenia, and autism, just to name a few.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finding Balance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given how important this intricate balance is, it is actually quite difficult
to measure at any moment the ratio between excitatory and inhibitory inputs. I
mentioned above that there is empirical evidence for balance and imbalance.
However, in the vast majority of these cases, measurements are done by poking
&lt;strong&gt;tiny&lt;/strong&gt; electrodes into single neurons and, via a protocol called voltage
clamping, scientists record &lt;em&gt;within&lt;/em&gt; a single neuron how much excitatory and
inhibitory input that neuron is receiving. Because the setup is so delicate,
it’s often done in slices of brain tissue kept alive in a dish, or sometimes
in a head-fixed, anesthetized mouse or rat – basically, in brain tissue that
can’t move much, but not in humans. I mean, imagine doing this in the intact
brain of a living human – yeah, I can’t either. And as far as I know, it’s
never been done. This presents a pretty big conundrum: if we want to link a
psychiatric disorder to an improper ratio between excitation and inhibition in
the human brain directly, but we can’t actually measure that thing, how can we
corroborate that EI (im)balance matters in the way we think it does?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2017-09-18-paper1-imbalance.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Our Approach: Parsing Balance From “Background Noise”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is exactly the problem we try to solve in our recent paper published in
&lt;em&gt;NeuroImage&lt;/em&gt; : how might one estimate the ratio between excitation and
inhibition in a brain region without having to invasively record from within a
brain cell (which is not something most people would like to happen to them)?&lt;/p&gt;

&lt;p&gt;Well, recording inside a brain cell is hard, but recording outside brain cells
– extracellularly – is a LOT easier. It’s still pretty invasive, depending on
the technique, but much safer and more feasible in moving, living, behaving
people. Of course, recording outside the brain cell is not the same as
recording the inside – when we record electrical fluctuations in the space
&lt;em&gt;around&lt;/em&gt; neurons, rather from within or right next to a single neuron, we’re
picking up the activity of &lt;em&gt;thousands to millions&lt;/em&gt; of cells all &lt;a href=&quot;http://www.scholarpedia.org/article/Local_field_potential&quot;&gt;mixed up
together&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first critical idea of our paper was that this aggregate signal – often
referred to as the local field potential (LFP) – reflects excitatory and
inhibitory inputs onto a large population of local cells, not just a single
one. Therefore, we should be able to get a general estimate of balance by
decoding this aggregate signal. The second piece of critical information was
the realization that (for the most part) excitatory inputs are fast and
inhibitory inputs are slow so that, even when they are mixed together from
millions of different sources like in the LFP signal, we are still able to
separate their effects: not in time, but in the frequency-domain (see our
&lt;a href=&quot;https://github.com/voytekresearch/tutorials/blob/master/Power%20Spectral%20Density%20and%20Sampling%20Tutorial.ipynb&quot;&gt;frequency domain
tutorial&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2017-09-18-paper1-model.png&quot; alt=&quot;&quot; /&gt;
A: LFP model with excitatory and inhibitory inputs; B: the time course of E
and I inputs from a single action potential; C: simulated synaptic inputs
(blue and red) and LFP (black); F: LFP index of E:I ratio&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Combining&lt;/strong&gt; &lt;strong&gt;Computational&lt;/strong&gt; &lt;strong&gt;Modeling&lt;/strong&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;strong&gt;Empirical (Open) Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pursuing this line of reasoning, we simulated populations of neurons &lt;em&gt;in
silico&lt;/em&gt; and looked at how their activity would generate a local field
potential recording. What this means is that we can generate, in a computer
simulation, different ratios of excitatory or inhibitory inputs into a brain
region and see how that influences the simulated LFP. Through this
computational model we found an index for the relative ratio between
excitation and inhibition.&lt;/p&gt;

&lt;p&gt;For those of you that are into frequency-domain analysis of neural signal,
this index is the 1/f power law exponent of the LFP power spectrum. Let’s
unpack that a bit. In the figure above (panel B) you can see that the
excitatory electrical currents (blue) that contribute to the LFP shoot up in
voltage really quickly—within a few thousands of a second—and then slowly
decay back down to zero. In contrast, the inhibitory currents (red) also shoot
up pretty quickly—but not &lt;em&gt;as&lt;/em&gt; quickly—and then decay back to zero &lt;em&gt;much&lt;/em&gt; more
slowly than the excitatory inputs. When you add up thousands of these currents
happening all at different times, the simulated voltage (panel C, black) looks
to us humans a lot like noise. But through the mathematical magic of the
Fourier transform, when we look at this same signal’s frequency
representation, they’re clearly distinguishable!&lt;/p&gt;

&lt;p&gt;More technically, the idea is that the ratio between fast excitation and slow
inhibition should be represented by the relative strength between high-
frequency (rapidly fluctuating) and low-frequency (slowly fluctuating)
signals. With this hypothesis in hand, we were able to make use of several
publicly available databases of neural recordings to validate the predictions
made by our computational models in a few different ways. One example from the
paper: we were lucky enough to find a recording from macaque monkeys
undergoing anesthesia, and the anesthetic agent, propofol, acts through
amplifying inhibitory inputs in the brain at GABA synapses. Therefore, we
predicted that when the monkey goes under, we should see a corresponding
change in the power law exponent, and that’s exactly what we found! As you can
see below, our EI index remains relatively stable during the awake state, then
immediately shoots down toward an inhibition-greater-than-excitation regime
during the anesthetized state before coming back to baseline after the
anesthesia wears off.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/2017-09-18-paper1-monkey.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Takeaways and Disclaimers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So to summarize, we were able to make predictions, borne out of observations
from previous physiological experiments and our own computational modeling,
and then validate these predictions using data from &lt;strong&gt;existing databases&lt;/strong&gt; to
draw a link between EI balance, which is a cellular-level process, and the
local field potential, which is an aggregate circuit-level signal. Personally,
I think that bridging the gap between these different levels of description in
the brain is super interesting, and it’s one way for us to confirm our
understanding of how the brain gives rise to cognition and behavior at
multiple scales. Furthermore, we can now make use of the theoretical idea of
EI balance in places where it was previously inaccessible, such as a human
patients responding to treatments.&lt;/p&gt;

&lt;p&gt;Before I wrap up, I just want to point out that &lt;strong&gt;this paper does not
conclusively show that EI balance directly shifts the power law exponent&lt;/strong&gt; –
what we show is a suggestive correlation. Nor does the correlation hold under
all circumstances. We had to make a lot of assumptions in our model and the
data we found, such as the noise-like process by which we generated the model
inputs. I’m not throwing this out here to inhibit the excitement (hah, hah),
but rather to limit the scope of our claim, especially for a public-facing
blog piece like this.&lt;/p&gt;

&lt;p&gt;Rather, ours is the first step of an ongoing investigation, and although we
will probably find evidence that &lt;a href=&quot;http://voyteklab.com/more-
evidence-1f-lffp-noise-indexes-excitationinhibition-balance/&quot;&gt;corroborates&lt;/a&gt; and
&lt;a href=&quot;https://arxiv.org/abs/1708.09042&quot;&gt;contradicts&lt;/a&gt; our findings later on, it’s
important that anyone reading this and getting excited (hah) about it
understands that we do not, and likely will not, have the last word on this.
Ultimately, though, I believe we stumbled onto something pretty cool and we’ll
definitely follow up on those assumptions one by one, and hopefully have more
blog posts to come!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some Personal Reflection&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This project was my first real scientific research project in grad school, and
it definitely created in me a lot of joy and excitement, as well as caused a
fair amount of brooding. As a whole, I really enjoyed the process of building
a computational model, even if it was quite simple, and using the predictions
from that to inform further empirical investigations. As I mentioned, I think
we really need to bridge the gap between molecular-level mechanisms in the
brain and circuit/organism-level “neural markers”, and computational modeling
work allows us to do that in situations where it would be intractable for many
reasons. I certainly subscribe to the notion that combining
theoretical/computational work with empirical data is an exciting and fruitful
line of research, because it fills a space between two successful but largely
non-overlapping subfields in neuroscience (though that trend is now changing).&lt;/p&gt;

&lt;p&gt;Also, the fact that we were able to test our predictions on publicly available
data was such a blessing, as we simply did not have the capacity, as a new
lab, to do those &lt;em&gt;in vitro&lt;/em&gt; and &lt;em&gt;in vivo&lt;/em&gt; experiments ourselves. However, that
meant combing through tons and tons of data where there might have been
unlabeled or badly labeled information, only to reach the conclusion that the
data is unusable for our purposes. There was some (a lot) of headbanging due
to this, but ultimately, we found useful (FREE!) data and I’m very grateful
for the people that made them available: &lt;a href=&quot;https://crcns.org/data-
sets/hc/hc-2/about-hc-2&quot;&gt;CRCNS&lt;/a&gt; and Buzsaki Lab, &lt;a href=&quot;http://neurotycho.org/&quot;&gt;Neurotycho&lt;/a&gt;
and Fujii Lab, as well as many friends and collaborators that donated data for
us to test different routes. To support this open-access endeavor, all code
used to produce the analysis and figures are on our lab GitHub, found
&lt;a href=&quot;https://github.com/voytekresearch/eislope&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Untold Story&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One last note, for those of you that find the process of scientific discovery
interesting: in this blog post, I tried to write the story as a lay-friendly
CliffsNotes version of the paper, starting with the importance of EI balance
and the motivation to find an accessible index of it in the LFP, then
outlining how we went about solving that problem. That’s the scientific story,
and while not false, it’s not chronological.&lt;/p&gt;

&lt;p&gt;The actual story began with &lt;a href=&quot;http://voyteklab.com/journal-
of-neuroscience-paper-age-related-changes-in-1f-neural-electrophysiological-
noise/&quot;&gt;Brad’s 2015 paper&lt;/a&gt; showing that aging is associated with 1/f changes. That was actually
what first interested me back when I started in 2014 – this seemingly
ubiquitous phenomenon (1/f scaling) in neural data. After digging a bit to
find various accounts for how 1/f observations arise in nature, we decided to
just simulate the LFP ourselves and see what happens. Turns out, the 1/f
naturally falls out of the temporal profile of synaptic currents, which both
have exponential rise and decay.&lt;/p&gt;

&lt;p&gt;Our model contained what I thought to be the bare minimum: excitatory and
inhibitory currents. At that point, I didn’t have a clue about what EI balance
was and what it has been linked to. I think I was twiddling parameters one
day, and realized that changing the relative weight of E and I inputs will
cause the 1/f exponent (or slope) to change because of their different time
constants. Then, like any modern-day graduate student, I Googled to see if
this is something that actually happens &lt;em&gt;in vivo&lt;/em&gt; , and the rest was history.
This little anecdote really just speaks to the serendipity of science, and it
couldn’t have happened without the many hours of spontaneous discussions in
the lab, which I’m also very grateful for, and Google. I think these little
stories really liven up the otherwise logical world of science, and I’d love
to read about such stories from other people!&lt;/p&gt;</content><author><name>Richard Gao</name></author><category term="Brain &amp; Cognition" /><category term="Science Communication" /><summary type="html">Publishing a paper is most definitely a non-linear path.</summary></entry><entry><title type="html">Holy sh*t I swam with a manta ray: the most incredible and awe-inspiring thing I’ve seen. (24/52)</title><link href="http://localhost:4000/the-most-incredible-and-awe-inspiring-thing-ive-seen/" rel="alternate" type="text/html" title="Holy sh*t I swam with a manta ray: the most incredible and awe-inspiring thing I've seen. (24/52)" /><published>2017-08-26T00:00:00-07:00</published><updated>2017-08-26T00:00:00-07:00</updated><id>http://localhost:4000/the-most-incredible-and-awe-inspiring-thing-ive-seen</id><content type="html" xml:base="http://localhost:4000/the-most-incredible-and-awe-inspiring-thing-ive-seen/">&lt;p&gt;We saw a lot of wonderful and incredible sights in Hawaii over the last two
weeks, all of which I will shortly detail. But the one thing that stood out
the most was this moment, so much so that I think it deserves its own entry.&lt;/p&gt;

&lt;p&gt;We were snorkeling at Two Steps Beach on the Big Island, near Captain Cook. It
was a treasure trove of marine life with an abundance of corals, but by that
point we had been in Hawaii for a week and half, and everything I saw became
unremarkable in that everything was routinely remarkable. I was swimming over
to the other end of the bay to meet Mei on the beach, still hopeful that I
might see a dolphin or two, when this great white shape appeared - about the
size of me - and fanned out in front of me.&lt;/p&gt;

&lt;p&gt;My immediate feeling was that of fear, since it’s not a run-of-the-mill
experience to come face to face with a wild animal the size of myself on land,
and who knows if this creature is benevolent. Even if it doesn’t eat me, what
if I pissed it off by swimming near its home? This feeling of apprehension
never quite went away, but gave way to so many more: excitement at this rare
opportunity, curiosity in this strange beast, joy in watching it tumble round
and round in the water. So I followed it around for two minutes, just out of
reach. Not that it seemed to care.&lt;/p&gt;

&lt;p&gt;I hate to anthropomorphize nature but in this moment, it’s impossible to not
ponder the mind of this being, as it performed its strange dance. Perhaps
joyously, perhaps hungrily. All of these thoughts and emotions combined
themselves into a single concept - awe - and I found myself completely
immersed in the presence of another entity in its undisturbed natural habitat,
wondering just how many more lives like this one surrounded our thin surface
of existence, evading our consciousness.&lt;/p&gt;

&lt;p&gt;Without further ado (only edit I made was adding the music, over which you can
still sometimes hear my gasps):&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com //--&gt;
&lt;div class=&quot;responsive-video-container&quot;&gt;

  &lt;iframe src=&quot;https://player.vimeo.com/video/230315536&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;/div&gt;</content><author><name>Richard Gao</name></author><category term="Travels" /><category term="Musings" /><summary type="html">I am the uninvited guest of this magnificent and playful host.</summary></entry><entry><title type="html">What is the hardest scientific endeavor of all? (23/52)</title><link href="http://localhost:4000/what-is-the-hardest-scientific-endeavor-of-all-answer-neuroscience/" rel="alternate" type="text/html" title="What is the hardest scientific endeavor of all? (23/52)" /><published>2017-06-12T00:00:00-07:00</published><updated>2017-06-12T00:00:00-07:00</updated><id>http://localhost:4000/what-is-the-hardest-scientific-endeavor-of-all-answer-neuroscience</id><content type="html" xml:base="http://localhost:4000/what-is-the-hardest-scientific-endeavor-of-all-answer-neuroscience/">&lt;p&gt;“The brain is the most complex thing in the universe.”&lt;/p&gt;

&lt;p&gt;If there was one quote that grinds my gears more than all else, it is this
one. It is simply an idiotic thing to say: the brain is not the most complex
thing in the universe; the universe, which contains billions and billions of
brains small and big, is the most complex thing in the universe. Even the
interaction between two brains, certainly, is more complex than the brain
itself. So why is neuroscience, the scientific study of the brain, the hardest
of all?&lt;/p&gt;

&lt;p&gt;Let’s take, as two examples, quantum physics and astrophysics, fields that
study the tiniest and the grandest objects in our universe. These are
extremely difficult things to study, because to even observe the signals
necessary to start answering the questions we’ve set forth, it takes some real
human ingenuity and delicate engineering to construct devices that can give us
reliable measurements we can then use to make inferences. Watch this really
great &lt;a href=&quot;https://www.youtube.com/watch?v=iphcyNWFD10&quot;&gt;video&lt;/a&gt; on LIGO and the
detection of gravitational waves if you are not convinced, and coincidentally,
it is a combination of both quantum physics and astrophysics (4:45 is the best
part…why are they even wearing goggles?)&lt;/p&gt;

&lt;p&gt;If that’s not the craziest thing I’ve heard of, which is literally at the
physical boundary of the universe (as we understand today) on both the
smallest and the largest scales, I don’t know what is. Maybe putting people on
Mars? Maybe Elon Musk’s new brain hat? Which brings me to how much more
difficult neuroscience is. Actually, it’s not just neuroscience, it’s all
scientific efforts that try to study some aspect of the human mind, like
psychology and cognitive science, but not, for example, neurobiology. And the
reason I believe this is not because neuroscience is intrinsically hard - it
most certainly pales in comparison to many branches of the physical and even
social sciences.&lt;/p&gt;

&lt;p&gt;What makes it hard, I think, is that it is incredibly difficult for a human to
be objective when we study the brain and the mind: the phenomena we are
interested in explaining are the ones that occur on a daily basis in our
mundane lives, like paying attention to traffic, perceiving color, etc, and it
is precisely these subjective experiences that we not only draw inspiration
from, but also try to dissect as if they are objective things in the universe.
I certainly don’t want to get into the debate of what is and what is not
objectively real, a photon or consciousness, but I think we can agree that one
is more objectively existing than the other, at least in terms of how we
operationalize them scientifically. In fact, I think the more “dead” we think
something is, the easier it is to study it objectively, which might explain
the incredible disparity in our level of understanding of the brain compared
to every other organ in our body. When we study the brain and the mind,
confirmation bias does not only creep in at the objective scientific level,
but the personal level as well. Yes, we can be diligent in checking results
that confirm our hypotheses, but it is damn near impossible to be diligent in
checking results that are consistent with our daily experiences and intuition.
After all, if I’ve had these experiences, it must be the right, right?&lt;/p&gt;

&lt;p&gt;Why is it so hard? I’m not sure, but if I were to venture a guess, I think it
is rooted in our wish to preserve our own identities, livingness, and
humanness. One of the things that makes us humans feel special is the belief
that we are special: yes, dogs and cows and rats and dolphins all have brains,
but we must be special in some way? And if we were to lose this feeling of
specialness in the pursuit of an objective understanding of the brain and our
humanness, we would paradoxically lose this humanness altogether. In fact, I
believe it is crucial that we treat the human brain like we treat any other
organism on this planet in order to properly study it, but that would create
such an intense dissonance, because at the end of the day, when we’re done
being neuroscientists, we’re back to being regular people - a friend, a
spouse, a parent - all of which requires this special humanness to maintain,
this special belief that we’re all the good guys at the end of the day.&lt;/p&gt;

&lt;p&gt;I recently finished reading Paul Kalanithi’s When Breath Becomes Air, and his
story contextualized our scientific effort in understanding the brain in a way
that’s never explicitly occurred to me before - though I soon realized
afterwards that it’s always been an implicit motivation for me, and perhaps
for many others - and that is the search for meaning. I believed that we can
ultimately objectively define the meaning of our existence by understanding
the brain, the organ that presumably gives us this sense of meaning - our
joys, pains, struggles, triumphs, and every other thing we feel - in the first
place. Now that I consciously think about it, I don’t know whether this will
ever happen: to objectively understand this sense of meaning, we may have to
give up that there is any meaning in the first place, and simply describe our
thoughts, movements, and interactions as physical quantities changing over
time, like how we objectively describe ant colonies, economics, and a
&lt;a href=&quot;https://www.wired.com/2011/11/starling-flock/&quot;&gt;murmuration of birds&lt;/a&gt;.&lt;/p&gt;</content><author><name>Richard Gao</name></author><category term="Brain &amp; Cognition" /><summary type="html">We cannot study what we cannot truly see, and can we truly see ourselves?</summary></entry></feed>